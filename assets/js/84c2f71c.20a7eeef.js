"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[53],{1924:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>_,frontMatter:()=>i,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-3-perception-sensing/inertial-sensing","title":"Inertial and Proprioceptive Sensing","description":"Introduction: Understanding Self-Motion and Internal State","source":"@site/docs/module-3-perception-sensing/inertial-sensing.md","sourceDirName":"module-3-perception-sensing","slug":"/module-3-perception-sensing/inertial-sensing","permalink":"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/inertial-sensing","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-perception-sensing/inertial-sensing.md","tags":[],"version":"current","frontMatter":{},"sidebar":"textbookSidebar","previous":{"title":"Tactile Sensing and Haptic Feedback","permalink":"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/tactile-sensing"},"next":{"title":"Sensor Fusion and State Estimation","permalink":"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/sensor-fusion"}}');var a=t(4848),r=t(8453);const i={},s="Inertial and Proprioceptive Sensing",l={},c=[{value:"Introduction: Understanding Self-Motion and Internal State",id:"introduction-understanding-self-motion-and-internal-state",level:2},{value:"The Role of Inertial Sensing in Humanoid Robotics",id:"the-role-of-inertial-sensing-in-humanoid-robotics",level:3},{value:"Inertial Measurement Units (IMUs)",id:"inertial-measurement-units-imus",level:2},{value:"IMU Components and Principles",id:"imu-components-and-principles",level:3},{value:"Accelerometers",id:"accelerometers",level:4},{value:"Gyroscopes",id:"gyroscopes",level:4},{value:"Magnetometers",id:"magnetometers",level:4},{value:"Complete IMU System",id:"complete-imu-system",level:3},{value:"Attitude and Heading Reference Systems (AHRS)",id:"attitude-and-heading-reference-systems-ahrs",level:2},{value:"Complementary Filter for Attitude Estimation",id:"complementary-filter-for-attitude-estimation",level:3},{value:"Extended Kalman Filter for IMU Fusion",id:"extended-kalman-filter-for-imu-fusion",level:3},{value:"Joint Position and Proprioceptive Sensing",id:"joint-position-and-proprioceptive-sensing",level:2},{value:"Encoder-Based Position Sensing",id:"encoder-based-position-sensing",level:3},{value:"Motor Current Sensing",id:"motor-current-sensing",level:3},{value:"Sensor Fusion for State Estimation",id:"sensor-fusion-for-state-estimation",level:2},{value:"Kalman Filter for Joint State Estimation",id:"kalman-filter-for-joint-state-estimation",level:3},{value:"Whole-Body State Estimation",id:"whole-body-state-estimation",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"ROS 2 Inertial Sensing Node",id:"ros-2-inertial-sensing-node",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"inertial-and-proprioceptive-sensing",children:"Inertial and Proprioceptive Sensing"})}),"\n",(0,a.jsx)(e.h2,{id:"introduction-understanding-self-motion-and-internal-state",children:"Introduction: Understanding Self-Motion and Internal State"}),"\n",(0,a.jsx)(e.p,{children:'Inertial and proprioceptive sensing systems provide humanoid robots with awareness of their own motion, orientation, and internal configuration. These sensors form the "vestibular system" and "proprioceptive sense" of the robot, enabling it to understand its state in space and maintain balance. This section explores the principles, technologies, and applications of these critical sensing modalities.'}),"\n",(0,a.jsx)(e.h3,{id:"the-role-of-inertial-sensing-in-humanoid-robotics",children:"The Role of Inertial Sensing in Humanoid Robotics"}),"\n",(0,a.jsx)(e.p,{children:"Inertial sensors provide essential information for humanoid robots:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Attitude Estimation"}),": Understanding orientation relative to gravity"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Motion Detection"}),": Detecting movement, acceleration, and rotation"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Balance Control"}),": Maintaining upright posture during locomotion"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"State Estimation"}),": Tracking position, velocity, and orientation"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Sensor Fusion"}),": Combining with other sensors for robust perception"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"inertial-measurement-units-imus",children:"Inertial Measurement Units (IMUs)"}),"\n",(0,a.jsx)(e.h3,{id:"imu-components-and-principles",children:"IMU Components and Principles"}),"\n",(0,a.jsx)(e.p,{children:"An IMU typically combines three types of sensors:"}),"\n",(0,a.jsx)(e.h4,{id:"accelerometers",children:"Accelerometers"}),"\n",(0,a.jsx)(e.p,{children:"Accelerometers measure linear acceleration along three orthogonal axes:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class Accelerometer:\n    def __init__(self, sensor_id, range_g=16.0, resolution=16):\n        self.sensor_id = sensor_id\n        self.range_g = range_g  # Measurement range in g (9.81 m/s^2)\n        self.resolution = resolution  # ADC resolution in bits\n        self.max_count = 2**(resolution-1) - 1\n        self.bias = np.zeros(3)  # Bias calibration values\n        self.scale_factor = np.array([1.0, 1.0, 1.0])  # Scale factor per axis\n        self.current_reading = np.zeros(3)\n\n    def read_raw(self):\n        """Read raw accelerometer data (simulated)"""\n        # In real hardware, this would interface with the sensor\n        # For simulation, we\'ll add some realistic noise and bias\n        true_acceleration = np.random.normal(0, 0.1, 3)  # 0.1 m/s^2 noise\n        # Add gravity component (assuming Z-axis points up)\n        true_acceleration[2] += 9.81  # Gravity\n\n        # Apply bias, scale factor, and quantization\n        biased_accel = true_acceleration + self.bias\n        scaled_accel = biased_accel * self.scale_factor\n\n        # Convert to digital counts and back to simulate quantization\n        counts = np.round(scaled_accel * self.max_count / (self.range_g * 9.81))\n        counts = np.clip(counts, -self.max_count, self.max_count)\n        digital_accel = counts * (self.range_g * 9.81) / self.max_count\n\n        return digital_accel\n\n    def calibrate_bias(self, static_readings=100):\n        """Calibrate accelerometer bias while sensor is static"""\n        readings = []\n        for _ in range(static_readings):\n            raw = self.read_raw()\n            readings.append(raw)\n\n        avg_reading = np.mean(readings, axis=0)\n        # Remove expected gravity (assuming Z-axis points up)\n        self.bias = avg_reading - np.array([0, 0, 9.81])\n        return self.bias\n\n    def get_specific_force(self):\n        """Get specific force (acceleration minus gravity)"""\n        raw = self.read_raw()\n        corrected = raw - self.bias\n        # Subtract gravity component to get specific force\n        specific_force = corrected - np.array([0, 0, 9.81])\n        return specific_force\n\n    def detect_impact(self, threshold=50.0):\n        """Detect impact based on high acceleration"""\n        raw = self.read_raw()\n        magnitude = np.linalg.norm(raw)\n        return magnitude > threshold\n'})}),"\n",(0,a.jsx)(e.h4,{id:"gyroscopes",children:"Gyroscopes"}),"\n",(0,a.jsx)(e.p,{children:"Gyroscopes measure angular velocity around three orthogonal axes:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class Gyroscope:\n    def __init__(self, sensor_id, range_dps=2000.0, resolution=16):\n        self.sensor_id = sensor_id\n        self.range_dps = range_dps  # Range in degrees per second\n        self.resolution = resolution\n        self.max_count = 2**(resolution-1) - 1\n        self.bias = np.zeros(3)  # Bias in dps\n        self.scale_factor = np.array([1.0, 1.0, 1.0])\n        self.drift_rate = np.array([0.01, 0.01, 0.01])  # Drift in dps/hr\n        self.integration_error = np.zeros(3)  # For drift compensation\n\n    def read_raw(self):\n        """Read raw gyroscope data (simulated)"""\n        # Simulate true angular velocity with noise\n        true_angular_vel = np.random.normal(0, 0.5, 3)  # 0.5 dps noise\n\n        # Add bias and drift\n        biased_vel = true_angular_vel + self.bias + self.integration_error\n\n        # Apply scale factor\n        scaled_vel = biased_vel * self.scale_factor\n\n        # Quantization simulation\n        counts = np.round(scaled_vel * self.max_count / self.range_dps)\n        counts = np.clip(counts, -self.max_count, self.max_count)\n        digital_vel = counts * self.range_dps / self.max_count\n\n        return digital_vel\n\n    def calibrate_bias(self, static_readings=100):\n        """Calibrate gyroscope bias while sensor is stationary"""\n        readings = []\n        for _ in range(static_readings):\n            raw = self.read_raw()\n            readings.append(raw)\n\n        self.bias = np.mean(readings, axis=0)\n        return self.bias\n\n    def integrate_for_orientation(self, dt, current_orientation_quat):\n        """Integrate gyroscope data to update orientation"""\n        angular_vel = self.read_raw() - self.bias  # Remove bias\n\n        # Convert angular velocity to quaternion derivative\n        # For quaternion q = [w, x, y, z], q_dot = 0.5 * Omega * q\n        # where Omega is the skew-symmetric matrix of angular velocity\n        omega_norm = np.linalg.norm(angular_vel)\n\n        if omega_norm > 1e-6:  # Avoid division by zero\n            axis = angular_vel / omega_norm\n            angle = omega_norm * dt\n\n            # Create rotation quaternion for this time step\n            sin_half_angle = np.sin(angle / 2)\n            cos_half_angle = np.cos(angle / 2)\n\n            rotation_quat = np.array([\n                cos_half_angle,\n                axis[0] * sin_half_angle,\n                axis[1] * sin_half_angle,\n                axis[2] * sin_half_angle\n            ])\n\n            # Multiply with current orientation\n            new_orientation = self.quaternion_multiply(rotation_quat, current_orientation_quat)\n        else:\n            new_orientation = current_orientation_quat\n\n        return new_orientation / np.linalg.norm(new_orientation)  # Normalize\n\n    def quaternion_multiply(self, q1, q2):\n        """Multiply two quaternions"""\n        w1, x1, y1, z1 = q1\n        w2, x2, y2, z2 = q2\n\n        w = w1*w2 - x1*x2 - y1*y2 - z1*z2\n        x = w1*x2 + x1*w2 + y1*z2 - z1*y2\n        y = w1*y2 - x1*z2 + y1*w2 + z1*x2\n        z = w1*z2 + x1*y2 - y1*x2 + z1*w2\n\n        return np.array([w, x, y, z])\n\n    def detect_rotation(self, threshold=10.0):\n        """Detect significant rotation"""\n        angular_vel = self.read_raw() - self.bias\n        magnitude = np.linalg.norm(angular_vel)\n        return magnitude > threshold\n'})}),"\n",(0,a.jsx)(e.h4,{id:"magnetometers",children:"Magnetometers"}),"\n",(0,a.jsx)(e.p,{children:"Magnetometers provide absolute heading reference:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class Magnetometer:\n    def __init__(self, sensor_id, range_ga=4.0, resolution=16):\n        self.sensor_id = sensor_id\n        self.range_ga = range_ga  # Range in Gauss\n        self.resolution = resolution\n        self.max_count = 2**(resolution-1) - 1\n        self.bias = np.zeros(3)  # Hard iron bias\n        self.soft_iron_matrix = np.eye(3)  # Soft iron distortion matrix\n        self.current_field = np.array([25000, 5000, 40000])  # Earth\'s magnetic field (nT)\n\n    def read_raw(self):\n        """Read raw magnetometer data (simulated)"""\n        # Simulate magnetic field reading with noise\n        noise = np.random.normal(0, 500, 3)  # 500 nT noise\n        true_field = self.current_field + noise\n\n        # Apply distortion (transpose of soft iron matrix)\n        distorted_field = np.dot(self.soft_iron_matrix.T, true_field / 1000.0)  # Convert to Gauss\n\n        # Add bias\n        biased_field = distorted_field + self.bias\n\n        # Quantization\n        counts = np.round(biased_field * self.max_count / self.range_ga)\n        counts = np.clip(counts, -self.max_count, self.max_count)\n        digital_field = counts * self.range_ga / self.max_count\n\n        return digital_field\n\n    def get_heading(self, orientation_quat):\n        """Calculate magnetic heading from magnetometer and orientation"""\n        # Read magnetic field\n        field_reading = self.read_raw()\n\n        # Remove bias\n        corrected_field = field_reading - self.bias\n\n        # Transform to world frame using orientation quaternion\n        world_field = self.quaternion_rotate_vector(orientation_quat, corrected_field)\n\n        # Calculate heading (angle from magnetic north)\n        heading = np.arctan2(world_field[1], world_field[0])\n        return heading\n\n    def quaternion_rotate_vector(self, q, v):\n        """Rotate vector v by quaternion q"""\n        # Convert vector to pure quaternion\n        v_quat = np.array([0, v[0], v[1], v[2]])\n\n        # q * v * q_conj\n        q_conj = np.array([q[0], -q[1], -q[2], -q[3]])\n        temp = self.quaternion_multiply(q, v_quat)\n        rotated_quat = self.quaternion_multiply(temp, q_conj)\n\n        return rotated_quat[1:]  # Return vector part\n\n    def calibrate_hard_iron(self, num_samples=100):\n        """Calibrate hard iron bias by sampling over 360 degrees"""\n        readings = []\n\n        print("Rotate sensor in figure-8 pattern for hard iron calibration...")\n        for i in range(num_samples):\n            reading = self.read_raw()\n            readings.append(reading)\n            time.sleep(0.1)  # 100ms between samples\n\n        readings = np.array(readings)\n        self.bias = np.mean(readings, axis=0)\n        return self.bias\n\n    def quaternion_multiply(self, q1, q2):\n        """Multiply two quaternions"""\n        w1, x1, y1, z1 = q1\n        w2, x2, y2, z2 = q2\n\n        w = w1*w2 - x1*x2 - y1*y2 - z1*z2\n        x = w1*x2 + x1*w2 + y1*z2 - z1*y2\n        y = w1*y2 - x1*z2 + y1*w2 + z1*x2\n        z = w1*z2 + x1*y2 - y1*x2 + z1*w2\n\n        return np.array([w, x, y, z])\n'})}),"\n",(0,a.jsx)(e.h3,{id:"complete-imu-system",children:"Complete IMU System"}),"\n",(0,a.jsx)(e.p,{children:"Combining all sensors into a complete IMU:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class IMU:\n    def __init__(self, sensor_id, position=np.zeros(3), orientation=np.eye(3)):\n        self.sensor_id = sensor_id\n        self.position = position  # Position relative to robot body\n        self.orientation = orientation  # Orientation matrix\n        self.accelerometer = Accelerometer(f"{sensor_id}_accel")\n        self.gyroscope = Gyroscope(f"{sensor_id}_gyro")\n        self.magnetometer = Magnetometer(f"{sensor_id}_mag")\n\n        # State estimation\n        self.orientation_quat = np.array([1.0, 0.0, 0.0, 0.0])  # [w, x, y, z]\n        self.angular_velocity = np.zeros(3)\n        self.linear_acceleration = np.zeros(3)\n        self.gravity = np.array([0, 0, 9.81])\n\n        # Time tracking\n        self.last_update_time = time.time()\n\n    def read_all(self):\n        """Read all IMU sensors"""\n        accel = self.accelerometer.read_raw()\n        gyro = self.gyroscope.read_raw()\n        mag = self.magnetometer.read_raw()\n\n        return {\n            \'accelerometer\': accel,\n            \'gyroscope\': gyro,\n            \'magnetometer\': mag\n        }\n\n    def update_state(self, dt):\n        """Update orientation and other state using sensor fusion"""\n        # Get current readings\n        readings = self.read_all()\n\n        # Update orientation using gyroscope integration\n        self.orientation_quat = self.gyroscope.integrate_for_orientation(\n            dt, self.orientation_quat\n        )\n\n        # Store linear acceleration (remove gravity)\n        accel_with_gravity = readings[\'accelerometer\']\n        gravity_in_sensor_frame = self.rotate_vector_to_sensor_frame(\n            self.gravity, self.orientation_quat\n        )\n        self.linear_acceleration = accel_with_gravity - gravity_in_sensor_frame\n\n        # Store angular velocity\n        self.angular_velocity = readings[\'gyroscope\'] - self.gyroscope.bias\n\n    def rotate_vector_to_sensor_frame(self, vector, quat):\n        """Rotate a vector from world frame to sensor frame using quaternion"""\n        # q_conj * v_quat * q (where v_quat is [0, v_x, v_y, v_z])\n        v_quat = np.array([0, vector[0], vector[1], vector[2]])\n        q_conj = np.array([quat[0], -quat[1], -quat[2], -quat[3]])\n\n        temp = self.gyroscope.quaternion_multiply(q_conj, v_quat)\n        rotated_quat = self.gyroscope.quaternion_multiply(temp, quat)\n\n        return rotated_quat[1:]  # Return vector part\n\n    def get_euler_angles(self):\n        """Convert quaternion to Euler angles (roll, pitch, yaw)"""\n        w, x, y, z = self.orientation_quat\n\n        # Normalize quaternion\n        norm = np.linalg.norm(self.orientation_quat)\n        if norm > 0:\n            w, x, y, z = self.orientation_quat / norm\n\n        # Convert to Euler angles\n        sinr_cosp = 2 * (w * x + y * z)\n        cosr_cosp = 1 - 2 * (x * x + y * y)\n        roll = np.arctan2(sinr_cosp, cosr_cosp)\n\n        sinp = 2 * (w * y - z * x)\n        if abs(sinp) >= 1:\n            pitch = np.sign(sinp) * np.pi / 2  # Use 90 degrees if out of range\n        else:\n            pitch = np.arcsin(sinp)\n\n        siny_cosp = 2 * (w * z + x * y)\n        cosy_cosp = 1 - 2 * (y * y + z * z)\n        yaw = np.arctan2(siny_cosp, cosy_cosp)\n\n        return np.array([roll, pitch, yaw])\n\n    def is_upright(self, threshold=0.1):\n        """Check if sensor is approximately upright"""\n        euler = self.get_euler_angles()\n        return abs(euler[0]) < threshold and abs(euler[1]) < threshold\n'})}),"\n",(0,a.jsx)(e.h2,{id:"attitude-and-heading-reference-systems-ahrs",children:"Attitude and Heading Reference Systems (AHRS)"}),"\n",(0,a.jsx)(e.h3,{id:"complementary-filter-for-attitude-estimation",children:"Complementary Filter for Attitude Estimation"}),"\n",(0,a.jsx)(e.p,{children:"Combining accelerometer, gyroscope, and magnetometer data:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class ComplementaryFilter:\n    def __init__(self, kp_acc=0.5, kp_mag=0.3, ki=0.0):\n        self.kp_acc = kp_acc  # Proportional gain for accelerometer\n        self.kp_mag = kp_mag  # Proportional gain for magnetometer\n        self.ki = ki          # Integral gain\n        self.integral_error = np.zeros(3)\n\n        # Initial orientation (quaternion)\n        self.orientation = np.array([1.0, 0.0, 0.0, 0.0])\n\n        # Bias estimation\n        self.gyro_bias = np.zeros(3)\n\n    def update(self, accel, gyro, mag, dt):\n        """Update orientation estimate using sensor fusion"""\n        # Normalize accelerometer measurement\n        if np.linalg.norm(accel) > 0:\n            accel_norm = accel / np.linalg.norm(accel)\n        else:\n            accel_norm = np.array([0, 0, 1])  # Default to Z-axis if no acceleration\n\n        # Integrate gyroscope measurements\n        gyro_corrected = gyro - self.gyro_bias\n        dq_gyro = self.integrate_gyro(gyro_corrected, dt)\n\n        # Calculate correction from accelerometer\n        gravity_est = self.rotate_vector(np.array([0, 0, 1]), self.orientation)\n        acc_error = np.cross(gravity_est, accel_norm)\n\n        # Apply proportional correction\n        acc_correction = self.kp_acc * acc_error\n        dq_acc = self.vector_to_quaternion(acc_correction)\n\n        # Calculate correction from magnetometer\n        if mag is not None and np.linalg.norm(mag) > 0:\n            mag_norm = mag / np.linalg.norm(mag)\n            # Calculate expected magnetic field in body frame\n            mag_body = self.rotate_vector(mag_norm,\n                                        self.quaternion_conjugate(self.orientation))\n            # Calculate error in horizontal plane\n            mag_error = np.array([\n                mag_body[1],  # East component should be 0\n                -mag_body[0], # North component should align with Y\n                0\n            ])\n            mag_correction = self.kp_mag * mag_error\n            dq_mag = self.vector_to_quaternion(mag_correction)\n        else:\n            dq_mag = np.array([0, 0, 0, 0])\n\n        # Combine corrections\n        dq_total = dq_gyro + dq_acc + dq_mag\n\n        # Normalize and integrate\n        dq_total = dq_total / np.linalg.norm(dq_total) if np.linalg.norm(dq_total) > 0 else dq_total\n        self.orientation = self.quaternion_multiply(\n            self.orientation,\n            dt * dq_total * 0.5\n        )\n\n        # Normalize quaternion\n        self.orientation = self.orientation / np.linalg.norm(self.orientation)\n\n        # Update bias estimation if correction is small (likely due to bias)\n        if np.linalg.norm(acc_error) < 0.1:\n            self.integral_error += acc_error * dt\n            self.gyro_bias += self.ki * self.integral_error\n\n    def integrate_gyro(self, gyro, dt):\n        """Integrate gyroscope to update orientation"""\n        # Convert angular velocity to quaternion derivative\n        gyro_quat = np.array([0, gyro[0], gyro[1], gyro[2]])\n        dq = self.quaternion_multiply(self.orientation, gyro_quat) * 0.5\n        return dq\n\n    def rotate_vector(self, v, q):\n        """Rotate vector v by quaternion q"""\n        # q * [0, v] * q_conj\n        v_quat = np.array([0, v[0], v[1], v[2]])\n        q_conj = self.quaternion_conjugate(q)\n        temp = self.quaternion_multiply(q, v_quat)\n        rotated_quat = self.quaternion_multiply(temp, q_conj)\n        return rotated_quat[1:]\n\n    def quaternion_conjugate(self, q):\n        """Return quaternion conjugate"""\n        return np.array([q[0], -q[1], -q[2], -q[3]])\n\n    def quaternion_multiply(self, q1, q2):\n        """Multiply two quaternions"""\n        w1, x1, y1, z1 = q1\n        w2, x2, y2, z2 = q2\n\n        w = w1*w2 - x1*x2 - y1*y2 - z1*z2\n        x = w1*x2 + x1*w2 + y1*z2 - z1*y2\n        y = w1*y2 - x1*z2 + y1*w2 + z1*x2\n        z = w1*z2 + x1*y2 - y1*x2 + z1*w2\n\n        return np.array([w, x, y, z])\n\n    def vector_to_quaternion(self, v):\n        """Convert 3D vector to quaternion (pure quaternion)"""\n        return np.array([0, v[0], v[1], v[2]])\n'})}),"\n",(0,a.jsx)(e.h3,{id:"extended-kalman-filter-for-imu-fusion",children:"Extended Kalman Filter for IMU Fusion"}),"\n",(0,a.jsx)(e.p,{children:"More sophisticated filtering approach:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class EKFFilter:\n    def __init__(self):\n        # State: [q_w, q_x, q_y, q_z, b_x, b_y, b_z]\n        # where q is orientation quaternion and b is gyroscope bias\n        self.state = np.array([1, 0, 0, 0, 0, 0, 0], dtype=float)\n        self.covariance = np.eye(7) * 0.1  # Initial uncertainty\n\n        # Process noise\n        self.Q = np.diag([1e-4, 1e-4, 1e-4, 1e-4, 1e-6, 1e-6, 1e-6])\n\n        # Measurement noise\n        self.R_acc = np.eye(3) * 0.01  # Accelerometer noise\n        self.R_mag = np.eye(3) * 0.02  # Magnetometer noise\n\n    def predict(self, gyro, dt):\n        """Prediction step using gyroscope measurements"""\n        q = self.state[:4]  # Orientation quaternion\n        b = self.state[4:]  # Gyroscope bias\n\n        # Corrected angular velocity\n        omega = gyro - b\n\n        # Quaternion derivative\n        omega_skew = np.array([\n            [0, -omega[0], -omega[1], -omega[2]],\n            [omega[0], 0, omega[2], -omega[1]],\n            [omega[1], -omega[2], 0, omega[0]],\n            [omega[2], omega[1], -omega[0], 0]\n        ])\n\n        # Update quaternion\n        q_dot = 0.5 * np.dot(omega_skew, q)\n        q_new = q + q_dot * dt\n\n        # Normalize quaternion\n        q_new = q_new / np.linalg.norm(q_new)\n\n        # Update state\n        self.state[:4] = q_new\n\n        # Jacobian of process model\n        F = np.eye(7)\n        F[:4, :4] = self.compute_quaternion_jacobian(omega, dt)\n        # Bias remains constant in prediction\n\n        # Update covariance\n        self.covariance = np.dot(np.dot(F, self.covariance), F.T) + self.Q\n\n    def compute_quaternion_jacobian(self, omega, dt):\n        """Compute Jacobian of quaternion propagation"""\n        norm_omega = np.linalg.norm(omega)\n        if norm_omega < 1e-6:\n            # Small angle approximation\n            return np.eye(4)\n\n        omega_unit = omega / norm_omega\n        cos_half_angle = np.cos(norm_omega * dt / 2)\n        sin_half_angle = np.sin(norm_omega * dt / 2)\n\n        # Jacobian computation\n        F = np.zeros((4, 4))\n        F[0, 0] = cos_half_angle\n        F[0, 1:] = -sin_half_angle * omega_unit * dt / 2\n        F[1:, 0] = sin_half_angle * omega_unit * dt / 2\n\n        # Cross product terms\n        omega_skew = np.array([\n            [0, -omega_unit[2], omega_unit[1]],\n            [omega_unit[2], 0, -omega_unit[0]],\n            [-omega_unit[1], omega_unit[0], 0]\n        ])\n\n        F[1:, 1:] = cos_half_angle * np.eye(3) + \\\n                   (1 - cos_half_angle) * np.outer(omega_unit, omega_unit) + \\\n                   sin_half_angle * omega_skew\n\n        return F\n\n    def update_with_accelerometer(self, accel):\n        """Update step using accelerometer measurement"""\n        if np.linalg.norm(accel) < 1e-6:\n            return\n\n        # Normalize accelerometer\n        z_acc = accel / np.linalg.norm(accel)\n\n        # Expected measurement (gravity in body frame)\n        q = self.state[:4]\n        gravity_body = self.rotate_vector_to_body(np.array([0, 0, 1]), q)\n\n        # Measurement residual\n        y = z_acc - gravity_body\n\n        # Measurement Jacobian\n        H = np.zeros((3, 7))\n        H[:, :4] = self.compute_h_jacobian(q)\n\n        # Kalman gain\n        S = np.dot(np.dot(H, self.covariance), H.T) + self.R_acc\n        K = np.dot(np.dot(self.covariance, H.T), np.linalg.inv(S))\n\n        # Update state and covariance\n        self.state += np.dot(K, y)\n        self.covariance = np.dot((np.eye(7) - np.dot(K, H)), self.covariance)\n\n        # Re-normalize quaternion\n        self.state[:4] = self.state[:4] / np.linalg.norm(self.state[:4])\n\n    def rotate_vector_to_body(self, v, q):\n        """Rotate vector from world to body frame"""\n        # q_conj * [0, v] * q\n        v_quat = np.array([0, v[0], v[1], v[2]])\n        q_conj = np.array([q[0], -q[1], -q[2], -q[3]])\n        temp = self.quaternion_multiply(q_conj, v_quat)\n        rotated = self.quaternion_multiply(temp, q)\n        return rotated[1:]\n\n    def compute_h_jacobian(self, q):\n        """Compute measurement Jacobian for accelerometer"""\n        # Simplified Jacobian computation\n        H = np.zeros((3, 7))\n\n        # For gravity vector [0, 0, 1] rotated to body frame\n        w, x, y, z = q\n        H[0, 0] = 2*(w*y - x*z)\n        H[0, 1] = 2*(x*y + w*z)\n        H[0, 2] = w*w - x*x + y*y - z*z\n        H[0, 3] = 2*(y*z - w*x)\n\n        H[1, 0] = 2*(-w*x - y*z)\n        H[1, 1] = w*w - x*x - y*y + z*z\n        H[1, 2] = 2*(x*y - w*z)\n        H[1, 3] = 2*(w*y + x*z)\n\n        H[2, 0] = 2*(w*z - x*y)\n        H[2, 1] = 2*(w*y + x*z)\n        H[2, 2] = 2*(-w*x + y*z)\n        H[2, 3] = w*w + x*x - y*y - z*z\n\n        return H\n\n    def quaternion_multiply(self, q1, q2):\n        """Multiply two quaternions"""\n        w1, x1, y1, z1 = q1\n        w2, x2, y2, z2 = q2\n\n        w = w1*w2 - x1*x2 - y1*y2 - z1*z2\n        x = w1*x2 + x1*w2 + y1*z2 - z1*y2\n        y = w1*y2 - x1*z2 + y1*w2 + z1*x2\n        z = w1*z2 + x1*y2 - y1*x2 + z1*w2\n\n        return np.array([w, x, y, z])\n'})}),"\n",(0,a.jsx)(e.h2,{id:"joint-position-and-proprioceptive-sensing",children:"Joint Position and Proprioceptive Sensing"}),"\n",(0,a.jsx)(e.h3,{id:"encoder-based-position-sensing",children:"Encoder-Based Position Sensing"}),"\n",(0,a.jsx)(e.p,{children:"Joint encoders provide precise position feedback:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class JointEncoder:\n    def __init__(self, joint_name, type=\'incremental\', resolution=12):\n        self.joint_name = joint_name\n        self.type = type  # \'incremental\' or \'absolute\'\n        self.resolution = resolution  # bits\n        self.max_count = 2**resolution\n        self.current_count = 0\n        self.position = 0.0  # radians\n        self.velocity = 0.0  # rad/s\n        self.acceleration = 0.0  # rad/s^2\n        self.gear_ratio = 1.0\n        self.offset = 0.0\n        self.previous_position = 0.0\n        self.previous_time = time.time()\n\n        # For absolute encoders\n        self.multi_turn_count = 0\n        self.last_raw_count = 0\n\n    def read_raw(self):\n        """Read raw encoder count (simulated)"""\n        # In real hardware, this would read from encoder\n        # For simulation, we\'ll return a changing value\n        import random\n        change = random.uniform(-10, 10)  # Simulate position change\n        self.current_count = (self.current_count + int(change)) % self.max_count\n        return self.current_count\n\n    def update_position(self, dt):\n        """Update position, velocity, and acceleration"""\n        raw_count = self.read_raw()\n\n        if self.type == \'incremental\':\n            # Handle overflow/underflow for incremental encoders\n            if raw_count - self.last_raw_count > self.max_count / 2:\n                # Wrapped around from max to 0\n                self.multi_turn_count -= 1\n            elif raw_count - self.last_raw_count < -self.max_count / 2:\n                # Wrapped around from 0 to max\n                self.multi_turn_count += 1\n\n            # Calculate total count including turns\n            total_count = raw_count + self.multi_turn_count * self.max_count\n            self.last_raw_count = raw_count\n        else:\n            # Absolute encoder - no multi-turn tracking needed\n            total_count = raw_count\n\n        # Convert to radians\n        raw_angle = (total_count / self.max_count) * 2 * np.pi\n        self.position = raw_angle * self.gear_ratio - self.offset\n\n        # Calculate velocity\n        current_time = time.time()\n        if dt > 0:\n            self.velocity = (self.position - self.previous_position) / dt\n            self.acceleration = (self.velocity - self.previous_velocity) / dt if dt > 0 else 0.0\n\n        # Update previous values\n        self.previous_position = self.position\n        self.previous_velocity = self.velocity\n        self.previous_time = current_time\n\n    def calibrate(self, reference_position):\n        """Calibrate encoder offset"""\n        raw_count = self.read_raw()\n        raw_angle = (raw_count / self.max_count) * 2 * np.pi\n        current_angle = raw_angle * self.gear_ratio\n        self.offset = current_angle - reference_position\n\n    def get_position(self):\n        """Get current joint position"""\n        return self.position\n\n    def get_velocity(self):\n        """Get current joint velocity"""\n        return self.velocity\n\n    def get_acceleration(self):\n        """Get current joint acceleration"""\n        return self.acceleration\n\n    def is_at_limit(self, min_pos, max_pos, tolerance=0.01):\n        """Check if joint is at position limits"""\n        return (self.position <= min_pos + tolerance or\n                self.position >= max_pos - tolerance)\n\n    def detect_backlash(self, direction_change_threshold=0.1):\n        """Detect backlash by monitoring direction changes"""\n        # This would require more sophisticated analysis\n        # For now, return a simple backlash indicator\n        if abs(self.velocity) > 0.1:  # Moving\n            if self.velocity * self.previous_velocity < 0:  # Direction changed\n                return True\n        return False\n'})}),"\n",(0,a.jsx)(e.h3,{id:"motor-current-sensing",children:"Motor Current Sensing"}),"\n",(0,a.jsx)(e.p,{children:"Motor current provides information about applied torque:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class MotorCurrentSensor:\n    def __init__(self, motor_name, max_current=10.0, torque_constant=0.1):\n        self.motor_name = motor_name\n        self.max_current = max_current  # Amperes\n        self.torque_constant = torque_constant  # Nm/A\n        self.current = 0.0\n        self.torque = 0.0\n        self.temperature = 25.0\n        self.overcurrent_threshold = 0.8 * max_current\n        self.current_history = deque(maxlen=100)\n\n    def read_current(self):\n        """Read motor current (simulated)"""\n        # Simulate current reading with noise\n        load_factor = np.random.uniform(0.1, 0.9)  # 10-90% of max current\n        noise = np.random.normal(0, 0.05)  # 50mA noise\n        current = load_factor * self.max_current + noise\n        return np.clip(current, -self.max_current, self.max_current)\n\n    def update_torque_estimate(self):\n        """Update torque estimate based on current"""\n        self.current = self.read_current()\n        self.torque = self.current * self.torque_constant\n        self.current_history.append(self.current)\n\n        # Update temperature estimate based on current\n        self.temperature += 0.01 * (self.current**2) * 0.001  # Simplified heating model\n\n        return self.torque\n\n    def detect_overload(self):\n        """Detect motor overload condition"""\n        return abs(self.current) > self.overcurrent_threshold\n\n    def detect_stall(self, current_threshold=0.9 * self.max_current,\n                     duration_threshold=0.1):\n        """Detect motor stall condition"""\n        if abs(self.current) > current_threshold:\n            # Check if high current persists\n            recent_currents = list(self.current_history)[-int(duration_threshold/0.001):]\n            if len(recent_currents) > 0:\n                avg_current = np.mean(np.abs(recent_currents))\n                return avg_current > current_threshold\n        return False\n\n    def get_effort_percentage(self):\n        """Get motor effort as percentage of maximum"""\n        return abs(self.current) / self.max_current * 100.0\n\n    def detect_oscillation(self, threshold=0.5):\n        """Detect oscillatory behavior in current"""\n        if len(self.current_history) < 10:\n            return False\n\n        recent_currents = list(self.current_history)[-10:]\n        current_changes = np.diff(recent_currents)\n\n        # Count direction changes\n        direction_changes = np.sum(current_changes[1:] * current_changes[:-1] < 0)\n        return direction_changes > threshold * len(current_changes)\n'})}),"\n",(0,a.jsx)(e.h2,{id:"sensor-fusion-for-state-estimation",children:"Sensor Fusion for State Estimation"}),"\n",(0,a.jsx)(e.h3,{id:"kalman-filter-for-joint-state-estimation",children:"Kalman Filter for Joint State Estimation"}),"\n",(0,a.jsx)(e.p,{children:"Combining multiple proprioceptive sensors:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class JointStateEstimator:\n    def __init__(self, joint_name):\n        self.joint_name = joint_name\n        self.dt = 0.001  # 1ms update rate\n\n        # State: [position, velocity, acceleration, torque]\n        self.state = np.array([0.0, 0.0, 0.0, 0.0])\n        self.covariance = np.eye(4) * 0.1\n\n        # Process model (constant acceleration model)\n        self.F = np.array([\n            [1, self.dt, 0.5*self.dt**2, 0],\n            [0, 1, self.dt, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1]  # Torque is independent\n        ])\n\n        # Process noise\n        self.Q = np.diag([1e-4, 1e-3, 1e-2, 1e-1])\n\n        # Measurement matrices\n        self.H_pos = np.array([[1, 0, 0, 0]])  # Position measurement\n        self.H_vel = np.array([[0, 1, 0, 0]])  # Velocity measurement\n        self.H_torque = np.array([[0, 0, 0, 1]])  # Torque measurement\n\n        # Measurement noise\n        self.R_pos = np.array([[0.001]])  # 1mrad position noise\n        self.R_vel = np.array([[0.01]])   # 0.01 rad/s velocity noise\n        self.R_torque = np.array([[0.1]]) # 0.1 Nm torque noise\n\n    def predict(self):\n        """Prediction step"""\n        self.state = np.dot(self.F, self.state)\n        self.covariance = np.dot(np.dot(self.F, self.covariance), self.F.T) + self.Q\n\n    def update_position(self, measured_pos, noise=None):\n        """Update with position measurement"""\n        if noise is not None:\n            R = np.array([[noise]])\n        else:\n            R = self.R_pos\n\n        y = np.array([measured_pos - self.state[0]])  # Measurement residual\n        S = np.dot(np.dot(self.H_pos, self.covariance), self.H_pos.T) + R\n        K = np.dot(np.dot(self.covariance, self.H_pos.T), np.linalg.inv(S))\n\n        self.state += np.dot(K, y)\n        self.covariance = np.dot((np.eye(4) - np.dot(K, self.H_pos)), self.covariance)\n\n    def update_velocity(self, measured_vel, noise=None):\n        """Update with velocity measurement"""\n        if noise is not None:\n            R = np.array([[noise]])\n        else:\n            R = self.R_vel\n\n        y = np.array([measured_vel - self.state[1]])  # Measurement residual\n        S = np.dot(np.dot(self.H_vel, self.covariance), self.H_vel.T) + R\n        K = np.dot(np.dot(self.covariance, self.H_vel.T), np.linalg.inv(S))\n\n        self.state += np.dot(K, y)\n        self.covariance = np.dot((np.eye(4) - np.dot(K, self.H_vel)), self.covariance)\n\n    def update_torque(self, measured_torque, noise=None):\n        """Update with torque measurement"""\n        if noise is not None:\n            R = np.array([[noise]])\n        else:\n            R = self.R_torque\n\n        y = np.array([measured_torque - self.state[3]])  # Measurement residual\n        S = np.dot(np.dot(self.H_torque, self.covariance), self.H_torque.T) + R\n        K = np.dot(np.dot(self.covariance, self.H_torque.T), np.linalg.inv(S))\n\n        self.state += np.dot(K, y)\n        self.covariance = np.dot((np.eye(4) - np.dot(K, self.H_torque)), self.covariance)\n\n    def get_state(self):\n        """Get current estimated state"""\n        return {\n            \'position\': self.state[0],\n            \'velocity\': self.state[1],\n            \'acceleration\': self.state[2],\n            \'torque\': self.state[3],\n            \'uncertainty\': np.sqrt(np.diag(self.covariance))\n        }\n'})}),"\n",(0,a.jsx)(e.h3,{id:"whole-body-state-estimation",children:"Whole-Body State Estimation"}),"\n",(0,a.jsx)(e.p,{children:"Estimating the state of the entire humanoid robot:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class WholeBodyStateEstimator:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.joint_estimators = {}\n        self.imus = {}\n        self.contact_sensors = {}\n\n        # Initialize estimators for each joint\n        for joint_name in robot_model.joint_names:\n            self.joint_estimators[joint_name] = JointStateEstimator(joint_name)\n\n        # Initialize IMUs (one on torso, one on each foot, etc.)\n        self.imus[\'torso\'] = IMU(\'torso_imu\')\n        self.imus[\'left_foot\'] = IMU(\'left_foot_imu\')\n        self.imus[\'right_foot\'] = IMU(\'right_foot_imu\')\n\n        # Robot state\n        self.com_position = np.zeros(3)\n        self.com_velocity = np.zeros(3)\n        self.com_acceleration = np.zeros(3)\n        self.support_polygon = None\n        self.zmp = np.zeros(2)  # Zero-Moment Point\n\n    def update_joint_states(self, joint_positions, joint_velocities, joint_torques):\n        """Update all joint state estimators"""\n        for i, joint_name in enumerate(self.robot_model.joint_names):\n            if i < len(joint_positions):\n                # Update joint estimator with measurements\n                self.joint_estimators[joint_name].update_position(joint_positions[i])\n                if i < len(joint_velocities):\n                    self.joint_estimators[joint_name].update_velocity(joint_velocities[i])\n                if i < len(joint_torques):\n                    self.joint_estimators[joint_name].update_torque(joint_torques[i])\n\n                # Prediction step\n                self.joint_estimators[joint_name].predict()\n\n    def update_imu_states(self, imu_data):\n        """Update IMU-based state estimates"""\n        for imu_name, data in imu_data.items():\n            if imu_name in self.imus:\n                # Update IMU state\n                self.imus[imu_name].accelerometer.current_reading = data[\'accel\']\n                self.imus[imu_name].gyroscope.current_reading = data[\'gyro\']\n                if \'mag\' in data:\n                    self.imus[imu_name].magnetometer.current_field = data[\'mag\']\n\n                # Update orientation\n                dt = 0.001  # 1ms\n                self.imus[imu_name].update_state(dt)\n\n    def estimate_center_of_mass(self):\n        """Estimate center of mass position and velocity"""\n        # This would involve forward kinematics and mass distribution\n        # For this example, we\'ll use a simplified approach\n\n        # Get current joint positions from estimators\n        joint_positions = {}\n        joint_velocities = {}\n\n        for joint_name, estimator in self.joint_estimators.items():\n            state = estimator.get_state()\n            joint_positions[joint_name] = state[\'position\']\n            joint_velocities[joint_name] = state[\'velocity\']\n\n        # Calculate CoM using robot model and current configuration\n        # This requires the robot\'s URDF and mass distribution\n        # For now, return a placeholder\n        self.com_position = np.array([0.0, 0.0, 0.8])  # Simplified\n        self.com_velocity = np.array([0.0, 0.0, 0.0])\n        self.com_acceleration = np.array([0.0, 0.0, 0.0])\n\n        return self.com_position, self.com_velocity, self.com_acceleration\n\n    def estimate_zmp(self):\n        """Estimate Zero-Moment Point"""\n        # ZMP = CoM - (CoM_height / g) * CoM_acceleration_xy\n        if self.com_acceleration[2] != 0:  # Avoid division by zero\n            zmp_x = self.com_position[0] - (self.com_position[2] / 9.81) * self.com_acceleration[0]\n            zmp_y = self.com_position[1] - (self.com_position[2] / 9.81) * self.com_acceleration[1]\n            self.zmp = np.array([zmp_x, zmp_y])\n\n        return self.zmp\n\n    def detect_support_polygon(self, contact_data):\n        """Detect support polygon from contact sensors"""\n        # Determine which feet/points are in contact\n        contacts = []\n\n        if contact_data.get(\'left_foot_contact\', False):\n            contacts.append(self.get_foot_position(\'left\'))\n\n        if contact_data.get(\'right_foot_contact\', False):\n            contacts.append(self.get_foot_position(\'right\'))\n\n        if len(contacts) > 0:\n            self.support_polygon = np.array(contacts)\n\n        return self.support_polygon\n\n    def get_foot_position(self, foot_side):\n        """Get foot position in world coordinates"""\n        # This would require forward kinematics\n        # For now, return a placeholder\n        if foot_side == \'left\':\n            return np.array([0.1, -0.1, 0.0])\n        else:\n            return np.array([0.1, 0.1, 0.0])\n\n    def is_balanced(self):\n        """Check if robot is in balance"""\n        if self.support_polygon is None or len(self.support_polygon) == 0:\n            return False\n\n        zmp = self.estimate_zmp()\n\n        # Check if ZMP is within support polygon\n        # Simple check for rectangular support polygon\n        if len(self.support_polygon) >= 3:\n            # Calculate bounding box of support polygon\n            min_x = np.min(self.support_polygon[:, 0])\n            max_x = np.max(self.support_polygon[:, 0])\n            min_y = np.min(self.support_polygon[:, 1])\n            max_y = np.max(self.support_polygon[:, 1])\n\n            return (min_x <= zmp[0] <= max_x and min_y <= zmp[1] <= max_y)\n\n        return False\n'})}),"\n",(0,a.jsx)(e.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,a.jsx)(e.h3,{id:"ros-2-inertial-sensing-node",children:"ROS 2 Inertial Sensing Node"}),"\n",(0,a.jsx)(e.p,{children:"Integrating inertial and proprioceptive sensing with ROS 2:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu, JointState\nfrom geometry_msgs.msg import Vector3, Quaternion\nfrom std_msgs.msg import Header, Float64MultiArray\nfrom tf2_ros import TransformBroadcaster\n\nclass InertialSensingNode(Node):\n    def __init__(self):\n        super().__init__('inertial_sensing_node')\n\n        # Create publishers\n        self.imu_pub = self.create_publisher(Imu, '/imu/data', 10)\n        self.joint_state_pub = self.create_publisher(JointState, '/joint_states_est', 10)\n        self.com_pub = self.create_publisher(Float64MultiArray, '/center_of_mass', 10)\n\n        # Create subscribers\n        self.raw_imu_sub = self.create_subscription(\n            Imu, '/imu/raw', self.raw_imu_callback, 10\n        )\n        self.raw_joint_sub = self.create_subscription(\n            JointState, '/joint_states', self.raw_joint_callback, 10\n        )\n\n        # Initialize sensors and estimators\n        self.imu_filter = ComplementaryFilter()\n        self.whole_body_estimator = WholeBodyStateEstimator(robot_model=None)  # Placeholder\n        self.joint_encoders = {}  # Will be populated from joint names\n\n        # Initialize joint encoders for common humanoid joints\n        humanoid_joints = [\n            'left_hip_roll', 'left_hip_pitch', 'left_hip_yaw',\n            'left_knee', 'left_ankle_pitch', 'left_ankle_roll',\n            'right_hip_roll', 'right_hip_pitch', 'right_hip_yaw',\n            'right_knee', 'right_ankle_pitch', 'right_ankle_roll',\n            'torso_yaw', 'torso_pitch', 'torso_roll',\n            'left_shoulder_pitch', 'left_shoulder_roll', 'left_shoulder_yaw',\n            'left_elbow', 'left_wrist_yaw', 'left_wrist_pitch',\n            'right_shoulder_pitch', 'right_shoulder_roll', 'right_shoulder_yaw',\n            'right_elbow', 'right_wrist_yaw', 'right_wrist_pitch'\n        ]\n\n        for joint_name in humanoid_joints:\n            self.joint_encoders[joint_name] = JointEncoder(joint_name)\n\n        # Timer for state estimation\n        self.estimation_timer = self.create_timer(0.01, self.update_estimation)  # 100Hz\n\n        # TF broadcaster for transforms\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        self.get_logger().info('Inertial sensing node initialized')\n\n    def raw_imu_callback(self, msg):\n        \"\"\"Process raw IMU data\"\"\"\n        # Extract IMU readings\n        accel = np.array([msg.linear_acceleration.x,\n                         msg.linear_acceleration.y,\n                         msg.linear_acceleration.z])\n        gyro = np.array([msg.angular_velocity.x,\n                        msg.angular_velocity.y,\n                        msg.angular_velocity.z])\n        mag = None  # Magnetometer might not be available in all IMU messages\n        if msg.orientation.w != 0:  # Check if orientation is valid\n            # Extract magnetic field from orientation (simplified)\n            # In practice, separate magnetometer data would be used\n            pass\n\n        # Update IMU filter\n        dt = 0.01  # Assuming 100Hz rate\n        self.imu_filter.update(accel, gyro, mag, dt)\n\n    def raw_joint_callback(self, msg):\n        \"\"\"Process raw joint state data\"\"\"\n        # Update joint encoders with raw measurements\n        for i, joint_name in enumerate(msg.name):\n            if joint_name in self.joint_encoders:\n                # Update encoder with raw position\n                self.joint_encoders[joint_name].position = msg.position[i]\n                if i < len(msg.velocity):\n                    self.joint_encoders[joint_name].velocity = msg.velocity[i]\n                if i < len(msg.effort):\n                    # Use effort to estimate torque\n                    self.joint_encoders[joint_name].torque = msg.effort[i]\n\n    def update_estimation(self):\n        \"\"\"Update state estimation and publish results\"\"\"\n        # Update joint state estimates\n        joint_positions = []\n        joint_velocities = []\n        joint_names = []\n\n        for joint_name, encoder in self.joint_encoders.items():\n            if encoder.position is not None:\n                joint_positions.append(encoder.position)\n                joint_velocities.append(encoder.velocity)\n                joint_names.append(joint_name)\n\n        # Update whole body state estimator\n        if joint_positions and joint_velocities:\n            self.whole_body_estimator.update_joint_states(\n                joint_positions, joint_velocities, [0]*len(joint_positions)  # Zero torques for now\n            )\n\n        # Publish estimated joint states\n        joint_state_msg = JointState()\n        joint_state_msg.header.stamp = self.get_clock().now().to_msg()\n        joint_state_msg.header.frame_id = 'base_link'\n        joint_state_msg.name = joint_names\n        joint_state_msg.position = joint_positions\n        joint_state_msg.velocity = joint_velocities\n        # No effort in this example\n\n        self.joint_state_pub.publish(joint_state_msg)\n\n        # Publish IMU data with filtered orientation\n        imu_msg = Imu()\n        imu_msg.header.stamp = self.get_clock().now().to_msg()\n        imu_msg.header.frame_id = 'imu_link'\n\n        # Set orientation from filter\n        orientation = self.imu_filter.orientation\n        imu_msg.orientation = Quaternion(\n            x=float(orientation[1]),\n            y=float(orientation[2]),\n            z=float(orientation[3]),\n            w=float(orientation[0])\n        )\n\n        # Set angular velocity and linear acceleration\n        # These would come from the actual sensors\n        imu_msg.angular_velocity = Vector3(x=0.0, y=0.0, z=0.0)\n        imu_msg.linear_acceleration = Vector3(x=0.0, y=0.0, z=9.81)\n\n        # Set covariance matrices (placeholder values)\n        imu_msg.orientation_covariance = [0.01, 0, 0, 0, 0.01, 0, 0, 0, 0.01]\n        imu_msg.angular_velocity_covariance = [0.01, 0, 0, 0, 0.01, 0, 0, 0, 0.01]\n        imu_msg.linear_acceleration_covariance = [0.01, 0, 0, 0, 0.01, 0, 0, 0, 0.01]\n\n        self.imu_pub.publish(imu_msg)\n\n        # Publish center of mass estimate\n        com_msg = Float64MultiArray()\n        com_msg.data = [0.0, 0.0, 0.8]  # Placeholder CoM position\n        self.com_pub.publish(com_msg)\n\n        # Broadcast TF transforms\n        self.broadcast_transforms()\n\n    def broadcast_transforms(self):\n        \"\"\"Broadcast coordinate frame transforms\"\"\"\n        from geometry_msgs.msg import TransformStamped\n\n        # Create transform from base to IMU\n        t = TransformStamped()\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = 'base_link'\n        t.child_frame_id = 'imu_link'\n\n        t.transform.translation.x = 0.0\n        t.transform.translation.y = 0.0\n        t.transform.translation.z = 0.8  # IMU at torso height\n\n        # Identity rotation (IMU aligned with base frame)\n        t.transform.rotation.x = 0.0\n        t.transform.rotation.y = 0.0\n        t.transform.rotation.z = 0.0\n        t.transform.rotation.w = 1.0\n\n        self.tf_broadcaster.sendTransform(t)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    inertial_node = InertialSensingNode()\n\n    try:\n        rclpy.spin(inertial_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        inertial_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(e.p,{children:"Inertial and proprioceptive sensing systems provide humanoid robots with crucial self-awareness, enabling them to understand their own motion, orientation, and internal configuration. These systems form the foundation for balance control, state estimation, and coordinated movement. Through sensor fusion techniques like complementary filtering and Kalman filtering, multiple sensor modalities are combined to provide robust and accurate state estimates."}),"\n",(0,a.jsx)(e.p,{children:"The integration of these sensors with ROS 2 enables system-wide awareness of the robot's state, supporting higher-level control and planning functions. The next section will explore sensor fusion techniques that combine data from multiple sensing modalities to create comprehensive environmental and self-state representations."})]})}function _(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>i,x:()=>s});var o=t(6540);const a={},r=o.createContext(a);function i(n){const e=o.useContext(r);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:i(n.components),o.createElement(r.Provider,{value:e},n.children)}}}]);