"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[352],{1871:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-3-perception-sensing/computer-vision","title":"Computer Vision for Humanoid Robots","description":"Introduction: Visual Perception in Physical AI","source":"@site/docs/module-3-perception-sensing/computer-vision.md","sourceDirName":"module-3-perception-sensing","slug":"/module-3-perception-sensing/computer-vision","permalink":"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/computer-vision","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-perception-sensing/computer-vision.md","tags":[],"version":"current","frontMatter":{},"sidebar":"textbookSidebar","previous":{"title":"Sensory Modalities in Humanoid Robotics","permalink":"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/sensory-modalities"},"next":{"title":"Tactile Sensing and Haptic Feedback","permalink":"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/tactile-sensing"}}');var s=t(4848),o=t(8453);const a={},r="Computer Vision for Humanoid Robots",c={},l=[{value:"Introduction: Visual Perception in Physical AI",id:"introduction-visual-perception-in-physical-ai",level:2},{value:"Challenges in Humanoid Computer Vision",id:"challenges-in-humanoid-computer-vision",level:3},{value:"Camera Systems for Humanoid Robots",id:"camera-systems-for-humanoid-robots",level:2},{value:"Stereo Vision Systems",id:"stereo-vision-systems",level:3},{value:"RGB-D Vision Systems",id:"rgb-d-vision-systems",level:3},{value:"Object Detection and Recognition",id:"object-detection-and-recognition",level:2},{value:"Deep Learning-Based Object Detection",id:"deep-learning-based-object-detection",level:3},{value:"Feature-Based Object Recognition",id:"feature-based-object-recognition",level:3},{value:"Human Detection and Tracking",id:"human-detection-and-tracking",level:2},{value:"Person Detection and Pose Estimation",id:"person-detection-and-pose-estimation",level:3},{value:"Visual Servoing",id:"visual-servoing",level:2},{value:"Image-Based Visual Servoing",id:"image-based-visual-servoing",level:3},{value:"Real-Time Performance Optimization",id:"real-time-performance-optimization",level:2},{value:"Efficient Processing Pipelines",id:"efficient-processing-pipelines",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"ROS 2 Computer Vision Node",id:"ros-2-computer-vision-node",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"computer-vision-for-humanoid-robots",children:"Computer Vision for Humanoid Robots"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction-visual-perception-in-physical-ai",children:"Introduction: Visual Perception in Physical AI"}),"\n",(0,s.jsx)(n.p,{children:"Computer vision enables humanoid robots to interpret and understand their visual environment, providing crucial information for navigation, manipulation, and human interaction. Unlike traditional computer vision applications that process pre-captured images, humanoid robots must perform real-time visual processing while moving and interacting in dynamic environments. This section explores the specialized computer vision techniques required for humanoid robotics."}),"\n",(0,s.jsx)(n.h3,{id:"challenges-in-humanoid-computer-vision",children:"Challenges in Humanoid Computer Vision"}),"\n",(0,s.jsx)(n.p,{children:"Humanoid robots face unique challenges in visual perception:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ego-motion"}),": The robot's own movement affects visual input"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time constraints"}),": Processing must occur at video frame rates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dynamic environments"}),": Scenes change rapidly with moving objects and people"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource limitations"}),": Embedded systems have limited computational power"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety requirements"}),": Vision systems must be robust and reliable"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"camera-systems-for-humanoid-robots",children:"Camera Systems for Humanoid Robots"}),"\n",(0,s.jsx)(n.h3,{id:"stereo-vision-systems",children:"Stereo Vision Systems"}),"\n",(0,s.jsx)(n.p,{children:"Stereo vision provides depth information essential for 3D scene understanding:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\n\nclass StereoVisionSystem:\n    def __init__(self, left_camera_params, right_camera_params):\n        self.left_cam = self.load_camera_params(left_camera_params)\n        self.right_cam = self.load_camera_params(right_camera_params)\n\n        # Stereo rectification parameters\n        self.R1, self.R2, self.P1, self.P2, self.Q = None, None, None, None, None\n        self.disparity_to_depth_map = None\n\n        # Initialize stereo matcher\n        self.stereo_matcher = cv2.StereoSGBM_create(\n            minDisparity=0,\n            numDisparities=16*10,  # Must be divisible by 16\n            blockSize=5,\n            P1=8 * 3 * 5**2,\n            P2=32 * 3 * 5**2,\n            disp12MaxDiff=1,\n            uniquenessRatio=15,\n            speckleWindowSize=0,\n            speckleRange=2,\n            preFilterCap=63,\n            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n        )\n\n    def load_camera_params(self, params_file):\n        """Load camera intrinsic and extrinsic parameters"""\n        # In practice, this would load from calibration file\n        return params_file\n\n    def stereo_rectify(self):\n        """Compute stereo rectification parameters"""\n        # This would typically be computed during calibration\n        # For this example, we\'ll use placeholder values\n        size = (640, 480)  # Image size\n\n        # Compute rectification transforms\n        self.R1, self.R2, self.P1, self.P2, self.Q = cv2.stereoRectify(\n            self.left_cam[\'camera_matrix\'], self.left_cam[\'dist_coeffs\'],\n            self.right_cam[\'camera_matrix\'], self.right_cam[\'dist_coeffs\'],\n            size, self.left_cam[\'R\'], self.left_cam[\'T\'],\n            flags=cv2.CALIB_ZERO_DISPARITY, alpha=-1\n        )\n\n    def compute_depth_map(self, left_image, right_image):\n        """Compute depth map from stereo pair"""\n        # Convert to grayscale if needed\n        if len(left_image.shape) == 3:\n            left_gray = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n            right_gray = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n        else:\n            left_gray = left_image\n            right_gray = right_image\n\n        # Compute disparity\n        disparity = self.stereo_matcher.compute(left_gray, right_gray).astype(np.float32)\n\n        # Convert disparity to depth\n        # Using the Q matrix from stereoRectify\n        # Points in 3D: (X, Y, Z) = (x, y, disparity) * Q\n        # We only need the Z component (depth)\n        if self.Q is not None:\n            # Apply Q matrix transformation\n            points_4d = cv2.reprojectImageTo3D(disparity, self.Q)\n            depth_map = points_4d[:, :, 2]  # Z component is depth\n        else:\n            # Use simplified formula if Q matrix not available\n            baseline = 0.1  # Distance between cameras (m)\n            focal_length = self.left_cam[\'camera_matrix\'][0, 0]\n            depth_map = (baseline * focal_length) / (disparity / 16.0 + 1e-6)\n\n        return depth_map\n\n    def get_3d_point(self, u, v, disparity):\n        """Convert image coordinates + disparity to 3D world coordinates"""\n        if self.Q is None:\n            return None\n\n        # Use Q matrix to convert disparity to 3D coordinates\n        disparity_value = disparity[v, u] if disparity.ndim > 1 else disparity\n        if disparity_value <= 0:\n            return None\n\n        # Reproject to 3D\n        points_4d = cv2.reprojectImageTo3D(np.array([[disparity_value]]), self.Q)\n        x, y, z = points_4d[0, 0]\n        return np.array([x, y, z])\n'})}),"\n",(0,s.jsx)(n.h3,{id:"rgb-d-vision-systems",children:"RGB-D Vision Systems"}),"\n",(0,s.jsx)(n.p,{children:"RGB-D cameras provide both color and depth information:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class RGBDVisionSystem:\n    def __init__(self, camera_params):\n        self.camera_params = camera_params\n        self.fx = camera_params['fx']\n        self.fy = camera_params['fy']\n        self.cx = camera_params['cx']\n        self.cy = camera_params['cy']\n\n    def depth_to_point_cloud(self, depth_image, color_image=None):\n        \"\"\"Convert depth image to 3D point cloud\"\"\"\n        height, width = depth_image.shape\n        points = []\n        colors = []\n\n        for v in range(height):\n            for u in range(width):\n                z = depth_image[v, u] / 1000.0  # Convert mm to m\n                if z > 0 and z < 5.0:  # Valid depth range\n                    # Convert pixel coordinates to 3D world coordinates\n                    x = (u - self.cx) * z / self.fx\n                    y = (v - self.cy) * z / self.fy\n\n                    points.append([x, y, z])\n\n                    if color_image is not None:\n                        color = color_image[v, u]\n                        colors.append(color)\n\n        return np.array(points), np.array(colors) if colors else None\n\n    def segment_objects(self, depth_image, color_image):\n        \"\"\"Segment objects based on depth and color\"\"\"\n        # Create mask for valid depth values\n        valid_depth = (depth_image > 100) & (depth_image < 3000)  # 10cm to 3m\n        valid_depth = valid_depth.astype(np.uint8) * 255\n\n        # Apply morphological operations to clean up the mask\n        kernel = np.ones((5, 5), np.uint8)\n        valid_depth = cv2.morphologyEx(valid_depth, cv2.MORPH_CLOSE, kernel)\n        valid_depth = cv2.morphologyEx(valid_depth, cv2.MORPH_OPEN, kernel)\n\n        # Find connected components (potential objects)\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(valid_depth)\n\n        objects = []\n        for i in range(1, num_labels):  # Skip background (label 0)\n            # Create mask for current component\n            mask = (labels == i).astype(np.uint8) * 255\n\n            # Calculate object properties\n            area = stats[i, cv2.CC_STAT_AREA]\n            if area > 1000:  # Filter small components\n                # Calculate bounding box\n                x, y, w, h = stats[i, cv2.CC_STAT_LEFT:cv2.CC_STAT_TOP+2]\n\n                # Calculate average depth in object region\n                avg_depth = np.mean(depth_image[labels == i]) / 1000.0  # Convert to meters\n\n                objects.append({\n                    'bbox': (x, y, w, h),\n                    'area': area,\n                    'avg_depth': avg_depth,\n                    'centroid': centroids[i],\n                    'mask': mask\n                })\n\n        return objects\n\n    def detect_planes(self, depth_image):\n        \"\"\"Detect planar surfaces using RANSAC\"\"\"\n        # Convert depth image to point cloud\n        points, _ = self.depth_to_point_cloud(depth_image)\n\n        if len(points) < 1000:  # Need sufficient points for RANSAC\n            return []\n\n        # RANSAC plane detection\n        planes = []\n        remaining_points = points.copy()\n\n        for _ in range(5):  # Find up to 5 planes\n            if len(remaining_points) < 100:\n                break\n\n            # Random sample for RANSAC\n            best_inliers = []\n            best_plane = None\n\n            for _ in range(100):  # RANSAC iterations\n                # Sample 3 random points\n                sample_indices = np.random.choice(len(remaining_points), 3, replace=False)\n                sample_points = remaining_points[sample_indices]\n\n                # Fit plane to 3 points\n                # Plane equation: ax + by + cz + d = 0\n                v1 = sample_points[1] - sample_points[0]\n                v2 = sample_points[2] - sample_points[0]\n                normal = np.cross(v1, v2)\n                if np.linalg.norm(normal) < 1e-6:\n                    continue\n\n                normal = normal / np.linalg.norm(normal)\n                d = -np.dot(normal, sample_points[0])\n\n                # Count inliers\n                distances = np.abs(np.dot(remaining_points, normal) + d)\n                inliers = remaining_points[distances < 0.02]  # 2cm threshold\n\n                if len(inliers) > len(best_inliers):\n                    best_inliers = inliers\n                    best_plane = (normal, d)\n\n            if best_plane is not None and len(best_inliers) > 500:\n                planes.append({\n                    'normal': best_plane[0],\n                    'd': best_plane[1],\n                    'inliers': best_inliers\n                })\n\n                # Remove inliers from remaining points\n                distances = np.abs(np.dot(remaining_points, best_plane[0]) + best_plane[1])\n                remaining_points = remaining_points[distances >= 0.02]\n\n        return planes\n"})}),"\n",(0,s.jsx)(n.h2,{id:"object-detection-and-recognition",children:"Object Detection and Recognition"}),"\n",(0,s.jsx)(n.h3,{id:"deep-learning-based-object-detection",children:"Deep Learning-Based Object Detection"}),"\n",(0,s.jsx)(n.p,{children:"Modern humanoid robots increasingly use deep learning for object detection:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torchvision.transforms as transforms\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\n\nclass DeepObjectDetector:\n    def __init__(self, model_path=None, confidence_threshold=0.5):\n        self.confidence_threshold = confidence_threshold\n\n        # Load pre-trained model\n        if model_path:\n            self.model = torch.load(model_path)\n        else:\n            self.model = fasterrcnn_resnet50_fpn(pretrained=True)\n\n        self.model.eval()\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n        # COCO dataset class names (first 20 for example)\n        self.class_names = [\n            '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n            'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow'\n        ]\n\n    def detect_objects(self, image):\n        \"\"\"Detect objects in image using deep learning model\"\"\"\n        # Preprocess image\n        image_tensor = self.transform(image).unsqueeze(0)\n\n        with torch.no_grad():\n            predictions = self.model(image_tensor)\n\n        # Extract results\n        boxes = predictions[0]['boxes'].cpu().numpy()\n        labels = predictions[0]['labels'].cpu().numpy()\n        scores = predictions[0]['scores'].cpu().numpy()\n\n        # Filter by confidence threshold\n        valid_indices = scores >= self.confidence_threshold\n        boxes = boxes[valid_indices]\n        labels = labels[valid_indices]\n        scores = scores[valid_indices]\n\n        # Create detection results\n        detections = []\n        for box, label, score in zip(boxes, labels, scores):\n            detections.append({\n                'bbox': box,  # [x1, y1, x2, y2]\n                'label': self.class_names[label] if label < len(self.class_names) else f'object_{label}',\n                'confidence': score,\n                'center': ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)  # Center coordinates\n            })\n\n        return detections\n\n    def track_objects(self, image, previous_detections):\n        \"\"\"Simple object tracking using detection matching\"\"\"\n        current_detections = self.detect_objects(image)\n\n        # For each previous detection, find the closest current detection\n        tracked_objects = []\n        used_current = set()\n\n        for prev_det in previous_detections:\n            min_dist = float('inf')\n            best_match = None\n\n            for i, curr_det in enumerate(current_detections):\n                if i in used_current:\n                    continue\n\n                # Calculate distance between centers\n                dist = np.sqrt((prev_det['center'][0] - curr_det['center'][0])**2 +\n                              (prev_det['center'][1] - curr_det['center'][1])**2)\n\n                if dist < min_dist and dist < 50:  # 50 pixel threshold\n                    min_dist = dist\n                    best_match = i\n\n            if best_match is not None:\n                tracked_objects.append({\n                    **current_detections[best_match],\n                    'id': prev_det.get('id', len(tracked_objects))\n                })\n                used_current.add(best_match)\n            else:\n                # Object lost\n                tracked_objects.append({\n                    **prev_det,\n                    'tracked': False\n                })\n\n        # Add new detections\n        for i, det in enumerate(current_detections):\n            if i not in used_current:\n                tracked_objects.append({\n                    **det,\n                    'id': len(tracked_objects)\n                })\n\n        return tracked_objects\n"})}),"\n",(0,s.jsx)(n.h3,{id:"feature-based-object-recognition",children:"Feature-Based Object Recognition"}),"\n",(0,s.jsx)(n.p,{children:"Traditional computer vision methods remain important for certain applications:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class FeatureBasedRecognizer:\n    def __init__(self):\n        self.sift = cv2.SIFT_create()\n        self.bf_matcher = cv2.BFMatcher()\n        self.known_objects = {}  # Dictionary of known object descriptors\n\n    def extract_features(self, image):\n        \"\"\"Extract SIFT features from image\"\"\"\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n        keypoints, descriptors = self.sift.detectAndCompute(gray, None)\n        return keypoints, descriptors\n\n    def register_object(self, object_name, template_image):\n        \"\"\"Register a known object with its features\"\"\"\n        keypoints, descriptors = self.extract_features(template_image)\n        self.known_objects[object_name] = {\n            'keypoints': keypoints,\n            'descriptors': descriptors\n        }\n\n    def recognize_object(self, image, min_matches=10):\n        \"\"\"Recognize objects in image using feature matching\"\"\"\n        img_keypoints, img_descriptors = self.extract_features(image)\n\n        if img_descriptors is None:\n            return []\n\n        recognized_objects = []\n\n        for obj_name, obj_data in self.known_objects.items():\n            if obj_data['descriptors'] is None:\n                continue\n\n            # Match features\n            matches = self.bf_matcher.knnMatch(\n                obj_data['descriptors'], img_descriptors, k=2\n            )\n\n            # Apply Lowe's ratio test\n            good_matches = []\n            for match_pair in matches:\n                if len(match_pair) == 2:\n                    m, n = match_pair\n                    if m.distance < 0.75 * n.distance:\n                        good_matches.append(m)\n\n            if len(good_matches) >= min_matches:\n                # Estimate object location using homography\n                src_pts = np.float32([obj_data['keypoints'][m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n                dst_pts = np.float32([img_keypoints[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n\n                # Find homography matrix\n                if len(src_pts) >= 4:\n                    H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n                    if H is not None:\n                        recognized_objects.append({\n                            'name': obj_name,\n                            'matches': len(good_matches),\n                            'confidence': len(good_matches) / min_matches\n                        })\n\n        return recognized_objects\n\n    def estimate_object_pose(self, object_name, image):\n        \"\"\"Estimate 3D pose of known object\"\"\"\n        if object_name not in self.known_objects:\n            return None\n\n        obj_data = self.known_objects[object_name]\n        img_keypoints, img_descriptors = self.extract_features(image)\n\n        if img_descriptors is None:\n            return None\n\n        # Match features\n        matches = self.bf_matcher.knnMatch(\n            obj_data['descriptors'], img_descriptors, k=2\n        )\n\n        # Apply Lowe's ratio test\n        good_matches = []\n        for match_pair in matches:\n            if len(match_pair) == 2:\n                m, n = match_pair\n                if m.distance < 0.75 * n.distance:\n                    good_matches.append(m)\n\n        if len(good_matches) < 10:\n            return None\n\n        # Get corresponding points\n        obj_pts = np.float32([obj_data['keypoints'][m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n        img_pts = np.float32([img_keypoints[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n\n        # For pose estimation, we need 3D object points\n        # This requires a 3D model of the object\n        # For now, we'll return the 2D transformation\n        H, mask = cv2.findHomography(obj_pts, img_pts, cv2.RANSAC, 5.0)\n\n        return {\n            'homography': H,\n            'matches': len(good_matches),\n            'inliers': np.sum(mask) if mask is not None else 0\n        }\n"})}),"\n",(0,s.jsx)(n.h2,{id:"human-detection-and-tracking",children:"Human Detection and Tracking"}),"\n",(0,s.jsx)(n.h3,{id:"person-detection-and-pose-estimation",children:"Person Detection and Pose Estimation"}),"\n",(0,s.jsx)(n.p,{children:"Humanoid robots need to detect and understand human poses:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class HumanDetector:\n    def __init__(self):\n        # Load Haar cascade for face detection\n        self.face_cascade = cv2.CascadeClassifier(\n            cv2.data.haarcascades + \'haarcascade_frontalface_default.xml\'\n        )\n\n        # For pose estimation, we\'ll use a simple body part detection approach\n        # In practice, you might use OpenPose, MediaPipe, or similar\n        self.body_part_cascades = {\n            \'upper_body\': cv2.CascadeClassifier(\'upper_body_cascade.xml\')  # Placeholder\n        }\n\n    def detect_faces(self, image):\n        """Detect faces in image"""\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        faces = self.face_cascade.detectMultiScale(\n            gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30)\n        )\n        return faces\n\n    def detect_humans_simple(self, image):\n        """Simple human detection using HOG and SVM"""\n        # HOG descriptor\n        hog = cv2.HOGDescriptor()\n        hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n\n        # Detect people\n        boxes, weights = hog.detectMultiScale(image, winStride=(8, 8))\n\n        humans = []\n        for (x, y, w, h), weight in zip(boxes, weights):\n            humans.append({\n                \'bbox\': (x, y, w, h),\n                \'confidence\': float(weight),\n                \'center\': (x + w//2, y + h//2)\n            })\n\n        return humans\n\n    def estimate_head_orientation(self, face_bbox, image):\n        """Estimate head orientation from face bounding box"""\n        x, y, w, h = face_bbox\n\n        # Extract face region\n        face_region = image[y:y+h, x:x+w]\n\n        # Simple method: track eye positions\n        # In practice, use facial landmark detection\n        gray_face = cv2.cvtColor(face_region, cv2.COLOR_BGR2GRAY)\n\n        # Use Haar cascades for eyes\n        eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \'haarcascade_eye.xml\')\n        eyes = eye_cascade.detectMultiScale(gray_face)\n\n        if len(eyes) >= 2:\n            # Calculate relative positions of eyes for orientation estimation\n            eye_centers = []\n            for (ex, ey, ew, eh) in eyes:\n                eye_centers.append((x + ex + ew//2, y + ey + eh//2))\n\n            if len(eye_centers) >= 2:\n                # Estimate orientation based on eye positions\n                eye1, eye2 = eye_centers[:2]\n                dx = eye2[0] - eye1[0]\n                dy = eye2[1] - eye1[1]\n                angle = np.arctan2(dy, dx) * 180 / np.pi\n                return angle\n\n        return 0  # Default: frontal face\n\n    def track_person(self, image, person_id, last_position):\n        """Track person using optical flow"""\n        # Convert to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Use Lucas-Kanade optical flow for tracking\n        if not hasattr(self, \'lk_params\'):\n            self.lk_params = dict(\n                winSize=(15, 15),\n                maxLevel=2,\n                criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n            )\n\n        # If this is the first frame for tracking, initialize points\n        if not hasattr(self, \'tracked_points\') or person_id not in self.tracked_points:\n            # Use face or person bounding box center as initial point\n            x, y = last_position\n            self.tracked_points = {person_id: np.array([[x, y]], dtype=np.float32)}\n            return (x, y)\n\n        # Calculate optical flow\n        new_points, status, error = cv2.calcOpticalFlowPyrLK(\n            getattr(self, \'prev_gray\', gray),\n            gray,\n            self.tracked_points[person_id],\n            None,\n            **self.lk_params\n        )\n\n        # Update tracked points\n        if status[0][0] == 1:\n            self.tracked_points[person_id] = new_points\n            self.prev_gray = gray\n            return (int(new_points[0][0][0]), int(new_points[0][0][1]))\n\n        # If tracking failed, return last known position\n        return last_position\n'})}),"\n",(0,s.jsx)(n.h2,{id:"visual-servoing",children:"Visual Servoing"}),"\n",(0,s.jsx)(n.h3,{id:"image-based-visual-servoing",children:"Image-Based Visual Servoing"}),"\n",(0,s.jsx)(n.p,{children:"Visual servoing enables robots to control their motion based on visual feedback:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class VisualServoController:\n    def __init__(self, camera_params, control_gain=1.0):\n        self.camera_params = camera_params\n        self.K = control_gain  # Control gain\n        self.feature_history = []  # Store feature history for stability\n\n    def compute_image_jacobian(self, feature_point, depth):\n        """Compute image Jacobian for a 2D feature point"""\n        u, v = feature_point\n        fx, fy = self.camera_params[\'fx\'], self.camera_params[\'fy\']\n        cx, cy = self.camera_params[\'cx\'], self.camera_params[\'cy\']\n\n        # Image Jacobian matrix (2x6) relating image velocity to camera velocity\n        # [du/dt, dv/dt]^T = L * [twist]^T\n        # where twist = [linear_vel; angular_vel] (6x1)\n        L = np.zeros((2, 6))\n\n        # Linear velocity terms\n        L[0, 0] = -fx / depth  # du/dtx\n        L[0, 1] = 0           # du/dty\n        L[0, 2] = (u - cx) / depth * fx / depth  # du/dtz\n        L[1, 0] = 0           # dv/dtx\n        L[1, 1] = -fy / depth  # dv/dty\n        L[1, 2] = (v - cy) / depth * fy / depth  # dv/dtz\n\n        # Angular velocity terms\n        L[0, 3] = (u - cx) * (v - cy) / depth  # du/dwx\n        L[0, 4] = -(fx + (u - cx)**2 / fx) / depth  # du/dwy\n        L[0, 5] = (v - cy) / fx  # du/dwz\n        L[1, 3] = fy + (v - cy)**2 / fy  # dv/dwx\n        L[1, 4] = -(u - cx) * (v - cy) / fy  # dv/dwy\n        L[1, 5] = -(u - cx) / fy  # dv/dwz\n\n        return L\n\n    def image_based_servo(self, current_features, desired_features, depth_estimates):\n        """Compute camera velocity to move features to desired positions"""\n        if len(current_features) != len(desired_features):\n            raise ValueError("Number of current and desired features must match")\n\n        # Compute feature errors\n        errors = []\n        for curr, desired in zip(current_features, desired_features):\n            error = np.array(desired) - np.array(curr)\n            errors.extend(error)\n\n        error_vector = np.array(errors)\n\n        # Compute combined Jacobian\n        J = []\n        for i, (feature, depth) in enumerate(zip(current_features, depth_estimates)):\n            feature_jac = self.compute_image_jacobian(feature, depth)\n            J.append(feature_jac)\n\n        J_combined = np.vstack(J)\n\n        # Compute camera velocity command\n        # v = -K * J^+ * e (where J^+ is pseudoinverse)\n        try:\n            J_pinv = np.linalg.pinv(J_combined)\n            camera_velocity = -self.K * J_pinv @ error_vector\n        except np.linalg.LinAlgError:\n            # If Jacobian is singular, use damped least squares\n            damping = 0.01\n            J_pinv = J_combined.T @ np.linalg.inv(J_combined @ J_combined.T + damping**2 * np.eye(J_combined.shape[0]))\n            camera_velocity = -self.K * J_pinv @ error_vector\n\n        return camera_velocity, error_vector\n\n    def position_based_servo(self, current_pose, desired_pose):\n        """Compute velocity to move camera to desired pose"""\n        # Position error\n        pos_error = desired_pose[:3] - current_pose[:3]\n\n        # Orientation error (using angle-axis representation)\n        current_rot = current_pose[3:]\n        desired_rot = desired_pose[3:]\n\n        # Convert to rotation matrices for easier computation\n        # For simplicity, we\'ll use a basic approach\n        # In practice, use proper rotation representation\n        rot_error = desired_rot - current_rot\n\n        # Combine position and orientation errors\n        pose_error = np.concatenate([pos_error, rot_error])\n\n        # Compute velocity command\n        velocity = -self.K * pose_error\n\n        return velocity, pose_error\n'})}),"\n",(0,s.jsx)(n.h2,{id:"real-time-performance-optimization",children:"Real-Time Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"efficient-processing-pipelines",children:"Efficient Processing Pipelines"}),"\n",(0,s.jsx)(n.p,{children:"Real-time computer vision requires careful optimization:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import threading\nimport queue\nfrom collections import deque\n\nclass RealTimeVisionProcessor:\n    def __init__(self, max_fps=30):\n        self.max_fps = max_fps\n        self.frame_interval = 1.0 / max_fps\n        self.input_queue = queue.Queue(maxsize=2)  # Only keep most recent frames\n        self.output_queue = queue.Queue(maxsize=10)\n        self.running = False\n        self.processing_thread = None\n\n        # Processing pipeline stages\n        self.pipeline_stages = {\n            \'preprocessing\': self.preprocess_frame,\n            \'detection\': self.detect_objects,\n            \'tracking\': self.track_objects,\n            \'postprocessing\': self.postprocess_results\n        }\n\n    def start_processing(self):\n        """Start the vision processing pipeline"""\n        self.running = True\n        self.processing_thread = threading.Thread(target=self._processing_loop)\n        self.processing_thread.start()\n\n    def stop_processing(self):\n        """Stop the vision processing pipeline"""\n        self.running = False\n        if self.processing_thread:\n            self.processing_thread.join()\n\n    def _processing_loop(self):\n        """Main processing loop"""\n        last_time = time.time()\n\n        while self.running:\n            start_time = time.time()\n\n            try:\n                # Get frame from input queue\n                frame = self.input_queue.get_nowait()\n            except queue.Empty:\n                time.sleep(0.001)  # Brief pause if no frame available\n                continue\n\n            # Process frame through pipeline\n            results = self.process_frame(frame)\n\n            # Add results to output queue\n            try:\n                self.output_queue.put_nowait(results)\n            except queue.Full:\n                pass  # Drop old results if output queue is full\n\n            # Maintain frame rate\n            elapsed = time.time() - start_time\n            sleep_time = max(0, self.frame_interval - elapsed)\n            time.sleep(sleep_time)\n\n    def process_frame(self, frame):\n        """Process a single frame through the pipeline"""\n        # Preprocessing\n        processed_frame = self.pipeline_stages[\'preprocessing\'](frame)\n\n        # Object detection\n        detections = self.pipeline_stages[\'detection\'](processed_frame)\n\n        # Object tracking (if needed)\n        tracked_objects = self.pipeline_stages[\'tracking\'](detections)\n\n        # Postprocessing\n        results = self.pipeline_stages[\'postprocessing\'](tracked_objects)\n\n        return results\n\n    def preprocess_frame(self, frame):\n        """Preprocess frame for faster processing"""\n        # Resize frame to reduce computational load\n        height, width = frame.shape[:2]\n        scale_factor = min(1.0, 640.0 / max(height, width))  # Max 640px in any dimension\n        new_width = int(width * scale_factor)\n        new_height = int(height * scale_factor)\n\n        resized_frame = cv2.resize(frame, (new_width, new_height))\n        return resized_frame\n\n    def detect_objects(self, frame):\n        """Detect objects in frame (placeholder)"""\n        # In practice, this would call the actual detection algorithm\n        return []\n\n    def track_objects(self, detections):\n        """Track objects across frames"""\n        # Implement tracking logic here\n        return detections\n\n    def postprocess_results(self, results):\n        """Postprocess detection results"""\n        # Add any final processing steps\n        return results\n\n    def submit_frame(self, frame):\n        """Submit a frame for processing"""\n        try:\n            self.input_queue.put_nowait(frame)\n        except queue.Full:\n            pass  # Drop frame if queue is full (older frames are more important)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,s.jsx)(n.h3,{id:"ros-2-computer-vision-node",children:"ROS 2 Computer Vision Node"}),"\n",(0,s.jsx)(n.p,{children:"Integrating computer vision with ROS 2 enables system-wide perception:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom cv_bridge import CvBridge\n\nclass VisionNode(Node):\n    def __init__(self):\n        super().__init__('vision_node')\n\n        # Initialize OpenCV bridge\n        self.cv_bridge = CvBridge()\n\n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/camera_info',\n            self.camera_info_callback,\n            10\n        )\n\n        # Create publishers\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            '/vision/detections',\n            10\n        )\n\n        self.debug_image_pub = self.create_publisher(\n            Image,\n            '/vision/debug_image',\n            10\n        )\n\n        # Initialize vision components\n        self.object_detector = DeepObjectDetector()\n        self.human_detector = HumanDetector()\n        self.camera_params = None\n\n        self.get_logger().info('Vision node initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming image messages\"\"\"\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n        except Exception as e:\n            self.get_logger().error(f'Error converting image: {e}')\n            return\n\n        # Perform object detection\n        detections = self.object_detector.detect_objects(cv_image)\n\n        # Perform human detection\n        humans = self.human_detector.detect_humans_simple(cv_image)\n\n        # Combine results\n        all_detections = []\n        for det in detections:\n            detection_msg = self.create_detection_msg(det, msg.header)\n            all_detections.append(detection_msg)\n\n        for human in humans:\n            human_det = {\n                'bbox': human['bbox'],\n                'label': 'person',\n                'confidence': human['confidence'],\n                'center': human['center']\n            }\n            detection_msg = self.create_detection_msg(human_det, msg.header)\n            all_detections.append(detection_msg)\n\n        # Create and publish detection array\n        detection_array = Detection2DArray()\n        detection_array.header = msg.header\n        detection_array.detections = all_detections\n\n        self.detection_pub.publish(detection_array)\n\n        # Create debug image with detections\n        debug_image = self.draw_detections(cv_image, detections, humans)\n        debug_msg = self.cv_bridge.cv2_to_imgmsg(debug_image, encoding='bgr8')\n        debug_msg.header = msg.header\n        self.debug_image_pub.publish(debug_msg)\n\n    def camera_info_callback(self, msg):\n        \"\"\"Update camera parameters from camera info\"\"\"\n        self.camera_params = {\n            'fx': msg.k[0],  # Focal length x\n            'fy': msg.k[4],  # Focal length y\n            'cx': msg.k[2],  # Principal point x\n            'cy': msg.k[5],  # Principal point y\n            'width': msg.width,\n            'height': msg.height\n        }\n\n    def create_detection_msg(self, detection, header):\n        \"\"\"Create vision_msgs/Detection2D message\"\"\"\n        detection_msg = Detection2D()\n        detection_msg.header = header\n\n        # Bounding box\n        bbox = detection['bbox']\n        detection_msg.bbox.center.x = (bbox[0] + bbox[2]) / 2.0\n        detection_msg.bbox.center.y = (bbox[1] + bbox[3]) / 2.0\n        detection_msg.bbox.size_x = bbox[2]\n        detection_msg.bbox.size_y = bbox[3]\n\n        # Results (classification)\n        result = ObjectHypothesisWithPose()\n        result.hypothesis.class_id = detection['label']\n        result.hypothesis.score = detection['confidence']\n        detection_msg.results = [result]\n\n        return detection_msg\n\n    def draw_detections(self, image, detections, humans):\n        \"\"\"Draw detection results on image for debugging\"\"\"\n        result_image = image.copy()\n\n        # Draw object detections\n        for det in detections:\n            bbox = det['bbox']\n            x1, y1, x2, y2 = map(int, bbox)\n            cv2.rectangle(result_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            cv2.putText(result_image, f\"{det['label']}: {det['confidence']:.2f}\",\n                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n        # Draw human detections\n        for human in humans:\n            x, y, w, h = human['bbox']\n            cv2.rectangle(result_image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n            cv2.putText(result_image, f\"person: {human['confidence']:.2f}\",\n                       (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n\n        return result_image\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vision_node = VisionNode()\n\n    try:\n        rclpy.spin(vision_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vision_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"Computer vision systems provide humanoid robots with the ability to perceive and understand their visual environment. From stereo vision and RGB-D sensing to deep learning-based object detection and visual servoing, these technologies enable robots to navigate, manipulate objects, and interact with humans effectively."}),"\n",(0,s.jsx)(n.p,{children:"The integration of computer vision with ROS 2 and real-time processing requirements presents unique challenges that require careful consideration of computational efficiency, sensor fusion, and system reliability. The next section will explore tactile sensing and haptic feedback systems that provide robots with the sense of touch."})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const s={},o=i.createContext(s);function a(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);