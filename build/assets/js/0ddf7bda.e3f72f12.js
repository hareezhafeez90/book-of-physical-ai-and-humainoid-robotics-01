"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[315],{4513:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>t,metadata:()=>a,toc:()=>c});var s=i(4848),o=i(8453);const t={},r="Module 3: Perception and Sensing Systems",a={id:"module-3-perception-sensing/index",title:"Module 3: Perception and Sensing Systems",description:"Introduction: The Robotic Senses",source:"@site/docs/module-3-perception-sensing/index.md",sourceDirName:"module-3-perception-sensing",slug:"/module-3-perception-sensing/",permalink:"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-perception-sensing/index.md",tags:[],version:"current",frontMatter:{},sidebar:"textbookSidebar",previous:{title:"Practical Examples and Exercises: Humanoid Control Implementation",permalink:"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-2-humanoid-control/practical-examples"},next:{title:"Sensory Modalities in Humanoid Robotics",permalink:"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/sensory-modalities"}},l={},c=[{value:"Introduction: The Robotic Senses",id:"introduction-the-robotic-senses",level:2},{value:"Learning Objectives",id:"learning-objectives",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Table of Contents",id:"table-of-contents",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"module-3-perception-and-sensing-systems",children:"Module 3: Perception and Sensing Systems"}),"\n",(0,s.jsx)(n.h2,{id:"introduction-the-robotic-senses",children:"Introduction: The Robotic Senses"}),"\n",(0,s.jsx)(n.p,{children:'Perception and sensing systems form the "sensory organs" of humanoid robots, enabling them to understand and interact with their environment. Just as biological systems rely on vision, touch, hearing, and proprioception, humanoid robots require sophisticated sensor arrays and processing algorithms to navigate, manipulate objects, and interact safely with humans and environments.'}),"\n",(0,s.jsx)(n.p,{children:"This module explores the various sensing modalities used in humanoid robotics, from basic proprioceptive sensors that provide internal state information to advanced exteroceptive sensors that perceive the external world. We'll examine how these sensors work, their integration into robotic systems, and the algorithms that transform raw sensor data into actionable information."}),"\n",(0,s.jsx)(n.h3,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the different types of sensors used in humanoid robotics"}),"\n",(0,s.jsx)(n.li,{children:"Learn how sensor data is processed and integrated"}),"\n",(0,s.jsx)(n.li,{children:"Explore computer vision techniques for humanoid robots"}),"\n",(0,s.jsx)(n.li,{children:"Understand tactile sensing and haptic feedback systems"}),"\n",(0,s.jsx)(n.li,{children:"Learn about sensor fusion for robust perception"}),"\n",(0,s.jsx)(n.li,{children:"Implement basic perception algorithms"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understanding of ROS 2 concepts (Module 1)"}),"\n",(0,s.jsx)(n.li,{children:"Basic knowledge of Python programming"}),"\n",(0,s.jsx)(n.li,{children:"Understanding of humanoid control systems (Module 2)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./sensory-modalities",children:"Sensory Modalities in Humanoid Robotics"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./computer-vision",children:"Computer Vision for Humanoid Robots"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./tactile-sensing",children:"Tactile Sensing and Haptic Feedback"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./inertial-sensing",children:"Inertial and Proprioceptive Sensing"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./sensor-fusion",children:"Sensor Fusion and State Estimation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./environmental-perception",children:"Environmental Perception and Mapping"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./practical-examples",children:"Practical Examples and Exercises"})}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This module builds upon the communication infrastructure established in Module 1 and the control systems from Module 2, providing the sensory input necessary for intelligent physical behavior."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var s=i(6540);const o={},t=s.createContext(o);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);