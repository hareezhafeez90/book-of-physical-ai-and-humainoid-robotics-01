"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[750],{4834:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>f,frontMatter:()=>t,metadata:()=>o,toc:()=>c});var r=s(4848),a=s(8453);const t={},i="Sensory Modalities in Humanoid Robotics",o={id:"module-3-perception-sensing/sensory-modalities",title:"Sensory Modalities in Humanoid Robotics",description:"Introduction: The Need for Multiple Senses",source:"@site/docs/module-3-perception-sensing/sensory-modalities.md",sourceDirName:"module-3-perception-sensing",slug:"/module-3-perception-sensing/sensory-modalities",permalink:"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/sensory-modalities",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-perception-sensing/sensory-modalities.md",tags:[],version:"current",frontMatter:{},sidebar:"textbookSidebar",previous:{title:"Module 3: Perception and Sensing Systems",permalink:"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/"},next:{title:"Computer Vision for Humanoid Robots",permalink:"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/computer-vision"}},l={},c=[{value:"Introduction: The Need for Multiple Senses",id:"introduction-the-need-for-multiple-senses",level:2},{value:"Classification of Sensors",id:"classification-of-sensors",level:3},{value:"Proprioceptive Sensors",id:"proprioceptive-sensors",level:4},{value:"Exteroceptive Sensors",id:"exteroceptive-sensors",level:4},{value:"Proprioceptive Sensing Systems",id:"proprioceptive-sensing-systems",level:2},{value:"Joint Position Sensors",id:"joint-position-sensors",level:3},{value:"Inertial Measurement Units (IMUs)",id:"inertial-measurement-units-imus",level:3},{value:"Force/Torque Sensors",id:"forcetorque-sensors",level:3},{value:"Exteroceptive Sensing Systems",id:"exteroceptive-sensing-systems",level:2},{value:"Vision Systems",id:"vision-systems",level:3},{value:"Range Sensors",id:"range-sensors",level:3},{value:"Tactile Sensing",id:"tactile-sensing",level:3},{value:"Sensor Integration and Synchronization",id:"sensor-integration-and-synchronization",level:2},{value:"Time Synchronization",id:"time-synchronization",level:3},{value:"Sensor Calibration",id:"sensor-calibration",level:3},{value:"Sensor Reliability and Fault Detection",id:"sensor-reliability-and-fault-detection",level:2},{value:"Health Monitoring",id:"health-monitoring",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"sensory-modalities-in-humanoid-robotics",children:"Sensory Modalities in Humanoid Robotics"}),"\n",(0,r.jsx)(n.h2,{id:"introduction-the-need-for-multiple-senses",children:"Introduction: The Need for Multiple Senses"}),"\n",(0,r.jsx)(n.p,{children:"Humanoid robots require diverse sensory modalities to operate effectively in human environments. Unlike traditional industrial robots that work in controlled settings, humanoid robots must perceive and respond to complex, dynamic environments filled with humans, objects, and unpredictable situations. This section explores the different types of sensors used in humanoid robotics, categorized by their function and the information they provide."}),"\n",(0,r.jsx)(n.h3,{id:"classification-of-sensors",children:"Classification of Sensors"}),"\n",(0,r.jsx)(n.p,{children:"Sensors in humanoid robotics can be broadly classified into two categories:"}),"\n",(0,r.jsx)(n.h4,{id:"proprioceptive-sensors",children:"Proprioceptive Sensors"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Function"}),": Provide information about the robot's internal state"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Examples"}),": Joint encoders, IMUs, motor current sensors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Monitor robot configuration, balance, and internal forces"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"exteroceptive-sensors",children:"Exteroceptive Sensors"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Function"}),": Provide information about the external environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Examples"}),": Cameras, LIDAR, tactile sensors, microphones"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Perceive objects, obstacles, humans, and environmental conditions"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"proprioceptive-sensing-systems",children:"Proprioceptive Sensing Systems"}),"\n",(0,r.jsx)(n.h3,{id:"joint-position-sensors",children:"Joint Position Sensors"}),"\n",(0,r.jsx)(n.p,{children:"Joint encoders are fundamental to humanoid operation, providing precise measurements of joint angles:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class JointEncoder:\n    def __init__(self, joint_name, resolution=16):\n        self.joint_name = joint_name\n        self.resolution = resolution  # bits\n        self.max_count = 2**resolution\n        self.offset = 0.0  # radians\n        self.position = 0.0  # radians\n\n    def read_position(self):\n        """Read raw encoder count and convert to angle"""\n        # In practice, this would interface with hardware\n        raw_count = self.get_raw_count()  # Placeholder\n        angle = (raw_count / self.max_count) * 2 * np.pi\n        return angle - self.offset\n\n    def calibrate(self, reference_position):\n        """Calibrate encoder offset"""\n        current_raw = self.get_raw_count()\n        current_angle = (current_raw / self.max_count) * 2 * np.pi\n        self.offset = current_angle - reference_position\n'})}),"\n",(0,r.jsx)(n.h3,{id:"inertial-measurement-units-imus",children:"Inertial Measurement Units (IMUs)"}),"\n",(0,r.jsx)(n.p,{children:"IMUs provide crucial information about orientation, acceleration, and angular velocity:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass IMUSensor:\n    def __init__(self, sensor_id):\n        self.sensor_id = sensor_id\n        self.orientation = np.array([1, 0, 0, 0])  # Quaternion [w, x, y, z]\n        self.angular_velocity = np.zeros(3)  # rad/s [x, y, z]\n        self.linear_acceleration = np.zeros(3)  # m/s^2 [x, y, z]\n        self.temperature = 25.0  # degrees Celsius\n\n    def update_from_raw_data(self, raw_orientation, raw_angular_vel, raw_linear_acc):\n        """Update sensor state from raw measurements"""\n        # Normalize quaternion\n        self.orientation = raw_orientation / np.linalg.norm(raw_orientation)\n        self.angular_velocity = raw_angular_vel\n        self.linear_acceleration = raw_linear_acc\n\n    def get_euler_angles(self):\n        """Convert quaternion to Euler angles (roll, pitch, yaw)"""\n        w, x, y, z = self.orientation\n\n        # Roll (x-axis rotation)\n        sinr_cosp = 2 * (w * x + y * z)\n        cosr_cosp = 1 - 2 * (x * x + y * y)\n        roll = np.arctan2(sinr_cosp, cosr_cosp)\n\n        # Pitch (y-axis rotation)\n        sinp = 2 * (w * y - z * x)\n        if np.abs(sinp) >= 1:\n            pitch = np.sign(sinp) * np.pi / 2  # Use 90 degrees if out of range\n        else:\n            pitch = np.arcsin(sinp)\n\n        # Yaw (z-axis rotation)\n        siny_cosp = 2 * (w * z + x * y)\n        cosy_cosp = 1 - 2 * (y * y + z * z)\n        yaw = np.arctan2(siny_cosp, cosy_cosp)\n\n        return np.array([roll, pitch, yaw])\n\n    def is_upright(self, threshold=0.1):\n        """Check if sensor is approximately upright (for balance)"""\n        euler = self.get_euler_angles()\n        return abs(euler[0]) < threshold and abs(euler[1]) < threshold\n'})}),"\n",(0,r.jsx)(n.h3,{id:"forcetorque-sensors",children:"Force/Torque Sensors"}),"\n",(0,r.jsx)(n.p,{children:"Force/torque sensors provide information about external forces acting on the robot:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class ForceTorqueSensor:\n    def __init__(self, sensor_frame):\n        self.frame = sensor_frame\n        self.force = np.zeros(3)  # [Fx, Fy, Fz] in Newtons\n        self.torque = np.zeros(3)  # [Mx, My, Mz] in Nm\n        self.bias = np.zeros(6)  # Bias values for calibration\n\n    def read_raw_data(self):\n        """Read raw force/torque data"""\n        # In practice, this would interface with hardware\n        raw_data = np.random.normal(0, 0.1, 6)  # Placeholder\n        return raw_data\n\n    def get_calibrated_data(self):\n        """Get bias-corrected force/torque measurements"""\n        raw_data = self.read_raw_data()\n        calibrated_data = raw_data - self.bias\n        self.force = calibrated_data[:3]\n        self.torque = calibrated_data[3:]\n        return self.force, self.torque\n\n    def detect_contact(self, force_threshold=5.0, torque_threshold=1.0):\n        """Detect if sensor is in contact with environment"""\n        force_magnitude = np.linalg.norm(self.force)\n        torque_magnitude = np.linalg.norm(self.torque)\n        return (force_magnitude > force_threshold or\n                torque_magnitude > torque_threshold)\n\n    def get_wrench(self):\n        """Return 6D wrench (force + torque)"""\n        return np.concatenate([self.force, self.torque])\n'})}),"\n",(0,r.jsx)(n.h2,{id:"exteroceptive-sensing-systems",children:"Exteroceptive Sensing Systems"}),"\n",(0,r.jsx)(n.h3,{id:"vision-systems",children:"Vision Systems"}),"\n",(0,r.jsx)(n.p,{children:"Cameras provide rich visual information for humanoid robots:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\n\nclass VisionSystem:\n    def __init__(self, camera_id=0, resolution=(640, 480)):\n        self.camera_id = camera_id\n        self.resolution = resolution\n        self.cap = cv2.VideoCapture(camera_id)\n        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, resolution[0])\n        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, resolution[1])\n\n        # Camera intrinsic parameters (example values)\n        self.fx = resolution[0] / 2  # Focal length x\n        self.fy = resolution[1] / 2  # Focal length y\n        self.cx = resolution[0] / 2  # Principal point x\n        self.cy = resolution[1] / 2  # Principal point y\n\n    def capture_image(self):\n        """Capture and return image"""\n        ret, frame = self.cap.read()\n        if ret:\n            return frame\n        else:\n            return None\n\n    def detect_faces(self, image):\n        """Detect faces in image using Haar cascades"""\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \'haarcascade_frontalface_default.xml\')\n        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n        return faces\n\n    def detect_objects(self, image):\n        """Detect objects using color-based segmentation"""\n        # Convert to HSV for better color detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define range for red color (example)\n        lower_red = np.array([0, 50, 50])\n        upper_red = np.array([10, 255, 255])\n\n        # Create mask\n        mask = cv2.inRange(hsv, lower_red, upper_red)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        objects = []\n        for contour in contours:\n            if cv2.contourArea(contour) > 100:  # Filter small contours\n                # Get bounding box\n                x, y, w, h = cv2.boundingRect(contour)\n                objects.append({\'bbox\': (x, y, w, h), \'area\': cv2.contourArea(contour)})\n\n        return objects\n\n    def depth_from_stereo(self, left_image, right_image):\n        """Compute depth from stereo camera pair"""\n        # Create stereo matcher\n        stereo = cv2.StereoSGBM_create(\n            minDisparity=0,\n            numDisparities=16*10,  # Must be divisible by 16\n            blockSize=5,\n            P1=8 * 3 * 5**2,\n            P2=32 * 3 * 5**2,\n            disp12MaxDiff=1,\n            uniquenessRatio=15,\n            speckleWindowSize=0,\n            speckleRange=2,\n            preFilterCap=63,\n            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n        )\n\n        # Compute disparity\n        disparity = stereo.compute(left_image, right_image).astype(np.float32) / 16.0\n\n        # Convert disparity to depth\n        baseline = 0.1  # Baseline distance between cameras (m)\n        depth = (self.fx * baseline) / (disparity + 1e-6)  # Avoid division by zero\n\n        return depth\n'})}),"\n",(0,r.jsx)(n.h3,{id:"range-sensors",children:"Range Sensors"}),"\n",(0,r.jsx)(n.p,{children:"Range sensors provide distance measurements to obstacles:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class RangeSensor:\n    def __init__(self, sensor_type="lidar", num_beams=360):\n        self.type = sensor_type\n        self.num_beams = num_beams\n        self.angles = np.linspace(0, 2*np.pi, num_beams, endpoint=False)\n        self.ranges = np.zeros(num_beams)\n        self.max_range = 10.0  # meters\n        self.min_range = 0.1   # meters\n\n    def update_ranges(self, new_ranges):\n        """Update range measurements"""\n        self.ranges = np.clip(new_ranges, self.min_range, self.max_range)\n\n    def get_obstacle_distances(self, angle_min=-np.pi/4, angle_max=np.pi/4):\n        """Get distances in a specific angular sector (e.g., front of robot)"""\n        angle_indices = np.where((self.angles >= angle_min) & (self.angles <= angle_max))[0]\n        return self.angles[angle_indices], self.ranges[angle_indices]\n\n    def detect_obstacles(self, threshold=2.0):\n        """Detect obstacles closer than threshold"""\n        obstacle_angles = self.angles[self.ranges < threshold]\n        obstacle_distances = self.ranges[self.ranges < threshold]\n        return obstacle_angles, obstacle_distances\n\n    def create_occupancy_grid(self, grid_size=(100, 100), resolution=0.1):\n        """Create occupancy grid from range data"""\n        grid = np.zeros(grid_size)  # 0 = unknown, 1 = free, -1 = occupied\n\n        robot_x, robot_y = grid_size[0] // 2, grid_size[1] // 2\n\n        for i, (angle, distance) in enumerate(zip(self.angles, self.ranges)):\n            if distance >= self.max_range:\n                continue  # No return, mark as free to max range\n\n            # Calculate endpoint of beam\n            end_x = robot_x + int((distance * np.cos(angle)) / resolution)\n            end_y = robot_y + int((distance * np.sin(angle)) / resolution)\n\n            # Bresenham\'s algorithm to mark free space\n            self._mark_free_space(grid, robot_x, robot_y, end_x, end_y)\n\n            # Mark endpoint as occupied if it\'s a valid obstacle\n            if distance < self.max_range:\n                if 0 <= end_x < grid_size[0] and 0 <= end_y < grid_size[1]:\n                    grid[end_x, end_y] = -1  # Occupied\n\n        return grid\n\n    def _mark_free_space(self, grid, x0, y0, x1, y1):\n        """Mark free space along a ray using Bresenham\'s algorithm"""\n        dx = abs(x1 - x0)\n        dy = abs(y1 - y0)\n        x, y = x0, y0\n        sx = -1 if x0 > x1 else 1\n        sy = -1 if y0 > y1 else 1\n        err = dx - dy\n\n        while True:\n            if 0 <= x < grid.shape[0] and 0 <= y < grid.shape[1]:\n                grid[x, y] = 1  # Mark as free\n            if x == x1 and y == y1:\n                break\n            e2 = 2 * err\n            if e2 > -dy:\n                err -= dy\n                x += sx\n            if e2 < dx:\n                err += dx\n                y += sy\n'})}),"\n",(0,r.jsx)(n.h3,{id:"tactile-sensing",children:"Tactile Sensing"}),"\n",(0,r.jsx)(n.p,{children:"Tactile sensors provide information about contact and pressure:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class TactileSensor:\n    def __init__(self, sensor_location, num_taxels=64):\n        self.location = sensor_location  # e.g., "hand", "foot", "torso"\n        self.num_taxels = num_taxels\n        self.pressure_map = np.zeros(num_taxels)\n        self.temperature = np.zeros(num_taxels)\n        self.contact_detected = np.zeros(num_taxels, dtype=bool)\n\n    def update_sensor_data(self, pressure_values, temp_values):\n        """Update tactile sensor readings"""\n        self.pressure_map = np.clip(pressure_values, 0, 100)  # 0-100 N/m^2\n        self.temperature = temp_values\n        self.contact_detected = self.pressure_map > 0.1  # Threshold for contact\n\n    def get_contact_info(self):\n        """Get contact information"""\n        contact_indices = np.where(self.contact_detected)[0]\n        contact_pressures = self.pressure_map[contact_indices]\n        return contact_indices, contact_pressures\n\n    def get_contact_center_of_pressure(self):\n        """Calculate center of pressure for contact area"""\n        if not np.any(self.contact_detected):\n            return None\n\n        # For simplicity, assume 2D taxel array\n        # In practice, taxel positions would be known\n        contact_indices = np.where(self.contact_detected)[0]\n        pressures = self.pressure_map[contact_indices]\n\n        # Calculate weighted average position\n        total_force = np.sum(pressures)\n        if total_force == 0:\n            return None\n\n        # Convert linear indices to 2D coordinates (assuming 8x8 grid)\n        rows = contact_indices // 8\n        cols = contact_indices % 8\n\n        center_row = np.sum(rows * pressures) / total_force\n        center_col = np.sum(cols * pressures) / total_force\n\n        return center_row, center_col\n\n    def detect_slip(self, time_window=0.1):\n        """Detect potential slip based on pressure changes"""\n        # This would require temporal analysis\n        # For now, return a simple slip indicator\n        pressure_changes = np.diff(self.pressure_map) if len(self.pressure_map) > 1 else np.zeros_like(self.pressure_map)\n        return np.any(np.abs(pressure_changes) > 10)  # Threshold for slip detection\n'})}),"\n",(0,r.jsx)(n.h2,{id:"sensor-integration-and-synchronization",children:"Sensor Integration and Synchronization"}),"\n",(0,r.jsx)(n.h3,{id:"time-synchronization",children:"Time Synchronization"}),"\n",(0,r.jsx)(n.p,{children:"Proper timing is crucial for sensor fusion:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\nfrom collections import deque\n\nclass SensorSynchronizer:\n    def __init__(self, max_buffer_size=100):\n        self.sensors = {}\n        self.buffers = {}\n        self.max_buffer_size = max_buffer_size\n        self.sync_threshold = 0.01  # 10ms sync window\n\n    def register_sensor(self, sensor_name, callback=None):\n        """Register a sensor with the synchronizer"""\n        self.sensors[sensor_name] = callback\n        self.buffers[sensor_name] = deque(maxlen=self.max_buffer_size)\n\n    def add_sensor_data(self, sensor_name, data, timestamp=None):\n        """Add sensor data with timestamp"""\n        if timestamp is None:\n            timestamp = time.time()\n\n        self.buffers[sensor_name].append((timestamp, data))\n\n    def get_synchronized_data(self):\n        """Get time-synchronized sensor data"""\n        if not self.buffers:\n            return None\n\n        # Find the latest common timestamp\n        latest_times = {}\n        for sensor_name, buffer in self.buffers.items():\n            if buffer:\n                latest_times[sensor_name] = buffer[-1][0]\n\n        if not latest_times:\n            return None\n\n        # Find the oldest of the latest times (most conservative sync)\n        sync_time = min(latest_times.values())\n\n        # Get data closest to sync time for each sensor\n        synchronized_data = {}\n        for sensor_name, buffer in self.buffers.items():\n            if not buffer:\n                continue\n\n            # Find closest timestamp within threshold\n            closest_data = None\n            closest_time_diff = float(\'inf\')\n\n            for ts, data in buffer:\n                time_diff = abs(ts - sync_time)\n                if time_diff < closest_time_diff and time_diff < self.sync_threshold:\n                    closest_time_diff = time_diff\n                    closest_data = data\n\n            if closest_data is not None:\n                synchronized_data[sensor_name] = (closest_data, sync_time)\n\n        return synchronized_data if synchronized_data else None\n'})}),"\n",(0,r.jsx)(n.h3,{id:"sensor-calibration",children:"Sensor Calibration"}),"\n",(0,r.jsx)(n.p,{children:"Calibration ensures accurate sensor readings:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class SensorCalibrator:\n    def __init__(self):\n        self.calibration_data = {}\n        self.is_calibrated = {}\n\n    def collect_calibration_data(self, sensor_name, raw_data, reference_data):\n        \"\"\"Collect data for calibration\"\"\"\n        if sensor_name not in self.calibration_data:\n            self.calibration_data[sensor_name] = {'raw': [], 'ref': []}\n\n        self.calibration_data[sensor_name]['raw'].append(raw_data)\n        self.calibration_data[sensor_name]['ref'].append(reference_data)\n\n    def compute_calibration_parameters(self, sensor_name):\n        \"\"\"Compute calibration parameters using least squares\"\"\"\n        if sensor_name not in self.calibration_data:\n            return None\n\n        raw_data = np.array(self.calibration_data[sensor_name]['raw'])\n        ref_data = np.array(self.calibration_data[sensor_name]['ref'])\n\n        # For linear calibration: y = ax + b\n        # Solve for a and b using least squares\n        A = np.vstack([raw_data, np.ones(len(raw_data))]).T\n        a, b = np.linalg.lstsq(A, ref_data, rcond=None)[0]\n\n        self.calibration_parameters = {'a': a, 'b': b}\n        self.is_calibrated[sensor_name] = True\n\n        return {'a': a, 'b': b}\n\n    def apply_calibration(self, sensor_name, raw_value):\n        \"\"\"Apply calibration to raw sensor value\"\"\"\n        if not self.is_calibrated.get(sensor_name, False):\n            return raw_value  # Return raw value if not calibrated\n\n        params = self.calibration_parameters\n        return params['a'] * raw_value + params['b']\n"})}),"\n",(0,r.jsx)(n.h2,{id:"sensor-reliability-and-fault-detection",children:"Sensor Reliability and Fault Detection"}),"\n",(0,r.jsx)(n.h3,{id:"health-monitoring",children:"Health Monitoring"}),"\n",(0,r.jsx)(n.p,{children:"Monitoring sensor health is essential for reliable operation:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class SensorHealthMonitor:\n    def __init__(self, sensor_names):\n        self.sensor_names = sensor_names\n        self.reading_history = {name: deque(maxlen=50) for name in sensor_names}\n        self.health_status = {name: True for name in sensor_names}\n\n    def update_sensor_reading(self, sensor_name, reading):\n        """Update sensor reading and check health"""\n        self.reading_history[sensor_name].append(reading)\n\n        # Check for sensor faults\n        self.health_status[sensor_name] = self._check_sensor_health(sensor_name)\n\n    def _check_sensor_health(self, sensor_name):\n        """Check if sensor is operating normally"""\n        if len(self.reading_history[sensor_name]) < 10:\n            return True  # Not enough data to assess\n\n        readings = list(self.reading_history[sensor_name])\n\n        # Check for constant values (stuck sensor)\n        if len(set(readings)) < len(readings) * 0.1:  # Less than 10% unique values\n            return False\n\n        # Check for extreme values\n        current = readings[-1]\n        mean = np.mean(readings[:-1])  # Exclude current reading\n        std = np.std(readings[:-1])\n\n        if std > 0 and abs(current - mean) > 5 * std:  # 5-sigma outlier\n            return False\n\n        # Check for sudden jumps\n        if len(readings) > 1:\n            recent_change = abs(readings[-1] - readings[-2])\n            historical_changes = [abs(readings[i] - readings[i-1])\n                                for i in range(1, len(readings)-1)]\n            if historical_changes:\n                mean_change = np.mean(historical_changes)\n                if mean_change > 0 and recent_change > 10 * mean_change:\n                    return False\n\n        return True\n\n    def get_unhealthy_sensors(self):\n        """Get list of unhealthy sensors"""\n        return [name for name, healthy in self.health_status.items() if not healthy]\n'})}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"Sensory modalities form the foundation of humanoid robot perception, providing the information necessary for intelligent behavior. Proprioceptive sensors enable the robot to understand its own state, while exteroceptive sensors provide awareness of the external environment. Proper sensor integration, calibration, and health monitoring are essential for reliable operation."}),"\n",(0,r.jsx)(n.p,{children:"The next section will explore computer vision techniques specifically designed for humanoid robots, building on these fundamental sensing concepts to enable visual perception and object recognition."})]})}function f(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>o});var r=s(6540);const a={},t=r.createContext(a);function i(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);