"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[597],{368:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var i=t(4848),s=t(8453);const a={},o="Sensor Fusion and State Estimation",r={id:"module-3-perception-sensing/sensor-fusion",title:"Sensor Fusion and State Estimation",description:"Introduction: Combining Multiple Sensory Inputs",source:"@site/docs/module-3-perception-sensing/sensor-fusion.md",sourceDirName:"module-3-perception-sensing",slug:"/module-3-perception-sensing/sensor-fusion",permalink:"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/sensor-fusion",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-perception-sensing/sensor-fusion.md",tags:[],version:"current",frontMatter:{},sidebar:"textbookSidebar",previous:{title:"Inertial and Proprioceptive Sensing",permalink:"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/inertial-sensing"},next:{title:"Environmental Perception and Mapping",permalink:"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/environmental-perception"}},l={},c=[{value:"Introduction: Combining Multiple Sensory Inputs",id:"introduction-combining-multiple-sensory-inputs",level:2},{value:"The Need for Sensor Fusion",id:"the-need-for-sensor-fusion",level:3},{value:"Mathematical Foundations",id:"mathematical-foundations",level:2},{value:"Probability and Uncertainty",id:"probability-and-uncertainty",level:3},{value:"Kalman Filtering Fundamentals",id:"kalman-filtering-fundamentals",level:3},{value:"Extended Kalman Filter (EKF)",id:"extended-kalman-filter-ekf",level:2},{value:"Particle Filtering",id:"particle-filtering",level:2},{value:"Multi-Sensor Fusion Examples",id:"multi-sensor-fusion-examples",level:2},{value:"IMU and Vision Fusion",id:"imu-and-vision-fusion",level:3},{value:"Multi-IMU Fusion",id:"multi-imu-fusion",level:3},{value:"Tactile and Force Sensing Fusion",id:"tactile-and-force-sensing-fusion",level:3},{value:"Advanced Fusion Techniques",id:"advanced-fusion-techniques",level:2},{value:"Information Filter",id:"information-filter",level:3},{value:"Covariance Intersection",id:"covariance-intersection",level:3},{value:"Real-Time Implementation Considerations",id:"real-time-implementation-considerations",level:2},{value:"Efficient Data Structures",id:"efficient-data-structures",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"ROS 2 Sensor Fusion Node",id:"ros-2-sensor-fusion-node",level:3},{value:"Conclusion",id:"conclusion",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"sensor-fusion-and-state-estimation",children:"Sensor Fusion and State Estimation"}),"\n",(0,i.jsx)(e.h2,{id:"introduction-combining-multiple-sensory-inputs",children:"Introduction: Combining Multiple Sensory Inputs"}),"\n",(0,i.jsx)(e.p,{children:"Sensor fusion is the process of combining data from multiple sensors to achieve more accurate, reliable, and robust perception than would be possible with any single sensor. In humanoid robotics, sensor fusion is critical because no single sensor modality can provide complete information about the robot's state and environment. This section explores the mathematical foundations, algorithms, and practical implementations of sensor fusion for humanoid robots."}),"\n",(0,i.jsx)(e.h3,{id:"the-need-for-sensor-fusion",children:"The Need for Sensor Fusion"}),"\n",(0,i.jsx)(e.p,{children:"Humanoid robots operate in complex environments where:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"No single sensor is perfect"}),": Each sensor has limitations in accuracy, range, or reliability"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Complementary information"}),": Different sensors provide different types of information"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Redundancy for safety"}),": Multiple sensors provide backup in case of sensor failure"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Enhanced performance"}),": Combined sensors can achieve better accuracy than individual sensors"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"mathematical-foundations",children:"Mathematical Foundations"}),"\n",(0,i.jsx)(e.h3,{id:"probability-and-uncertainty",children:"Probability and Uncertainty"}),"\n",(0,i.jsx)(e.p,{children:"Sensor fusion relies on probabilistic models to handle uncertainty:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom scipy.stats import multivariate_normal\nimport matplotlib.pyplot as plt\n\nclass ProbabilityDistribution:\n    def __init__(self, mean, covariance):\n        self.mean = np.array(mean)\n        self.covariance = np.array(covariance)\n        self.dimension = len(mean)\n\n    def pdf(self, x):\n        """Probability density function"""\n        return multivariate_normal.pdf(x, self.mean, self.covariance)\n\n    def sample(self, n_samples=1):\n        """Draw samples from the distribution"""\n        return multivariate_normal.rvs(self.mean, self.covariance, n_samples)\n\n    def add_noise(self, noise_covariance):\n        """Add noise to the distribution"""\n        new_covariance = self.covariance + noise_covariance\n        return ProbabilityDistribution(self.mean, new_covariance)\n\n    def combine_with(self, other):\n        """Combine two Gaussian distributions (sensor fusion)"""\n        # For two Gaussians with means \u03bc1, \u03bc2 and covariances \u03a31, \u03a32\n        # Combined mean: \u03bc = \u03a32*(\u03a31+\u03a32)^(-1)*\u03bc1 + \u03a31*(\u03a31+\u03a32)^(-1)*\u03bc2\n        # Combined covariance: \u03a3 = \u03a31*\u03a32*(\u03a31+\u03a32)^(-1)\n\n        sigma1_inv = np.linalg.inv(self.covariance)\n        sigma2_inv = np.linalg.inv(other.covariance)\n\n        # Combined covariance\n        combined_cov = np.linalg.inv(sigma1_inv + sigma2_inv)\n\n        # Combined mean\n        combined_mean = combined_cov @ (sigma1_inv @ self.mean + sigma2_inv @ other.mean)\n\n        return ProbabilityDistribution(combined_mean, combined_cov)\n\n# Example: Fusing two position estimates\npos1 = ProbabilityDistribution([1.0, 0.0], [[0.1, 0], [0, 0.1]])  # High confidence\npos2 = ProbabilityDistribution([1.2, 0.1], [[0.3, 0], [0, 0.3]])  # Lower confidence\n\nfused_pos = pos1.combine_with(pos2)\nprint(f"Fused position: {fused_pos.mean}")\nprint(f"Fused covariance: {fused_pos.covariance}")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"kalman-filtering-fundamentals",children:"Kalman Filtering Fundamentals"}),"\n",(0,i.jsx)(e.p,{children:"The Kalman filter is the optimal linear estimator for systems with Gaussian noise:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class KalmanFilter:\n    def __init__(self, state_dim, measurement_dim):\n        self.state_dim = state_dim\n        self.measurement_dim = measurement_dim\n\n        # State: [x, y, vx, vy] (position and velocity)\n        self.state = np.zeros(state_dim)\n        self.covariance = np.eye(state_dim) * 1000  # Initial uncertainty\n\n        # Process model (how state evolves over time)\n        self.F = np.eye(state_dim)  # Will be updated in predict step\n\n        # Measurement model (how state relates to measurements)\n        self.H = np.zeros((measurement_dim, state_dim))\n        if measurement_dim == 2 and state_dim >= 2:\n            # Measure only position (first 2 elements)\n            self.H[0, 0] = 1  # x position\n            self.H[1, 1] = 1  # y position\n\n        # Process noise covariance\n        self.Q = np.eye(state_dim) * 0.1\n\n        # Measurement noise covariance\n        self.R = np.eye(measurement_dim) * 1.0\n\n    def predict(self, dt):\n        """Prediction step: predict next state"""\n        # Update process model for motion\n        # For constant velocity model: x(k+1) = x(k) + v(k)*dt\n        if self.state_dim >= 4:  # [x, y, vx, vy]\n            self.F = np.array([\n                [1, 0, dt, 0],    # x = x + vx*dt\n                [0, 1, 0, dt],    # y = y + vy*dt\n                [0, 0, 1, 0],     # vx = vx (constant)\n                [0, 0, 0, 1]      # vy = vy (constant)\n            ])\n\n        # Predict state\n        self.state = self.F @ self.state\n\n        # Predict covariance\n        self.covariance = self.F @ self.covariance @ self.F.T + self.Q\n\n    def update(self, measurement):\n        """Update step: incorporate new measurement"""\n        # Innovation (measurement residual)\n        innovation = measurement - self.H @ self.state\n\n        # Innovation covariance\n        innovation_cov = self.H @ self.covariance @ self.H.T + self.R\n\n        # Kalman gain\n        K = self.covariance @ self.H.T @ np.linalg.inv(innovation_cov)\n\n        # Update state\n        self.state = self.state + K @ innovation\n\n        # Update covariance\n        self.covariance = (np.eye(self.state_dim) - K @ self.H) @ self.covariance\n\n    def get_state(self):\n        """Get current state estimate"""\n        return self.state.copy()\n\n    def get_covariance(self):\n        """Get current covariance estimate"""\n        return self.covariance.copy()\n\n# Example usage\nkf = KalmanFilter(state_dim=4, measurement_dim=2)  # [x, y, vx, vy] with position measurements\n\n# Simulate tracking\ntrue_positions = []\nmeasurements = []\nestimates = []\n\nfor t in range(100):\n    dt = 0.1\n    kf.predict(dt)\n\n    # Simulate true position (with constant velocity)\n    true_pos = np.array([t * 0.1, t * 0.05])  # Moving with constant velocity\n    noise = np.random.normal(0, 0.1, 2)  # Measurement noise\n    measurement = true_pos + noise\n\n    kf.update(measurement)\n\n    true_positions.append(true_pos)\n    measurements.append(measurement)\n    estimates.append(kf.get_state()[:2])  # Only position part\n\ntrue_positions = np.array(true_positions)\nmeasurements = np.array(measurements)\nestimates = np.array(estimates)\n\nprint(f"Final position estimate: {estimates[-1]}")\nprint(f"Final position uncertainty: {np.sqrt(kf.get_covariance()[0,0])}")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"extended-kalman-filter-ekf",children:"Extended Kalman Filter (EKF)"}),"\n",(0,i.jsx)(e.p,{children:"For nonlinear systems, the Extended Kalman Filter linearizes around the current state:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class ExtendedKalmanFilter:\n    def __init__(self, state_dim, measurement_dim):\n        self.state_dim = state_dim\n        self.measurement_dim = measurement_dim\n        self.state = np.zeros(state_dim)\n        self.covariance = np.eye(state_dim) * 1000\n        self.Q = np.eye(state_dim) * 0.1  # Process noise\n        self.R = np.eye(measurement_dim) * 1.0  # Measurement noise\n\n    def motion_model(self, state, dt):\n        """Nonlinear motion model"""\n        # For example, constant turn rate and velocity model\n        x, y, theta, v = state\n        new_x = x + v * np.cos(theta) * dt\n        new_y = y + v * np.sin(theta) * dt\n        new_theta = theta  # Assuming no angular velocity for simplicity\n        new_v = v  # Constant velocity\n        return np.array([new_x, new_y, new_theta, new_v])\n\n    def motion_jacobian(self, state, dt):\n        """Jacobian of motion model"""\n        x, y, theta, v = state\n        F = np.eye(self.state_dim)\n        F[0, 2] = -v * np.sin(theta) * dt  # dx/dtheta\n        F[0, 3] = np.cos(theta) * dt      # dx/dv\n        F[1, 2] = v * np.cos(theta) * dt  # dy/dtheta\n        F[1, 3] = np.sin(theta) * dt      # dy/dv\n        return F\n\n    def measurement_model(self, state):\n        """Nonlinear measurement model"""\n        # Measure position (x, y)\n        return state[:2]\n\n    def measurement_jacobian(self, state):\n        """Jacobian of measurement model"""\n        H = np.zeros((self.measurement_dim, self.state_dim))\n        H[0, 0] = 1  # dx/dx\n        H[1, 1] = 1  # dy/dy\n        return H\n\n    def predict(self, dt):\n        """Prediction step for EKF"""\n        # Linearize motion model around current state\n        F = self.motion_jacobian(self.state, dt)\n\n        # Predict state\n        self.state = self.motion_model(self.state, dt)\n\n        # Predict covariance\n        self.covariance = F @ self.covariance @ F.T + self.Q\n\n    def update(self, measurement):\n        """Update step for EKF"""\n        # Linearize measurement model around current state\n        H = self.measurement_jacobian(self.state)\n\n        # Innovation\n        predicted_measurement = self.measurement_model(self.state)\n        innovation = measurement - predicted_measurement\n\n        # Innovation covariance\n        innovation_cov = H @ self.covariance @ H.T + self.R\n\n        # Kalman gain\n        K = self.covariance @ H.T @ np.linalg.inv(innovation_cov)\n\n        # Update state\n        self.state = self.state + K @ innovation\n\n        # Update covariance\n        self.covariance = (np.eye(self.state_dim) - K @ H) @ self.covariance\n\n# Example: Tracking with EKF\nekf = ExtendedKalmanFilter(state_dim=4, measurement_dim=2)  # [x, y, theta, v]\nekf.state = np.array([0, 0, 0, 1])  # Start at origin, facing right, speed 1\n\n# Simulate\nfor t in range(50):\n    dt = 0.1\n    ekf.predict(dt)\n\n    # Simulate measurement\n    true_pos = ekf.motion_model(ekf.state, 0)[:2]  # Get true position\n    measurement = true_pos + np.random.normal(0, 0.1, 2)\n\n    ekf.update(measurement)\n\nprint(f"EKF final state: {ekf.state}")\nprint(f"EKF final covariance diagonal: {np.diag(ekf.covariance)}")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"particle-filtering",children:"Particle Filtering"}),"\n",(0,i.jsx)(e.p,{children:"For highly nonlinear and non-Gaussian systems, particle filters provide a more general solution:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class ParticleFilter:\n    def __init__(self, state_dim, num_particles=1000):\n        self.state_dim = state_dim\n        self.num_particles = num_particles\n        self.particles = np.random.normal(0, 1, (num_particles, state_dim))\n        self.weights = np.ones(num_particles) / num_particles\n\n    def predict(self, control_input, dt):\n        """Predict step: propagate particles forward"""\n        # Add process noise and apply motion model\n        process_noise = np.random.normal(0, 0.1, (self.num_particles, self.state_dim))\n\n        # Simple motion model: add control input with some dynamics\n        for i in range(self.num_particles):\n            # Example: simple integrator model\n            self.particles[i] += control_input * dt + process_noise[i]\n\n    def update(self, measurement, measurement_noise_std=0.5):\n        """Update step: reweight particles based on measurement"""\n        # Calculate likelihood of each particle given measurement\n        for i in range(self.num_particles):\n            # Assume measurement is of first two state dimensions\n            predicted_measurement = self.particles[i][:2]\n            measurement_diff = measurement - predicted_measurement\n            # Calculate likelihood (Gaussian)\n            likelihood = np.exp(-0.5 * np.sum((measurement_diff / measurement_noise_std)**2))\n            self.weights[i] *= likelihood\n\n        # Normalize weights\n        self.weights += 1e-300  # Avoid numerical issues\n        self.weights /= np.sum(self.weights)\n\n    def resample(self):\n        """Resample particles based on weights"""\n        # Systematic resampling\n        indices = []\n        cumulative_sum = np.cumsum(self.weights)\n        start = np.random.uniform(0, 1/self.num_particles)\n        i, j = 0, 0\n        while i < self.num_particles:\n            if start + i / self.num_particles < cumulative_sum[j]:\n                indices.append(j)\n                i += 1\n            else:\n                j += 1\n\n        # Resample particles and reset weights\n        self.particles = self.particles[indices]\n        self.weights = np.ones(self.num_particles) / self.num_particles\n\n    def estimate(self):\n        """Get state estimate as weighted average of particles"""\n        return np.average(self.particles, axis=0, weights=self.weights)\n\n    def get_covariance(self):\n        """Get covariance estimate from particles"""\n        mean = self.estimate()\n        diff = self.particles - mean\n        cov = np.zeros((self.state_dim, self.state_dim))\n        for i in range(self.num_particles):\n            cov += self.weights[i] * np.outer(diff[i], diff[i])\n        return cov\n\n# Example: Particle filter for tracking\npf = ParticleFilter(state_dim=4, num_particles=1000)  # [x, y, vx, vy]\ncontrol = np.array([0.1, 0.05, 0, 0])  # Constant velocity control\n\nfor t in range(20):\n    dt = 0.1\n    pf.predict(control, dt)\n\n    # Simulate measurement\n    true_pos = np.array([t * 0.1, t * 0.05])\n    measurement = true_pos + np.random.normal(0, 0.1, 2)\n    pf.update(measurement)\n\n    if t % 5 == 0:  # Resample every 5 steps\n        pf.resample()\n\nestimate = pf.estimate()\nprint(f"Particle filter estimate: {estimate[:2]}")\nprint(f"True position: {true_pos}")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"multi-sensor-fusion-examples",children:"Multi-Sensor Fusion Examples"}),"\n",(0,i.jsx)(e.h3,{id:"imu-and-vision-fusion",children:"IMU and Vision Fusion"}),"\n",(0,i.jsx)(e.p,{children:"Combining inertial and visual sensors for robust state estimation:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class IMUVisionFusion:\n    def __init__(self):\n        # Initialize EKF for state estimation\n        # State: [x, y, z, roll, pitch, yaw, vx, vy, vz]\n        self.state_dim = 9\n        self.state = np.zeros(self.state_dim)\n        self.covariance = np.eye(self.state_dim) * 1.0\n\n        # Process noise\n        self.Q = np.diag([0.1, 0.1, 0.1, 0.01, 0.01, 0.01, 0.5, 0.5, 0.5])\n\n        # Initialize for prediction step\n        self.F = np.eye(self.state_dim)  # Will be updated\n        self.last_time = time.time()\n\n    def predict_with_imu(self, imu_data, dt):\n        \"\"\"Predict state using IMU data\"\"\"\n        # Extract IMU measurements\n        linear_acceleration = np.array([\n            imu_data.get('ax', 0),\n            imu_data.get('ay', 0),\n            imu_data.get('az', 0)\n        ])\n\n        angular_velocity = np.array([\n            imu_data.get('wx', 0),\n            imu_data.get('wy', 0),\n            imu_data.get('wz', 0)\n        ])\n\n        # Update state based on IMU data\n        # Position integration\n        self.state[6:9] += linear_acceleration * dt  # Update velocity\n        self.state[0:3] += self.state[6:9] * dt      # Update position\n\n        # Orientation integration\n        self.state[3:6] += angular_velocity * dt     # Update roll, pitch, yaw\n\n        # Update process model Jacobian\n        self.F = self.compute_jacobian(dt)\n\n        # Predict covariance\n        self.covariance = self.F @ self.covariance @ self.F.T + self.Q\n\n    def compute_jacobian(self, dt):\n        \"\"\"Compute process model Jacobian\"\"\"\n        F = np.eye(self.state_dim)\n\n        # Position-velocity relationships\n        F[0, 6] = dt  # dx/dvx\n        F[1, 7] = dt  # dy/dvy\n        F[2, 8] = dt  # dz/dvz\n\n        # Velocity-acceleration relationships (simplified)\n        # In a full implementation, this would involve rotation matrices\n        return F\n\n    def update_with_vision(self, vision_pos, vision_pos_cov):\n        \"\"\"Update state using vision data\"\"\"\n        # Measurement model: vision provides position [x, y, z]\n        H = np.zeros((3, self.state_dim))\n        H[0, 0] = 1  # x measurement\n        H[1, 1] = 1  # y measurement\n        H[2, 2] = 1  # z measurement\n\n        # Innovation\n        y = vision_pos - self.state[0:3]\n\n        # Innovation covariance\n        S = H @ self.covariance @ H.T + vision_pos_cov\n\n        # Kalman gain\n        K = self.covariance @ H.T @ np.linalg.inv(S)\n\n        # Update state and covariance\n        self.state += K @ y\n        self.covariance = (np.eye(self.state_dim) - K @ H) @ self.covariance\n\n    def update_with_depth(self, depth_measurement, depth_cov):\n        \"\"\"Update state using depth sensor data\"\"\"\n        # Depth provides distance to known landmark\n        # For simplicity, assume landmark is at [5, 0, 0]\n        landmark_pos = np.array([5.0, 0.0, 0.0])\n\n        # Predicted distance\n        current_pos = self.state[0:3]\n        predicted_distance = np.linalg.norm(landmark_pos - current_pos)\n\n        # Measurement residual\n        innovation = depth_measurement - predicted_distance\n\n        # Measurement Jacobian\n        if predicted_distance > 0:\n            H = np.zeros((1, self.state_dim))\n            range_vector = current_pos - landmark_pos\n            H[0, 0:3] = range_vector / predicted_distance  # d/distance_d/d_pos\n\n            # Innovation covariance\n            S = H @ self.covariance @ H.T + depth_cov\n\n            # Kalman gain\n            K = self.covariance @ H.T @ np.linalg.inv(S)\n\n            # Update\n            self.state += K.flatten() * innovation\n            self.covariance = (np.eye(self.state_dim) - np.outer(K, H)) @ self.covariance\n\n    def get_state(self):\n        \"\"\"Get current state estimate\"\"\"\n        return {\n            'position': self.state[0:3],\n            'orientation': self.state[3:6],\n            'velocity': self.state[6:9],\n            'uncertainty': np.sqrt(np.diag(self.covariance))\n        }\n\n# Example usage\nimu_vision_fusion = IMUVisionFusion()\n\n# Simulate sensor fusion over time\nfor step in range(100):\n    dt = 0.01  # 100Hz\n\n    # Simulate IMU data (with some motion)\n    imu_data = {\n        'ax': 0.1 + np.random.normal(0, 0.01),\n        'ay': 0.05 + np.random.normal(0, 0.01),\n        'az': -9.81 + np.random.normal(0, 0.01),  # Gravity\n        'wx': np.random.normal(0, 0.001),\n        'wy': np.random.normal(0, 0.001),\n        'wz': np.random.normal(0, 0.001)\n    }\n\n    # Predict with IMU\n    imu_vision_fusion.predict_with_imu(imu_data, dt)\n\n    # Occasionally update with vision (e.g., every 10 steps)\n    if step % 10 == 0:\n        vision_pos = np.array([\n            step * 0.001 + np.random.normal(0, 0.01),\n            step * 0.0005 + np.random.normal(0, 0.01),\n            0.8 + np.random.normal(0, 0.01)\n        ])\n        vision_cov = np.eye(3) * 0.01\n        imu_vision_fusion.update_with_vision(vision_pos, vision_cov)\n\n    # Occasionally update with depth\n    if step % 15 == 0:\n        true_distance = max(0.1, 5.0 - step * 0.001)  # Moving toward landmark\n        measured_distance = true_distance + np.random.normal(0, 0.02)\n        imu_vision_fusion.update_with_depth(measured_distance, 0.04)\n\nfinal_state = imu_vision_fusion.get_state()\nprint(f\"Final estimated position: {final_state['position']}\")\nprint(f\"Position uncertainty: {final_state['uncertainty'][:3]}\")\n"})}),"\n",(0,i.jsx)(e.h3,{id:"multi-imu-fusion",children:"Multi-IMU Fusion"}),"\n",(0,i.jsx)(e.p,{children:"Combining data from multiple IMUs for improved orientation estimation:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class MultiIMUFusion:\n    def __init__(self, num_imus):\n        self.num_imus = num_imus\n        self.imu_quaternions = [np.array([1, 0, 0, 0]) for _ in range(num_imus)]\n        self.imu_covariances = [np.eye(4) * 0.1 for _ in range(num_imus)]\n\n        # Global state (best estimate)\n        self.global_quaternion = np.array([1, 0, 0, 0])\n        self.global_covariance = np.eye(4) * 0.1\n\n    def update_imu_reading(self, imu_idx, accel, gyro, dt):\n        """Update individual IMU estimate"""\n        if imu_idx >= self.num_imus:\n            return\n\n        # Simple complementary filter for this IMU\n        # Normalize accelerometer\n        if np.linalg.norm(accel) > 0.1:\n            accel_norm = accel / np.linalg.norm(accel)\n        else:\n            accel_norm = np.array([0, 0, 1])  # Default to Z-axis\n\n        # Integrate gyroscope\n        gyro_quat = self._integrate_gyro(self.imu_quaternions[imu_idx], gyro, dt)\n\n        # Get gravity direction in IMU frame\n        gravity_in_imu = self._rotate_vector(np.array([0, 0, 1]),\n                                           self.imu_quaternions[imu_idx])\n\n        # Calculate error between measured and expected gravity\n        gravity_error = np.cross(gravity_in_imu, accel_norm)\n\n        # Complementary filter: blend gyro integration with accelerometer\n        kp = 0.1  # Proportional gain\n        correction = kp * gravity_error\n        correction_quat = np.array([0, correction[0], correction[1], correction[2]]) * 0.5\n\n        # Apply correction\n        corrected_quat = gyro_quat + correction_quat\n        corrected_quat = corrected_quat / np.linalg.norm(corrected_quat)\n\n        self.imu_quaternions[imu_idx] = corrected_quat\n\n    def _integrate_gyro(self, quat, gyro, dt):\n        """Integrate gyroscope to update quaternion"""\n        gyro_quat = np.array([0, gyro[0], gyro[1], gyro[2]])\n        quat_dot = 0.5 * self._quat_multiply(quat, gyro_quat)\n        new_quat = quat + quat_dot * dt\n        return new_quat / np.linalg.norm(new_quat)\n\n    def _rotate_vector(self, v, q):\n        """Rotate vector v by quaternion q"""\n        q_conj = np.array([q[0], -q[1], -q[2], -q[3]])\n        v_quat = np.array([0, v[0], v[1], v[2]])\n        temp = self._quat_multiply(q_conj, v_quat)\n        rotated = self._quat_multiply(temp, q)\n        return rotated[1:]\n\n    def _quat_multiply(self, q1, q2):\n        """Multiply two quaternions"""\n        w1, x1, y1, z1 = q1\n        w2, x2, y2, z2 = q2\n        w = w1*w2 - x1*x2 - y1*y2 - z1*z2\n        x = w1*x2 + x1*w2 + y1*z2 - z1*y2\n        y = w1*y2 - x1*z2 + y1*w2 + z1*x2\n        z = w1*z2 + x1*y2 - y1*x2 + z1*w2\n        return np.array([w, x, y, z])\n\n    def fuse_imu_data(self):\n        """Fuse all IMU estimates using weighted averaging"""\n        # Convert quaternions to a representation suitable for averaging\n        # Use the chordal distance approach for quaternion averaging\n\n        # Weights based on inverse covariance\n        weights = []\n        for cov in self.imu_covariances:\n            # Use trace of covariance as a simple uncertainty measure\n            uncertainty = np.trace(cov)\n            weight = 1.0 / max(uncertainty, 1e-6)\n            weights.append(weight)\n\n        # Normalize weights\n        total_weight = sum(weights)\n        weights = [w / total_weight for w in weights]\n\n        # Weighted quaternion averaging using Slerp-like approach\n        # For simplicity, we\'ll use a chordal L2 mean approach\n        avg_quat = np.zeros(4)\n        for i, (quat, weight) in enumerate(zip(self.imu_quaternions, weights)):\n            # Ensure quaternions have same handedness\n            if np.dot(avg_quat, quat) < 0:\n                quat = -quat\n            avg_quat += weight * quat\n\n        avg_quat = avg_quat / np.linalg.norm(avg_quat)\n        self.global_quaternion = avg_quat\n\n        # Update global covariance (simplified)\n        self.global_covariance = np.mean(self.imu_covariances, axis=0)\n\n    def get_global_orientation(self):\n        """Get fused orientation estimate"""\n        return {\n            \'quaternion\': self.global_quaternion.copy(),\n            \'covariance\': self.global_covariance.copy(),\n            \'euler_angles\': self._quat_to_euler(self.global_quaternion)\n        }\n\n    def _quat_to_euler(self, q):\n        """Convert quaternion to Euler angles (roll, pitch, yaw)"""\n        w, x, y, z = q\n        sinr_cosp = 2 * (w * x + y * z)\n        cosr_cosp = 1 - 2 * (x * x + y * y)\n        roll = np.arctan2(sinr_cosp, cosr_cosp)\n\n        sinp = 2 * (w * y - z * x)\n        if abs(sinp) >= 1:\n            pitch = np.sign(sinp) * np.pi / 2\n        else:\n            pitch = np.arcsin(sinp)\n\n        siny_cosp = 2 * (w * z + x * y)\n        cosy_cosp = 1 - 2 * (y * y + z * z)\n        yaw = np.arctan2(siny_cosp, cosy_cosp)\n\n        return np.array([roll, pitch, yaw])\n\n# Example: Fusing data from 3 IMUs (torso, left foot, right foot)\nmulti_imu = MultiIMUFusion(num_imus=3)\n\nfor step in range(100):\n    dt = 0.01\n\n    for i in range(3):\n        # Simulate IMU readings (with slight differences)\n        accel = np.array([0, 0, 9.81]) + np.random.normal(0, 0.1, 3)\n        gyro = np.random.normal(0, 0.01, 3)\n        multi_imu.update_imu_reading(i, accel, gyro, dt)\n\n    # Fuse all IMU data\n    multi_imu.fuse_imu_data()\n\nfinal_orientation = multi_imu.get_global_orientation()\nprint(f"Fused quaternion: {final_orientation[\'quaternion\']}")\nprint(f"Fused Euler angles (deg): {np.degrees(final_orientation[\'euler_angles\'])}")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"tactile-and-force-sensing-fusion",children:"Tactile and Force Sensing Fusion"}),"\n",(0,i.jsx)(e.p,{children:"Combining tactile and force sensors for manipulation:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"class TactileForceFusion:\n    def __init__(self):\n        self.tactile_data = np.zeros(64)  # Example: 64 taxels\n        self.force_torque = np.zeros(6)  # [Fx, Fy, Fz, Mx, My, Mz]\n        self.contact_points = []\n        self.grasp_quality = 0.0\n\n        # State for object properties estimation\n        self.object_properties = {\n            'compliance': 0.0,\n            'friction': 0.0,\n            'weight': 0.0,\n            'shape': 'unknown'\n        }\n\n    def update_tactile_data(self, tactile_readings):\n        \"\"\"Update tactile sensor readings\"\"\"\n        if len(tactile_readings) == len(self.tactile_data):\n            self.tactile_data = np.array(tactile_readings)\n        else:\n            # Handle different sized tactile arrays\n            min_len = min(len(tactile_readings), len(self.tactile_data))\n            self.tactile_data[:min_len] = tactile_readings[:min_len]\n\n    def update_force_torque(self, force_torque_readings):\n        \"\"\"Update force/torque sensor readings\"\"\"\n        if len(force_torque_readings) == 6:\n            self.force_torque = np.array(force_torque_readings)\n\n    def estimate_contact_info(self):\n        \"\"\"Estimate contact information from tactile data\"\"\"\n        # Find contact points (taxels with pressure above threshold)\n        contact_threshold = 0.1\n        contact_indices = np.where(self.tactile_data > contact_threshold)[0]\n\n        contact_points = []\n        total_force = 0\n\n        if len(contact_indices) > 0:\n            # For a 8x8 taxel array, convert linear indices to 2D positions\n            for idx in contact_indices:\n                row = idx // 8\n                col = idx % 8\n                # Convert to physical coordinates (e.g., meters)\n                x = col * 0.005  # 5mm spacing\n                y = row * 0.005\n                pressure = self.tactile_data[idx]\n\n                contact_points.append({\n                    'position': np.array([x, y]),\n                    'pressure': pressure,\n                    'taxel_idx': idx\n                })\n\n                total_force += pressure\n\n        self.contact_points = contact_points\n\n        # Calculate center of pressure\n        if total_force > 0:\n            cop_x = sum(c['position'][0] * c['pressure'] for c in contact_points) / total_force\n            cop_y = sum(c['position'][1] * c['pressure'] for c in contact_points) / total_force\n            self.center_of_pressure = np.array([cop_x, cop_y])\n        else:\n            self.center_of_pressure = np.array([0.0, 0.0])\n\n    def estimate_grasp_quality(self):\n        \"\"\"Estimate grasp quality from tactile and force data\"\"\"\n        # Calculate various grasp quality metrics\n\n        # 1. Contact distribution quality\n        contact_area = len(self.contact_points)\n        if contact_area > 0:\n            # Calculate how spread out the contacts are\n            positions = np.array([c['position'] for c in self.contact_points])\n            if len(positions) > 1:\n                centroid = np.mean(positions, axis=0)\n                distances = np.linalg.norm(positions - centroid, axis=1)\n                spread = np.mean(distances)\n            else:\n                spread = 0\n        else:\n            spread = 0\n\n        # 2. Force distribution quality\n        force_magnitude = np.linalg.norm(self.force_torque[:3])\n\n        # 3. Tactile force uniformity\n        active_pressures = [c['pressure'] for c in self.contact_points]\n        if len(active_pressures) > 1:\n            uniformity = 1.0 - np.std(active_pressures) / (np.mean(active_pressures) + 1e-6)\n        else:\n            uniformity = 1.0 if len(active_pressures) > 0 else 0.0\n\n        # Combine metrics into overall quality score\n        self.grasp_quality = (0.3 * min(1.0, contact_area / 10.0) +  # Adequate contacts\n                             0.3 * min(1.0, spread / 0.02) +         # Good spread (2cm)\n                             0.2 * min(1.0, force_magnitude / 50.0) + # Adequate force\n                             0.2 * uniformity)                       # Uniform pressure\n\n        return self.grasp_quality\n\n    def detect_slip(self, time_window=0.1):\n        \"\"\"Detect slip using tactile pattern analysis\"\"\"\n        # This would require temporal analysis\n        # For now, return a simple slip detection based on pressure changes\n        if len(self.contact_points) == 0:\n            return False\n\n        # Calculate pressure gradient across contact area\n        positions = np.array([c['position'] for c in self.contact_points])\n        pressures = np.array([c['pressure'] for c in self.contact_points])\n\n        if len(positions) > 2:\n            # Calculate spatial gradient of pressure\n            from scipy.spatial.distance import pdist, squareform\n            distances = squareform(pdist(positions))\n            pressure_diffs = squareform(pdist(pressures.reshape(-1, 1)))\n\n            # High pressure gradients might indicate slip\n            avg_gradient = np.mean(pressure_diffs[distances > 1e-6]) if np.any(distances > 1e-6) else 0\n\n            return avg_gradient > 5.0  # Threshold for slip detection\n\n        return False\n\n    def estimate_object_properties(self):\n        \"\"\"Estimate object properties from tactile and force data\"\"\"\n        if len(self.contact_points) == 0:\n            return self.object_properties\n\n        # Estimate compliance (softness) from pressure distribution\n        pressures = [c['pressure'] for c in self.contact_points]\n        avg_pressure = np.mean(pressures) if pressures else 0\n\n        # Higher variance in pressure might indicate softer material\n        if len(pressures) > 1:\n            pressure_variance = np.var(pressures)\n            self.object_properties['compliance'] = pressure_variance / (avg_pressure + 1e-6)\n        else:\n            self.object_properties['compliance'] = 0\n\n        # Estimate friction from force data\n        normal_force = max(0.1, abs(self.force_torque[2]))  # Fz component\n        tangential_force = np.linalg.norm(self.force_torque[:2])  # Fx, Fy components\n        friction_coeff = tangential_force / normal_force\n        self.object_properties['friction'] = min(1.0, friction_coeff)\n\n        # Estimate weight from gravitational force\n        self.object_properties['weight'] = max(0, -self.force_torque[2])  # Assuming object pulls down\n\n        # Estimate shape from contact pattern\n        if len(self.contact_points) > 2:\n            positions = np.array([c['position'] for c in self.contact_points])\n            from scipy.spatial.distance import pdist\n            all_distances = pdist(positions)\n            max_distance = np.max(all_distances) if len(all_distances) > 0 else 0\n\n            if max_distance < 0.01:  # Small contact area\n                self.object_properties['shape'] = 'small'\n            elif max_distance < 0.03:  # Medium contact area\n                self.object_properties['shape'] = 'medium'\n            else:  # Large contact area\n                self.object_properties['shape'] = 'large'\n\n        return self.object_properties\n\n    def get_fusion_results(self):\n        \"\"\"Get all fusion results\"\"\"\n        self.estimate_contact_info()\n        quality = self.estimate_grasp_quality()\n        properties = self.estimate_object_properties()\n\n        return {\n            'grasp_quality': quality,\n            'contact_points': self.contact_points,\n            'center_of_pressure': getattr(self, 'center_of_pressure', np.array([0, 0])),\n            'object_properties': properties,\n            'slip_detected': self.detect_slip(),\n            'tactile_data': self.tactile_data.copy(),\n            'force_torque': self.force_torque.copy()\n        }\n\n# Example usage\ntactile_force_fusion = TactileForceFusion()\n\n# Simulate tactile data (e.g., from a 8x8 taxel array)\ntactile_readings = np.random.random(64) * 5  # 0-5 N pressure\ntactile_readings[10:20] = 10  # Simulate contact area\ntactile_force_fusion.update_tactile_data(tactile_readings)\n\n# Simulate force/torque data\nforce_torque_readings = [2.0, 1.0, -5.0, 0.1, 0.05, 0.02]  # [Fx, Fy, Fz, Mx, My, Mz]\ntactile_force_fusion.update_force_torque(force_torque_readings)\n\nresults = tactile_force_fusion.get_fusion_results()\nprint(f\"Grasp quality: {results['grasp_quality']:.3f}\")\nprint(f\"Object compliance: {results['object_properties']['compliance']:.3f}\")\nprint(f\"Object friction: {results['object_properties']['friction']:.3f}\")\nprint(f\"Estimated weight: {results['object_properties']['weight']:.3f} N\")\nprint(f\"Number of contact points: {len(results['contact_points'])}\")\nprint(f\"Slip detected: {results['slip_detected']}\")\n"})}),"\n",(0,i.jsx)(e.h2,{id:"advanced-fusion-techniques",children:"Advanced Fusion Techniques"}),"\n",(0,i.jsx)(e.h3,{id:"information-filter",children:"Information Filter"}),"\n",(0,i.jsx)(e.p,{children:"The information filter is the inverse of the Kalman filter, useful for certain fusion scenarios:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class InformationFilter:\n    def __init__(self, state_dim):\n        self.state_dim = state_dim\n        self.information_state = np.zeros(state_dim)  # \u03be = Y * x (information state)\n        self.information_matrix = np.zeros((state_dim, state_dim))  # Y = P^(-1) (information matrix)\n\n        # Initialize with high uncertainty (low information)\n        self.information_matrix = np.eye(state_dim) * 0.001\n\n    def predict(self, F, Q):\n        """Prediction step in information form"""\n        # Convert to covariance form temporarily\n        covariance = np.linalg.inv(self.information_matrix)\n        state = np.linalg.solve(self.information_matrix, self.information_state)\n\n        # Standard Kalman prediction\n        predicted_state = F @ state\n        predicted_covariance = F @ covariance @ F.T + Q\n\n        # Convert back to information form\n        self.information_matrix = np.linalg.inv(predicted_covariance)\n        self.information_state = self.information_matrix @ predicted_state\n\n    def update(self, measurement, H, R):\n        """Update step - incorporate measurement"""\n        # Information contribution from measurement\n        innovation_cov = H @ np.linalg.inv(self.information_matrix) @ H.T + R\n        information_contribution = H.T @ np.linalg.inv(innovation_cov) @ H\n        state_contribution = H.T @ np.linalg.inv(innovation_cov) @ measurement\n\n        # Update information state and matrix\n        self.information_matrix += information_contribution\n        self.information_state += state_contribution\n\n    def get_state_and_covariance(self):\n        """Convert back to standard form"""\n        covariance = np.linalg.inv(self.information_matrix)\n        state = np.linalg.solve(self.information_matrix, self.information_state)\n        return state, covariance\n\n# Example: Information filter for sensor fusion\ninfo_filter = InformationFilter(state_dim=2)  # [x, y] position\n\n# Simulate measurements from different sensors\nmeasurements = [\n    (np.array([1.0, 0.1]), np.array([[1, 0], [0, 1]]), np.eye(2) * 0.1),   # High-accuracy sensor\n    (np.array([1.1, 0.05]), np.array([[1, 0], [0, 1]]), np.eye(2) * 0.5),  # Low-accuracy sensor\n    (np.array([0.95, 0.15]), np.array([[1, 0], [0, 1]]), np.eye(2) * 0.2)  # Medium-accuracy sensor\n]\n\nfor measurement, H, R in measurements:\n    info_filter.update(measurement, H, R)\n\nfinal_state, final_cov = info_filter.get_state_and_covariance()\nprint(f"Fused position: {final_state}")\nprint(f"Uncertainty: {np.sqrt(np.diag(final_cov))}")\n'})}),"\n",(0,i.jsx)(e.h3,{id:"covariance-intersection",children:"Covariance Intersection"}),"\n",(0,i.jsx)(e.p,{children:"For fusing estimates when the correlation between sensors is unknown:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'def covariance_intersection(estimate1, cov1, estimate2, cov2):\n    """\n    Fuse two estimates using Covariance Intersection when correlation is unknown\n    """\n    # Compute fusion weights\n    S1 = np.linalg.inv(cov1)\n    S2 = np.linalg.inv(cov2)\n\n    # The CI method finds optimal omega to minimize the trace of the fused covariance\n    # This is a simplified approach - in practice, you\'d solve for optimal omega\n    omega = 0.5  # Equal weighting as a simple approach\n\n    # Compute fused information matrix\n    S_fused = omega * S1 + (1 - omega) * S2\n\n    # Compute fused estimate\n    info1 = S1 @ estimate1\n    info2 = S2 @ estimate2\n    info_fused = omega * info1 + (1 - omega) * info2\n\n    # Convert back to state and covariance\n    cov_fused = np.linalg.inv(S_fused)\n    estimate_fused = cov_fused @ info_fused\n\n    return estimate_fused, cov_fused\n\n# Example: Fusing two position estimates with unknown correlation\nest1 = np.array([1.0, 0.0])\ncov1 = np.array([[0.1, 0.02], [0.02, 0.1]])\n\nest2 = np.array([1.2, 0.1])\ncov2 = np.array([[0.3, 0.05], [0.05, 0.3]])\n\nfused_est, fused_cov = covariance_intersection(est1, cov1, est2, cov2)\nprint(f"CI fused estimate: {fused_est}")\nprint(f"CI fused covariance: {fused_cov}")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"real-time-implementation-considerations",children:"Real-Time Implementation Considerations"}),"\n",(0,i.jsx)(e.h3,{id:"efficient-data-structures",children:"Efficient Data Structures"}),"\n",(0,i.jsx)(e.p,{children:"For real-time fusion, efficient data structures are crucial:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"from collections import deque\nimport time\n\nclass RealTimeFusionEngine:\n    def __init__(self, max_buffer_size=100):\n        self.max_buffer_size = max_buffer_size\n        self.sensors = {}\n        self.data_buffers = {}\n        self.time_stamps = deque(maxlen=max_buffer_size)\n        self.fusion_results = {}\n        self.last_fusion_time = time.time()\n\n    def register_sensor(self, sensor_name, topic_type, callback=None):\n        \"\"\"Register a sensor with the fusion engine\"\"\"\n        self.sensors[sensor_name] = {\n            'type': topic_type,\n            'callback': callback,\n            'buffer': deque(maxlen=self.max_buffer_size),\n            'time_stamps': deque(maxlen=self.max_buffer_size)\n        }\n\n    def add_sensor_data(self, sensor_name, data, timestamp=None):\n        \"\"\"Add sensor data to the fusion engine\"\"\"\n        if timestamp is None:\n            timestamp = time.time()\n\n        if sensor_name in self.sensors:\n            self.sensors[sensor_name]['buffer'].append(data)\n            self.sensors[sensor_name]['time_stamps'].append(timestamp)\n\n    def synchronize_data(self, time_window=0.01):\n        \"\"\"Synchronize data from different sensors within time window\"\"\"\n        if not self.sensors:\n            return None\n\n        # Find the latest common time\n        latest_times = {}\n        for sensor_name, sensor_data in self.sensors.items():\n            if sensor_data['buffer']:\n                latest_times[sensor_name] = sensor_data['time_stamps'][-1]\n\n        if not latest_times:\n            return None\n\n        sync_time = min(latest_times.values())\n\n        # Get data closest to sync time for each sensor\n        synchronized_data = {}\n        for sensor_name, sensor_data in self.sensors.items():\n            buffer = sensor_data['buffer']\n            time_stamps = sensor_data['time_stamps']\n\n            if not buffer:\n                continue\n\n            # Find closest timestamp within window\n            closest_idx = None\n            closest_diff = float('inf')\n\n            for i, ts in enumerate(time_stamps):\n                diff = abs(ts - sync_time)\n                if diff < closest_diff and diff < time_window:\n                    closest_diff = diff\n                    closest_idx = i\n\n            if closest_idx is not None:\n                synchronized_data[sensor_name] = {\n                    'data': buffer[closest_idx],\n                    'timestamp': time_stamps[closest_idx],\n                    'age': time.time() - time_stamps[closest_idx]\n                }\n\n        return synchronized_data\n\n    def run_fusion_cycle(self):\n        \"\"\"Run one cycle of sensor fusion\"\"\"\n        start_time = time.time()\n\n        # Synchronize data\n        sync_data = self.synchronize_data()\n        if not sync_data:\n            return None\n\n        # Perform fusion based on available data\n        fusion_result = self.perform_fusion(sync_data)\n\n        # Update timing statistics\n        self.last_fusion_time = time.time()\n        self.fusion_results['cycle_time'] = time.time() - start_time\n        self.fusion_results['data_sync'] = sync_data\n\n        return fusion_result\n\n    def perform_fusion(self, sync_data):\n        \"\"\"Perform the actual fusion algorithm\"\"\"\n        # This would contain the specific fusion algorithm\n        # For example, a Kalman filter or other fusion method\n        result = {\n            'timestamp': time.time(),\n            'sensors_used': list(sync_data.keys()),\n            'data_quality': {}\n        }\n\n        # Calculate data quality metrics\n        for sensor_name, sensor_data in sync_data.items():\n            age = sensor_data['age']\n            # Data becomes less reliable as it gets older\n            quality = max(0, 1 - age)  # Simple quality model\n            result['data_quality'][sensor_name] = quality\n\n        return result\n\n    def get_fusion_performance(self):\n        \"\"\"Get performance statistics\"\"\"\n        return {\n            'cycle_time': getattr(self, 'last_fusion_time', 0),\n            'sensors_registered': len(self.sensors),\n            'data_quality': self.fusion_results.get('data_quality', {}),\n            'fusion_rate': 1.0 / self.fusion_results.get('cycle_time', 1.0) if self.fusion_results.get('cycle_time') else 0\n        }\n\n# Example usage\nfusion_engine = RealTimeFusionEngine(max_buffer_size=50)\n\n# Register different sensor types\nfusion_engine.register_sensor('imu', 'sensor_msgs/Imu')\nfusion_engine.register_sensor('camera', 'sensor_msgs/Image')\nfusion_engine.register_sensor('lidar', 'sensor_msgs/PointCloud2')\n\n# Simulate adding data\nfor i in range(10):\n    fusion_engine.add_sensor_data('imu', {'acc': [0, 0, 9.8], 'gyro': [0, 0, 0]})\n    fusion_engine.add_sensor_data('camera', {'objects': []})\n    fusion_engine.add_sensor_data('lidar', {'points': []})\n\n    result = fusion_engine.run_fusion_cycle()\n    if result:\n        print(f\"Fusion cycle {i}: {len(result['sensors_used'])} sensors used\")\n        print(f\"Data qualities: {result['data_quality']}\")\n"})}),"\n",(0,i.jsx)(e.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,i.jsx)(e.h3,{id:"ros-2-sensor-fusion-node",children:"ROS 2 Sensor Fusion Node"}),"\n",(0,i.jsx)(e.p,{children:"Integrating sensor fusion with ROS 2 for system-wide state estimation:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu, JointState, PointCloud2\nfrom geometry_msgs.msg import PoseWithCovarianceStamped, TwistWithCovarianceStamped\nfrom nav_msgs.msg import Odometry\nfrom std_msgs.msg import Header\nimport message_filters\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__('sensor_fusion_node')\n\n        # Create publishers for fused state\n        self.odom_pub = self.create_publisher(Odometry, '/fused_odom', 10)\n        self.pose_pub = self.create_publisher(PoseWithCovarianceStamped, '/fused_pose', 10)\n        self.twist_pub = self.create_publisher(TwistWithCovarianceStamped, '/fused_twist', 10)\n\n        # Create subscribers with time synchronization\n        self.imu_sub = message_filters.Subscriber(self, Imu, '/imu/data')\n        self.joint_sub = message_filters.Subscriber(self, JointState, '/joint_states')\n\n        # Synchronize messages based on timestamps\n        self.ts = message_filters.ApproximateTimeSynchronizer(\n            [self.imu_sub, self.joint_sub],\n            queue_size=10,\n            slop=0.1  # 100ms tolerance\n        )\n        self.ts.registerCallback(self.sync_callback)\n\n        # Initialize fusion components\n        self.kalman_filter = KalmanFilter(state_dim=13, measurement_dim=6)  # Pose + Twist\n        self.multi_imu_fusion = MultiIMUFusion(num_imus=3)\n        self.tactile_force_fusion = TactileForceFusion()\n\n        # Robot state\n        self.position = np.zeros(3)\n        self.orientation = np.array([0, 0, 0, 1])  # x, y, z, w\n        self.linear_velocity = np.zeros(3)\n        self.angular_velocity = np.zeros(3)\n\n        # TF broadcaster\n        from tf2_ros import TransformBroadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        self.get_logger().info('Sensor fusion node initialized')\n\n    def sync_callback(self, imu_msg, joint_msg):\n        \"\"\"Callback for synchronized IMU and joint messages\"\"\"\n        # Extract IMU data\n        imu_data = {\n            'ax': imu_msg.linear_acceleration.x,\n            'ay': imu_msg.linear_acceleration.y,\n            'az': imu_msg.linear_acceleration.z,\n            'wx': imu_msg.angular_velocity.x,\n            'wy': imu_msg.angular_velocity.y,\n            'wz': imu_msg.angular_velocity.z\n        }\n\n        # Extract joint data\n        joint_positions = dict(zip(joint_msg.name, joint_msg.position))\n        joint_velocities = dict(zip(joint_msg.name, joint_msg.velocity))\n\n        # Perform sensor fusion\n        self.update_fusion(imu_data, joint_positions, joint_velocities)\n\n        # Publish results\n        self.publish_fused_state(imu_msg.header.stamp)\n\n    def update_fusion(self, imu_data, joint_positions, joint_velocities):\n        \"\"\"Update state estimate using sensor fusion\"\"\"\n        # Update IMU-based orientation\n        dt = 0.01  # Assuming 100Hz rate\n        self.multi_imu_fusion.update_imu_reading(0,\n            [imu_data['ax'], imu_data['ay'], imu_data['az']],\n            [imu_data['wx'], imu_data['wy'], imu_data['wz']],\n            dt)\n\n        # Estimate position from joint encoders (forward kinematics)\n        # This is a simplified example - in practice, you'd use full FK\n        if 'left_knee' in joint_positions and 'right_knee' in joint_positions:\n            # Simple leg extension model\n            left_knee_angle = joint_positions['left_knee']\n            right_knee_angle = joint_positions['right_knee']\n\n            # Update position based on leg extension (simplified)\n            self.position[2] += (np.sin(left_knee_angle) + np.sin(right_knee_angle)) * 0.001\n\n        # Update Kalman filter\n        measurement = np.array([\n            self.position[0], self.position[1], self.position[2],  # Position\n            self.orientation[0], self.orientation[1], self.orientation[2]  # Orientation (simplified)\n        ])\n\n        self.kalman_filter.update(measurement)\n\n        # Predict next state with IMU\n        self.kalman_filter.predict(dt)\n\n        # Update with fused orientation\n        fused_orientation = self.multi_imu_fusion.get_global_orientation()\n        self.orientation = fused_orientation['quaternion']\n\n    def publish_fused_state(self, stamp):\n        \"\"\"Publish fused state to ROS topics\"\"\"\n        # Create and publish odometry message\n        odom_msg = Odometry()\n        odom_msg.header.stamp = stamp\n        odom_msg.header.frame_id = 'odom'\n        odom_msg.child_frame_id = 'base_link'\n\n        # Set pose\n        odom_msg.pose.pose.position.x = float(self.position[0])\n        odom_msg.pose.pose.position.y = float(self.position[1])\n        odom_msg.pose.pose.position.z = float(self.position[2])\n\n        odom_msg.pose.pose.orientation.x = float(self.orientation[0])\n        odom_msg.pose.pose.orientation.y = float(self.orientation[1])\n        odom_msg.pose.pose.orientation.z = float(self.orientation[2])\n        odom_msg.pose.pose.orientation.w = float(self.orientation[3])\n\n        # Set twist (velocity)\n        odom_msg.twist.twist.linear.x = float(self.linear_velocity[0])\n        odom_msg.twist.twist.linear.y = float(self.linear_velocity[1])\n        odom_msg.twist.twist.linear.z = float(self.linear_velocity[2])\n\n        odom_msg.twist.twist.angular.x = float(self.angular_velocity[0])\n        odom_msg.twist.twist.angular.y = float(self.angular_velocity[1])\n        odom_msg.twist.twist.angular.z = float(self.angular_velocity[2])\n\n        # Set covariances (placeholder values)\n        odom_msg.pose.covariance = [0.1] * 36  # Placeholder\n        odom_msg.twist.covariance = [0.1] * 36  # Placeholder\n\n        self.odom_pub.publish(odom_msg)\n\n        # Publish pose with covariance\n        pose_msg = PoseWithCovarianceStamped()\n        pose_msg.header = odom_msg.header\n        pose_msg.pose = odom_msg.pose\n        self.pose_pub.publish(pose_msg)\n\n        # Publish twist with covariance\n        twist_msg = TwistWithCovarianceStamped()\n        twist_msg.header = odom_msg.header\n        twist_msg.twist = odom_msg.twist\n        self.twist_pub.publish(twist_msg)\n\n        # Broadcast TF\n        self.broadcast_transform(odom_msg)\n\n    def broadcast_transform(self, odom_msg):\n        \"\"\"Broadcast transform from odom to base_link\"\"\"\n        from geometry_msgs.msg import TransformStamped\n\n        t = TransformStamped()\n        t.header.stamp = odom_msg.header.stamp\n        t.header.frame_id = 'odom'\n        t.child_frame_id = 'base_link'\n\n        t.transform.translation.x = odom_msg.pose.pose.position.x\n        t.transform.translation.y = odom_msg.pose.pose.position.y\n        t.transform.translation.z = odom_msg.pose.pose.position.z\n\n        t.transform.rotation = odom_msg.pose.pose.orientation\n\n        self.tf_broadcaster.sendTransform(t)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    fusion_node = SensorFusionNode()\n\n    try:\n        rclpy.spin(fusion_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        fusion_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(e.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(e.p,{children:"Sensor fusion is a critical capability for humanoid robots, enabling them to create coherent, accurate, and robust representations of their state and environment from multiple, often noisy and incomplete, sensor sources. Through mathematical frameworks like Kalman filtering, particle filtering, and information fusion, robots can optimally combine data from different modalities."}),"\n",(0,i.jsx)(e.p,{children:"The choice of fusion algorithm depends on the specific application requirements, including the nature of the sensors, the dynamics of the system, and real-time constraints. Modern humanoid robots typically employ multiple fusion techniques simultaneously, with different algorithms handling different aspects of perception and state estimation."}),"\n",(0,i.jsx)(e.p,{children:"The integration of sensor fusion with ROS 2 enables system-wide state estimation that can be utilized by various robot subsystems, from low-level control to high-level planning and decision-making."})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(m,{...n})}):m(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>r});var i=t(6540);const s={},a=i.createContext(s);function o(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);