"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[285],{948:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>i,metadata:()=>r,toc:()=>p});var s=t(4848),o=t(8453);const i={},a="Practical Examples and Exercises: Environmental Perception Implementation",r={id:"module-3-perception-sensing/practical-examples",title:"Practical Examples and Exercises: Environmental Perception Implementation",description:"Introduction: From Theory to Practice",source:"@site/docs/module-3-perception-sensing/practical-examples.md",sourceDirName:"module-3-perception-sensing",slug:"/module-3-perception-sensing/practical-examples",permalink:"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/practical-examples",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-perception-sensing/practical-examples.md",tags:[],version:"current",frontMatter:{},sidebar:"textbookSidebar",previous:{title:"Environmental Perception and Mapping",permalink:"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/environmental-perception"},next:{title:"Module 4: Locomotion and Mobility",permalink:"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-4-locomotion-mobility/"}},l={},p=[{value:"Introduction: From Theory to Practice",id:"introduction-from-theory-to-practice",level:2},{value:"Exercise 1: Point Cloud Processing Pipeline",id:"exercise-1-point-cloud-processing-pipeline",level:2},{value:"Exercise 2: 3D Object Recognition System",id:"exercise-2-3d-object-recognition-system",level:2},{value:"Exercise 3: Occupancy Grid Mapping",id:"exercise-3-occupancy-grid-mapping",level:2},{value:"Exercise 4: Path Planning with A* Algorithm",id:"exercise-4-path-planning-with-a-algorithm",level:2},{value:"Exercise 5: Sensor Fusion for State Estimation",id:"exercise-5-sensor-fusion-for-state-estimation",level:2},{value:"Exercise 6: Complete Environmental Perception Pipeline",id:"exercise-6-complete-environmental-perception-pipeline",level:2},{value:"Exercise 7: Real-time Perception Simulation",id:"exercise-7-real-time-perception-simulation",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.h1,{id:"practical-examples-and-exercises-environmental-perception-implementation",children:"Practical Examples and Exercises: Environmental Perception Implementation"}),"\n",(0,s.jsx)(e.h2,{id:"introduction-from-theory-to-practice",children:"Introduction: From Theory to Practice"}),"\n",(0,s.jsx)(e.p,{children:"This section provides hands-on examples and exercises that demonstrate the implementation of environmental perception concepts using Python and ROS 2. Through these practical examples, students will gain experience implementing the theoretical concepts covered in previous sections, from basic point cloud processing to advanced SLAM and navigation systems."}),"\n",(0,s.jsx)(e.h2,{id:"exercise-1-point-cloud-processing-pipeline",children:"Exercise 1: Point Cloud Processing Pipeline"}),"\n",(0,s.jsx)(e.p,{children:"Let's start with a complete point cloud processing pipeline:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import DBSCAN\nfrom scipy.spatial import ConvexHull\n\nclass PointCloudPipeline:\n    def __init__(self):\n        self.points = None\n        self.colors = None\n        self.processed_points = None\n        self.processed_colors = None\n\n    def generate_sample_point_cloud(self, num_points=1000):\n        \"\"\"Generate a sample point cloud with multiple objects\"\"\"\n        np.random.seed(42)  # For reproducible results\n\n        # Create a plane (floor)\n        floor_points = []\n        for _ in range(300):\n            x = np.random.uniform(-2, 2)\n            y = np.random.uniform(-2, 2)\n            z = -1 + np.random.normal(0, 0.01)  # Floor at z=-1\n            floor_points.append([x, y, z])\n\n        # Create a box\n        box_points = []\n        for _ in range(200):\n            x = np.random.uniform(0.5, 1.5)\n            y = np.random.uniform(-0.5, 0.5)\n            z = np.random.uniform(-0.5, 0.5)\n            box_points.append([x, y, z])\n\n        # Create a cylinder\n        cylinder_points = []\n        for _ in range(200):\n            angle = np.random.uniform(0, 2*np.pi)\n            radius = np.random.uniform(0, 0.3)\n            x = -1 + radius * np.cos(angle)\n            y = radius * np.sin(angle)\n            z = np.random.uniform(-0.5, 0.5)\n            cylinder_points.append([x, y, z])\n\n        # Add some noise\n        noise_points = []\n        for _ in range(100):\n            x = np.random.uniform(-3, 3)\n            y = np.random.uniform(-3, 3)\n            z = np.random.uniform(-2, 2)\n            noise_points.append([x, y, z])\n\n        # Combine all points\n        all_points = floor_points + box_points + cylinder_points + noise_points\n        self.points = np.array(all_points)\n\n        # Generate colors based on Z-height\n        self.colors = plt.cm.viridis((self.points[:, 2] - np.min(self.points[:, 2])) /\n                                   (np.max(self.points[:, 2]) - np.min(self.points[:, 2])))\n\n        return self.points, self.colors\n\n    def downsample(self, voxel_size=0.05):\n        \"\"\"Downsample point cloud using voxel grid filter\"\"\"\n        if self.points is None:\n            return None\n\n        # Create voxel grid\n        min_bound = np.min(self.points, axis=0)\n        max_bound = np.max(self.points, axis=0)\n        dims = np.ceil((max_bound - min_bound) / voxel_size).astype(int)\n\n        # Create voxel hash map\n        voxel_map = {}\n        for i, point in enumerate(self.points):\n            voxel_idx = ((point - min_bound) / voxel_size).astype(int)\n            voxel_key = tuple(voxel_idx)\n\n            if voxel_key not in voxel_map:\n                voxel_map[voxel_key] = {'points': [], 'colors': [] if self.colors is not None else None}\n\n            voxel_map[voxel_key]['points'].append(point)\n            if self.colors is not None:\n                voxel_map[voxel_key]['colors'].append(self.colors[i])\n\n        # Take centroid of each voxel\n        downsampled_points = []\n        downsampled_colors = []\n\n        for voxel_data in voxel_map.values():\n            centroid = np.mean(voxel_data['points'], axis=0)\n            downsampled_points.append(centroid)\n\n            if self.colors is not None:\n                color_centroid = np.mean(voxel_data['colors'], axis=0)\n                downsampled_colors.append(color_centroid)\n\n        self.processed_points = np.array(downsampled_points)\n        if self.colors is not None:\n            self.processed_colors = np.array(downsampled_colors)\n\n        return self.processed_points, self.processed_colors\n\n    def remove_outliers(self, method='statistical', k=20, std_ratio=2.0):\n        \"\"\"Remove outlier points\"\"\"\n        if self.processed_points is None:\n            self.processed_points = self.points.copy()\n            self.processed_colors = self.colors.copy() if self.colors is not None else None\n\n        if method == 'statistical':\n            distances = []\n            for i, point in enumerate(self.processed_points):\n                neighbor_distances = np.linalg.norm(self.processed_points - point, axis=1)\n                k_nearest = np.partition(neighbor_distances, k+1)[1:k+1]\n                avg_distance = np.mean(k_nearest)\n                distances.append(avg_distance)\n\n            distances = np.array(distances)\n            mean_dist = np.mean(distances)\n            std_dist = np.std(distances)\n\n            valid_indices = distances < (mean_dist + std_ratio * std_dist)\n            self.processed_points = self.processed_points[valid_indices]\n            if self.processed_colors is not None:\n                self.processed_colors = self.processed_colors[valid_indices]\n\n        return self.processed_points, self.processed_colors\n\n    def segment_planes(self, distance_threshold=0.01, min_points=100):\n        \"\"\"Segment planar surfaces using RANSAC\"\"\"\n        if self.processed_points is None:\n            return []\n\n        remaining_points = self.processed_points.copy()\n        planes = []\n\n        while len(remaining_points) > min_points:\n            best_model = None\n            best_inliers = []\n            best_score = 0\n\n            for _ in range(100):  # RANSAC iterations\n                sample_indices = np.random.choice(len(remaining_points), 3, replace=False)\n                sample_points = remaining_points[sample_indices]\n\n                v1 = sample_points[1] - sample_points[0]\n                v2 = sample_points[2] - sample_points[0]\n                normal = np.cross(v1, v2)\n                if np.linalg.norm(normal) < 1e-6:\n                    continue\n\n                normal = normal / np.linalg.norm(normal)\n                d = -np.dot(normal, sample_points[0])\n\n                distances = np.abs(np.dot(remaining_points, normal) + d)\n                inliers = remaining_points[distances < distance_threshold]\n\n                if len(inliers) > len(best_inliers):\n                    best_inliers = inliers\n                    best_model = (normal, d)\n\n            if len(best_inliers) >= min_points and best_model is not None:\n                planes.append({\n                    'model': best_model,\n                    'inliers': best_inliers,\n                    'center': np.mean(best_inliers, axis=0),\n                    'normal': best_model[0],\n                    'd': best_model[1]\n                })\n\n                distances = np.abs(np.dot(remaining_points, best_model[0]) + best_model[1])\n                remaining_points = remaining_points[distances >= distance_threshold]\n            else:\n                break\n\n        return planes\n\n    def segment_objects(self, eps=0.05, min_samples=20):\n        \"\"\"Segment objects using DBSCAN clustering\"\"\"\n        if self.processed_points is None:\n            return []\n\n        # Apply DBSCAN clustering\n        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(self.processed_points)\n        labels = clustering.labels_\n\n        objects = []\n        for label in set(labels):\n            if label == -1:  # Noise points\n                continue\n\n            object_points = self.processed_points[labels == label]\n            object_colors = self.processed_colors[labels == label] if self.processed_colors is not None else None\n\n            center = np.mean(object_points, axis=0)\n            bbox_min = np.min(object_points, axis=0)\n            bbox_max = np.max(object_points, axis=0)\n            dimensions = bbox_max - bbox_min\n\n            objects.append({\n                'points': object_points,\n                'colors': object_colors,\n                'center': center,\n                'bbox_min': bbox_min,\n                'bbox_max': bbox_max,\n                'dimensions': dimensions,\n                'label': label\n            })\n\n        return objects\n\n    def visualize(self, title=\"Point Cloud\"):\n        \"\"\"Visualize the point cloud\"\"\"\n        fig = plt.figure(figsize=(12, 5))\n\n        # Original point cloud\n        ax1 = fig.add_subplot(121, projection='3d')\n        if self.points is not None:\n            ax1.scatter(self.points[:, 0], self.points[:, 1], self.points[:, 2],\n                      c=self.colors, s=1, alpha=0.6)\n        ax1.set_title('Original Point Cloud')\n        ax1.set_xlabel('X')\n        ax1.set_ylabel('Y')\n        ax1.set_zlabel('Z')\n\n        # Processed point cloud\n        ax2 = fig.add_subplot(122, projection='3d')\n        if self.processed_points is not None:\n            ax2.scatter(self.processed_points[:, 0], self.processed_points[:, 1], self.processed_points[:, 2],\n                      c=self.processed_colors, s=1, alpha=0.6)\n        ax2.set_title('Processed Point Cloud')\n        ax2.set_xlabel('X')\n        ax2.set_ylabel('Y')\n        ax2.set_zlabel('Z')\n\n        plt.tight_layout()\n        plt.show()\n\n# Demonstrate the pipeline\npipeline = PointCloudPipeline()\n\n# Generate sample data\npoints, colors = pipeline.generate_sample_point_cloud()\nprint(f\"Generated {len(points)} points\")\n\n# Process the point cloud\ndownsampled_points, downsampled_colors = pipeline.downsample(voxel_size=0.05)\nprint(f\"After downsampling: {len(downsampled_points)} points\")\n\nfiltered_points, filtered_colors = pipeline.remove_outliers()\nprint(f\"After outlier removal: {len(filtered_points)} points\")\n\n# Segment planes\nplanes = pipeline.segment_planes()\nprint(f\"Detected {len(planes)} planes\")\n\n# Segment objects\nobjects = pipeline.segment_objects()\nprint(f\"Detected {len(objects)} objects\")\n\n# Visualize results\npipeline.visualize()\n\n# Print object information\nfor i, obj in enumerate(objects):\n    print(f\"Object {i}: Center={obj['center']}, Dimensions={obj['dimensions']}\")\n"})}),"\n",(0,s.jsx)(e.h2,{id:"exercise-2-3d-object-recognition-system",children:"Exercise 2: 3D Object Recognition System"}),"\n",(0,s.jsx)(e.p,{children:"Now let's implement a complete 3D object recognition system:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class ObjectRecognitionSystem:\n    def __init__(self):\n        self.known_objects = {}\n        self.feature_extractor = PointCloudPipeline()  # Reusing for simplicity\n\n    def extract_geometric_features(self, points):\n        \"\"\"Extract geometric features for object recognition\"\"\"\n        features = {}\n\n        # Statistical features\n        centroid = np.mean(points, axis=0)\n        centered_points = points - centroid\n\n        # Covariance-based features\n        cov_matrix = np.cov(centered_points.T)\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n        eigenvalues = np.sort(eigenvalues)[::-1]  # Sort in descending order\n\n        # Shape descriptors\n        features['linearity'] = (eigenvalues[0] - eigenvalues[1]) / eigenvalues[0]\n        features['planarity'] = (eigenvalues[1] - eigenvalues[2]) / eigenvalues[0]\n        features['scattering'] = eigenvalues[2] / eigenvalues[0]\n        features['omnivariance'] = np.cbrt(eigenvalues[0] * eigenvalues[1] * eigenvalues[2])\n        features['anisotropy'] = (eigenvalues[0] - eigenvalues[2]) / eigenvalues[0]\n\n        # Size features\n        bbox_min = np.min(points, axis=0)\n        bbox_max = np.max(points, axis=0)\n        dimensions = bbox_max - bbox_min\n        features['volume'] = np.prod(dimensions)\n        features['dimensions'] = dimensions\n\n        # Density features\n        try:\n            from scipy.spatial import ConvexHull\n            hull = ConvexHull(points)\n            convex_volume = hull.volume\n            features['compactness'] = features['volume'] / (convex_volume + 1e-6)\n        except:\n            # Fallback for collinear points\n            features['compactness'] = 0.0\n\n        return features\n\n    def register_object(self, object_name, point_cloud):\n        \"\"\"Register a known object with its features\"\"\"\n        features = self.extract_geometric_features(point_cloud)\n        self.known_objects[object_name] = features\n        print(f\"Registered object: {object_name}\")\n\n    def recognize_object(self, point_cloud, threshold=0.3):\n        \"\"\"Recognize object by comparing features with registered objects\"\"\"\n        if not self.known_objects:\n            return None, 0.0\n\n        query_features = self.extract_geometric_features(point_cloud)\n\n        best_match = None\n        best_score = 0.0\n\n        for obj_name, ref_features in self.known_objects.items():\n            score = self.compare_features(query_features, ref_features)\n            if score > best_score:\n                best_score = score\n                best_match = obj_name\n\n        if best_score > threshold:\n            return best_match, best_score\n        else:\n            return None, best_score\n\n    def compare_features(self, features1, features2):\n        \"\"\"Compare two feature sets\"\"\"\n        scores = []\n        weights = {\n            'linearity': 0.15,\n            'planarity': 0.15,\n            'scattering': 0.15,\n            'volume': 0.1,\n            'dimensions': 0.2,\n            'compactness': 0.15,\n            'anisotropy': 0.1\n        }\n\n        for feature_name, weight in weights.items():\n            if feature_name in features1 and feature_name in features2:\n                val1 = features1[feature_name]\n                val2 = features2[feature_name]\n\n                if isinstance(val1, (list, np.ndarray)):\n                    # Handle array features like dimensions\n                    diff = np.mean(np.abs(np.array(val1) - np.array(val2)))\n                    max_val = max(np.max(np.abs(val1)), np.max(np.abs(val2)), 1e-6)\n                    similarity = max(0, 1 - diff / max_val)\n                else:\n                    # Handle scalar features\n                    diff = abs(val1 - val2)\n                    max_val = max(abs(val1), abs(val2), 1e-6)\n                    similarity = max(0, 1 - diff / max_val)\n\n                scores.append(similarity * weight)\n\n        return sum(scores) if scores else 0.0\n\n    def generate_test_objects(self):\n        \"\"\"Generate test objects for recognition\"\"\"\n        # Create a cube\n        cube_points = []\n        for _ in range(500):\n            x = np.random.uniform(-0.5, 0.5)\n            y = np.random.uniform(-0.5, 0.5)\n            z = np.random.uniform(-0.5, 0.5)\n            cube_points.append([x, y, z])\n        cube_points = np.array(cube_points)\n\n        # Create a cylinder\n        cylinder_points = []\n        for _ in range(500):\n            angle = np.random.uniform(0, 2*np.pi)\n            radius = np.random.uniform(0, 0.5)\n            x = radius * np.cos(angle)\n            y = radius * np.sin(angle)\n            z = np.random.uniform(-0.5, 0.5)\n            cylinder_points.append([x, y, z])\n        cylinder_points = np.array(cylinder_points)\n\n        # Create a sphere\n        sphere_points = []\n        for _ in range(500):\n            vec = np.random.normal(0, 1, 3)\n            vec = vec / np.linalg.norm(vec) * np.random.uniform(0, 0.5)\n            sphere_points.append(vec)\n        sphere_points = np.array(sphere_points)\n\n        return {\n            'cube': cube_points,\n            'cylinder': cylinder_points,\n            'sphere': sphere_points\n        }\n\n# Demonstrate object recognition\nrecognition_system = ObjectRecognitionSystem()\n\n# Generate test objects\ntest_objects = recognition_system.generate_test_objects()\n\n# Register known objects\nfor obj_name, points in test_objects.items():\n    recognition_system.register_object(obj_name, points)\n\n# Test recognition\nprint(\"\\nTesting object recognition:\")\nfor obj_name, points in test_objects.items():\n    # Add some noise to test object\n    noisy_points = points + np.random.normal(0, 0.05, points.shape)\n\n    recognized_obj, confidence = recognition_system.recognize_object(noisy_points)\n    print(f\"Test {obj_name}: Recognized as '{recognized_obj}' with confidence {confidence:.3f}\")\n"})}),"\n",(0,s.jsx)(e.h2,{id:"exercise-3-occupancy-grid-mapping",children:"Exercise 3: Occupancy Grid Mapping"}),"\n",(0,s.jsx)(e.p,{children:"Implement a complete occupancy grid mapping system:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class OccupancyGridMapper:\n    def __init__(self, width=10, height=10, resolution=0.1):\n        self.width = width  # meters\n        self.height = height  # meters\n        self.resolution = resolution  # meters per cell\n        self.grid_size = (int(width / resolution), int(height / resolution))\n\n        # Initialize grid with unknown (0.5) occupancy\n        self.occupancy_grid = np.full(self.grid_size, 0.5)\n\n        # Log-odds representation for mathematical convenience\n        self.log_odds = np.zeros(self.grid_size)\n\n        # Robot position in grid coordinates (center of map)\n        self.robot_x = self.grid_size[0] // 2\n        self.robot_y = self.grid_size[1] // 2\n\n        # Sensor parameters\n        self.max_range = 5.0  # meters\n        self.prob_hit = 0.7   # Probability of hit\n        self.prob_miss = 0.4  # Probability of miss\n\n    def world_to_grid(self, x, y):\n        """Convert world coordinates to grid coordinates"""\n        grid_x = int((x + self.width/2) / self.resolution)\n        grid_y = int((y + self.height/2) / self.resolution)\n        return grid_x, grid_y\n\n    def grid_to_world(self, grid_x, grid_y):\n        """Convert grid coordinates to world coordinates"""\n        x = grid_x * self.resolution - self.width/2\n        y = grid_y * self.resolution - self.height/2\n        return x, y\n\n    def ray_trace(self, start_x, start_y, end_x, end_y):\n        """Ray tracing using Bresenham\'s algorithm"""\n        dx = abs(end_x - start_x)\n        dy = abs(end_y - start_y)\n        x_step = 1 if end_x > start_x else -1\n        y_step = 1 if end_y > start_y else -1\n\n        error = dx - dy\n        x, y = start_x, start_y\n\n        points = []\n        while True:\n            points.append((x, y))\n            if x == end_x and y == end_y:\n                break\n\n            error2 = 2 * error\n            if error2 > -dy:\n                error -= dy\n                x += x_step\n            if error2 < dx:\n                error += dx\n                y += y_step\n\n        return points\n\n    def update_with_laser_scan(self, robot_pose, ranges, angles):\n        """Update occupancy grid with laser scan data"""\n        robot_x, robot_y, robot_theta = robot_pose\n\n        # Convert robot pose to grid coordinates\n        robot_grid_x, robot_grid_y = self.world_to_grid(robot_x, robot_y)\n\n        # Update grid for each laser beam\n        for range_reading, angle in zip(ranges, angles):\n            if range_reading < 0.1 or range_reading > self.max_range:\n                continue  # Invalid reading\n\n            # Calculate end point of beam in world coordinates\n            beam_angle = robot_theta + angle\n            end_x = robot_x + range_reading * np.cos(beam_angle)\n            end_y = robot_y + range_reading * np.sin(beam_angle)\n\n            # Convert to grid coordinates\n            end_grid_x, end_grid_y = self.world_to_grid(end_x, end_y)\n\n            # Ensure endpoints are within grid bounds\n            end_grid_x = np.clip(end_grid_x, 0, self.grid_size[0] - 1)\n            end_grid_y = np.clip(end_grid_y, 0, self.grid_size[1] - 1)\n\n            # Ray trace to update free space\n            free_cells = self.ray_trace(robot_grid_x, robot_grid_y,\n                                      end_grid_x, end_grid_y)\n\n            # Update free space (all cells except the last one)\n            for x, y in free_cells[:-1]:\n                if 0 <= x < self.grid_size[0] and 0 <= y < self.grid_size[1]:\n                    # Convert probability to log odds and update\n                    old_log_odds = self.log_odds[x, y]\n                    new_log_odds = old_log_odds + self.log_probability_update(self.prob_miss)\n                    self.log_odds[x, y] = np.clip(new_log_odds, -10, 10)  # Limit range\n\n            # Update the endpoint with obstacle information (if not max range)\n            if range_reading < self.max_range:\n                if 0 <= end_grid_x < self.grid_size[0] and 0 <= end_grid_y < self.grid_size[1]:\n                    old_log_odds = self.log_odds[end_grid_x, end_grid_y]\n                    new_log_odds = old_log_odds + self.log_probability_update(self.prob_hit)\n                    self.log_odds[end_grid_x, end_grid_y] = np.clip(new_log_odds, -10, 10)\n\n        # Convert log odds back to probability\n        self.occupancy_grid = self.log_odds_to_probability(self.log_odds)\n\n    def log_probability_update(self, prob):\n        """Convert probability to log odds for updating"""\n        prob = max(0.01, min(0.99, prob))  # Clamp to avoid log(0)\n        return np.log(prob / (1 - prob))\n\n    def log_odds_to_probability(self, log_odds):\n        """Convert log odds back to probability"""\n        odds = np.exp(log_odds)\n        return odds / (1 + odds)\n\n    def get_occupancy_probability(self, x, y):\n        """Get occupancy probability at world coordinates"""\n        grid_x, grid_y = self.world_to_grid(x, y)\n        if 0 <= grid_x < self.grid_size[0] and 0 <= grid_y < self.grid_size[1]:\n            return self.occupancy_grid[grid_x, grid_y]\n        return 0.5  # Unknown\n\n    def get_map_as_image(self):\n        """Return occupancy grid as an image array"""\n        # Normalize to 0-255 for image representation\n        img = (self.occupancy_grid * 255).astype(np.uint8)\n        return img\n\n    def visualize_map(self):\n        """Visualize the occupancy grid"""\n        plt.figure(figsize=(10, 10))\n        plt.imshow(self.occupancy_grid, cmap=\'gray\', origin=\'lower\',\n                  extent=[-self.width/2, self.width/2, -self.height/2, self.height/2])\n        plt.colorbar(label=\'Occupancy Probability\')\n        plt.title(\'Occupancy Grid Map\')\n        plt.xlabel(\'X (m)\')\n        plt.ylabel(\'Y (m)\')\n        plt.grid(True, alpha=0.3)\n        plt.show()\n\n# Simulate robot movement and mapping\nmapper = OccupancyGridMapper(width=10, height=10, resolution=0.1)\n\n# Create a simple environment with obstacles\ndef simulate_laser_scan(robot_x, robot_y, robot_theta, map_func):\n    """Simulate laser scan in a simple environment"""\n    num_beams = 360\n    angles = np.linspace(0, 2*np.pi, num_beams, endpoint=False)\n    ranges = []\n\n    for angle in angles:\n        world_angle = robot_theta + angle\n        max_range = 5.0  # Max sensor range\n\n        # Ray march to find obstacle\n        step_size = 0.1\n        current_range = 0\n        hit_obstacle = False\n\n        while current_range < max_range:\n            test_x = robot_x + current_range * np.cos(world_angle)\n            test_y = robot_y + current_range * np.sin(world_angle)\n\n            # Check if we hit an obstacle in our simulated environment\n            # Simple environment: walls at boundaries and a few obstacles\n            if (abs(test_x) > 4.5 or abs(test_y) > 4.5 or  # Walls\n                (1 < test_x < 2 and -1 < test_y < 1) or   # Obstacle 1\n                (-2 < test_x < -1 and 1 < test_y < 2) or  # Obstacle 2\n                (0 < test_x < 1 and 2 < test_y < 3)):     # Obstacle 3\n                ranges.append(current_range)\n                hit_obstacle = True\n                break\n\n            current_range += step_size\n\n        if not hit_obstacle:\n            ranges.append(max_range)\n\n    return ranges, angles\n\n# Simulate robot exploring the environment\nrobot_path = []\nfor step in range(50):\n    # Robot moves in a spiral pattern\n    t = step * 0.5\n    robot_x = 2 * np.cos(t * 0.3) * (1 - t * 0.02)\n    robot_y = 2 * np.sin(t * 0.3) * (1 - t * 0.02)\n    robot_theta = t * 0.3  # Robot orientation\n\n    robot_path.append((robot_x, robot_y))\n\n    # Get simulated laser scan\n    ranges, angles = simulate_laser_scan(robot_x, robot_y, robot_theta, None)\n\n    # Update map with laser scan\n    mapper.update_with_laser_scan([robot_x, robot_y, robot_theta], ranges, angles)\n\n    if step % 10 == 0:  # Visualize every 10 steps\n        print(f"Step {step}: Updated map")\n\nprint("Mapping completed!")\nmapper.visualize_map()\n\n# Print statistics\noccupied_cells = np.sum(mapper.occupancy_grid > 0.7)\nfree_cells = np.sum(mapper.occupancy_grid < 0.3)\nunknown_cells = np.sum((mapper.occupancy_grid >= 0.3) & (mapper.occupancy_grid <= 0.7))\n\nprint(f"Occupied cells: {occupied_cells}")\nprint(f"Free cells: {free_cells}")\nprint(f"Unknown cells: {unknown_cells}")\n'})}),"\n",(0,s.jsx)(e.h2,{id:"exercise-4-path-planning-with-a-algorithm",children:"Exercise 4: Path Planning with A* Algorithm"}),"\n",(0,s.jsx)(e.p,{children:"Implement a complete path planning system:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import heapq\n\nclass PathPlanner:\n    def __init__(self, occupancy_grid, resolution=0.1):\n        self.occupancy_grid = occupancy_grid\n        self.resolution = resolution\n        self.grid_height, self.grid_width = occupancy_grid.shape\n\n    def a_star(self, start, goal, occupancy_threshold=0.7):\n        """A* path planning algorithm"""\n        start_x, start_y = start\n        goal_x, goal_y = goal\n\n        # Convert to grid coordinates\n        start_grid = (int(start_x / self.resolution), int(start_y / self.resolution))\n        goal_grid = (int(goal_x / self.resolution), int(goal_y / self.resolution))\n\n        # Check if start and goal are valid\n        if not self.is_valid_cell(start_grid[0], start_grid[1], occupancy_threshold) or \\\n           not self.is_valid_cell(goal_grid[0], goal_grid[1], occupancy_threshold):\n            return None\n\n        # A* algorithm\n        open_set = [(0, start_grid)]\n        came_from = {}\n        g_score = {start_grid: 0}\n        f_score = {start_grid: self.heuristic(start_grid, goal_grid)}\n\n        while open_set:\n            current = heapq.heappop(open_set)[1]\n\n            if current == goal_grid:\n                # Reconstruct path\n                path = []\n                while current in came_from:\n                    path.append(current)\n                    current = came_from[current]\n                path.append(start_grid)\n                path.reverse()\n\n                # Convert back to world coordinates\n                world_path = [(x * self.resolution, y * self.resolution) for x, y in path]\n                return world_path\n\n            for neighbor in self.get_neighbors(current, occupancy_threshold):\n                tentative_g_score = g_score[current] + self.distance(current, neighbor)\n\n                if neighbor not in g_score or tentative_g_score < g_score[neighbor]:\n                    came_from[neighbor] = current\n                    g_score[neighbor] = tentative_g_score\n                    f_score[neighbor] = tentative_g_score + self.heuristic(neighbor, goal_grid)\n                    heapq.heappush(open_set, (f_score[neighbor], neighbor))\n\n        return None  # No path found\n\n    def is_valid_cell(self, x, y, threshold=0.7):\n        """Check if a grid cell is valid (not occupied)"""\n        if 0 <= x < self.grid_width and 0 <= y < self.grid_height:\n            return self.occupancy_grid[y, x] < threshold\n        return False\n\n    def get_neighbors(self, cell, threshold=0.7):\n        """Get valid neighboring cells"""\n        x, y = cell\n        neighbors = []\n        # 8-connected neighborhood\n        for dx, dy in [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]:\n            nx, ny = x + dx, y + dy\n            if self.is_valid_cell(nx, ny, threshold):\n                neighbors.append((nx, ny))\n        return neighbors\n\n    def heuristic(self, a, b):\n        """Heuristic function (Euclidean distance)"""\n        return np.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)\n\n    def distance(self, a, b):\n        """Distance between adjacent cells"""\n        return np.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)\n\n    def visualize_path(self, start, goal, path):\n        """Visualize the planned path"""\n        plt.figure(figsize=(10, 10))\n        plt.imshow(self.occupancy_grid, cmap=\'gray\', origin=\'lower\',\n                  extent=[0, self.grid_width * self.resolution, 0, self.grid_height * self.resolution])\n\n        if path:\n            path_x = [p[0] for p in path]\n            path_y = [p[1] for p in path]\n            plt.plot(path_x, path_y, \'r-\', linewidth=2, label=\'Planned Path\')\n\n        plt.plot(start[0], start[1], \'go\', markersize=10, label=\'Start\')\n        plt.plot(goal[0], goal[1], \'ro\', markersize=10, label=\'Goal\')\n\n        plt.colorbar(label=\'Occupancy Probability\')\n        plt.title(\'A* Path Planning\')\n        plt.xlabel(\'X (m)\')\n        plt.ylabel(\'Y (m)\')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.show()\n\n# Use the mapper from previous exercise\nplanner = PathPlanner(mapper.occupancy_grid, resolution=0.1)\n\n# Plan a path\nstart = (0.0, 0.0)  # Robot start position\ngoal = (3.0, 3.0)   # Goal position\n\npath = planner.a_star(start, goal)\n\nif path:\n    print(f"Path found with {len(path)} waypoints")\n    planner.visualize_path(start, goal, path)\n\n    # Print path details\n    total_distance = 0\n    for i in range(1, len(path)):\n        total_distance += np.sqrt((path[i][0] - path[i-1][0])**2 +\n                                 (path[i][1] - path[i-1][1])**2)\n    print(f"Total path distance: {total_distance:.2f} m")\nelse:\n    print("No path found!")\n\n# Test with different start/goal positions\nstart2 = (-3.0, -3.0)\ngoal2 = (4.0, 4.0)\npath2 = planner.a_star(start2, goal2)\n\nif path2:\n    print(f"Alternative path found with {len(path2)} waypoints")\n    planner.visualize_path(start2, goal2, path2)\nelse:\n    print("No path found for alternative route!")\n'})}),"\n",(0,s.jsx)(e.h2,{id:"exercise-5-sensor-fusion-for-state-estimation",children:"Exercise 5: Sensor Fusion for State Estimation"}),"\n",(0,s.jsx)(e.p,{children:"Implement a complete sensor fusion system:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'class SensorFusionSystem:\n    def __init__(self):\n        # State: [x, y, theta, vx, vy, omega] (position, orientation, velocities)\n        self.state = np.array([0, 0, 0, 0, 0, 0], dtype=float)\n        self.covariance = np.eye(6) * 0.1\n\n        # Process noise\n        self.Q = np.diag([0.1, 0.1, 0.01, 0.5, 0.5, 0.1])\n\n        # Measurement noise\n        self.R_position = np.diag([0.05, 0.05])  # GPS/visual position noise\n        self.R_orientation = np.array([[0.01]])  # IMU orientation noise\n        self.R_velocity = np.diag([0.1, 0.1, 0.05])  # IMU velocity noise\n\n    def predict(self, dt, control_input=None):\n        """Prediction step using motion model"""\n        x, y, theta, vx, vy, omega = self.state\n\n        # Motion model: constant velocity with rotation\n        # For simplicity, assume control input affects acceleration\n        if control_input is not None:\n            ax, ay = control_input\n        else:\n            ax, ay = 0, 0\n\n        # Update state\n        new_state = self.state.copy()\n        new_state[0] += (vx * np.cos(theta) - vy * np.sin(theta)) * dt  # x\n        new_state[1] += (vx * np.sin(theta) + vy * np.cos(theta)) * dt  # y\n        new_state[2] += omega * dt  # theta\n        new_state[3] += ax * dt  # vx\n        new_state[4] += ay * dt  # vy\n        # omega remains the same (constant angular velocity assumption)\n\n        self.state = new_state\n\n        # Jacobian of motion model\n        F = np.eye(6)\n        F[0, 2] = (-vx * np.sin(theta) - vy * np.cos(theta)) * dt  # dx/dtheta\n        F[0, 3] = np.cos(theta) * dt  # dx/dvx\n        F[0, 4] = -np.sin(theta) * dt  # dx/dvy\n        F[1, 2] = (vx * np.cos(theta) - vy * np.sin(theta)) * dt  # dy/dtheta\n        F[1, 3] = np.sin(theta) * dt  # dy/dvx\n        F[1, 4] = np.cos(theta) * dt  # dy/dvy\n        F[3, 5] = 0  # No direct coupling in this simple model\n        F[4, 5] = 0\n\n        # Update covariance\n        self.covariance = F @ self.covariance @ F.T + self.Q\n\n    def update_position(self, measurement):\n        """Update with position measurement (e.g., from visual odometry or GPS)"""\n        # Measurement model: H maps state to measurement\n        H = np.array([\n            [1, 0, 0, 0, 0, 0],  # measure x\n            [0, 1, 0, 0, 0, 0]   # measure y\n        ])\n\n        # Innovation\n        innovation = measurement - H @ self.state\n\n        # Innovation covariance\n        S = H @ self.covariance @ H.T + self.R_position\n\n        # Kalman gain\n        K = self.covariance @ H.T @ np.linalg.inv(S)\n\n        # Update state and covariance\n        self.state = self.state + K @ innovation\n        self.covariance = (np.eye(6) - K @ H) @ self.covariance\n\n    def update_orientation(self, measurement):\n        """Update with orientation measurement (e.g., from IMU)"""\n        # Measurement model for orientation\n        H = np.array([[0, 0, 1, 0, 0, 0]])  # measure theta\n\n        # Innovation (handle angle wrapping)\n        innovation = measurement - self.state[2]\n        # Normalize angle to [-pi, pi]\n        innovation = np.arctan2(np.sin(innovation), np.cos(innovation))\n\n        # Innovation covariance\n        S = H @ self.covariance @ H.T + self.R_orientation\n\n        # Kalman gain\n        K = self.covariance @ H.T @ np.linalg.inv(S)\n\n        # Update state and covariance\n        self.state[2] += K[2, 0] * innovation\n        # Normalize orientation angle\n        self.state[2] = np.arctan2(np.sin(self.state[2]), np.cos(self.state[2]))\n        self.covariance = (np.eye(6) - K @ H) @ self.covariance\n\n    def update_velocity(self, measurement):\n        """Update with velocity measurement (e.g., from wheel encoders or IMU)"""\n        # Measurement model for velocity in robot frame\n        theta = self.state[2]\n        H = np.array([\n            [0, 0, 0, np.cos(theta), -np.sin(theta), 0],  # v_x in world frame\n            [0, 0, 0, np.sin(theta), np.cos(theta), 0],   # v_y in world frame\n            [0, 0, 0, 0, 0, 1]                            # omega (angular velocity)\n        ])\n\n        # Innovation\n        predicted_velocity = H @ self.state\n        innovation = measurement - predicted_velocity\n\n        # Innovation covariance\n        S = H @ self.covariance @ H.T + self.R_velocity\n\n        # Kalman gain\n        K = self.covariance @ H.T @ np.linalg.inv(S)\n\n        # Update state and covariance\n        self.state = self.state + K @ innovation\n        self.covariance = (np.eye(6) - K @ H) @ self.covariance\n\n    def get_state(self):\n        """Get current state estimate"""\n        return self.state.copy()\n\n    def get_covariance(self):\n        """Get current covariance estimate"""\n        return self.covariance.copy()\n\n# Demonstrate sensor fusion\nfusion_system = SensorFusionSystem()\n\n# Simulate robot movement with sensor measurements\ntrue_positions = []\nestimated_positions = []\nmeasurements = []\n\nfor step in range(100):\n    dt = 0.1  # 10Hz\n\n    # True motion (simulated)\n    true_control = [0.1 * np.sin(step * 0.1), 0.05 * np.cos(step * 0.1)]  # Oscillating motion\n\n    # Predict step\n    fusion_system.predict(dt, true_control)\n\n    # Simulate measurements with noise\n    true_pos = fusion_system.state[0:2] + np.random.normal(0, 0.1, 2)  # Position with noise\n    true_orientation = fusion_system.state[2] + np.random.normal(0, 0.01)  # Orientation with noise\n    true_velocity = np.array([1.0, 0.1, 0.05]) + np.random.normal(0, 0.05, 3)  # Velocity with noise\n\n    # Update with different sensor measurements\n    fusion_system.update_position(true_pos)\n    fusion_system.update_orientation(true_orientation)\n    fusion_system.update_velocity(true_velocity)\n\n    # Store for visualization\n    true_positions.append([step * dt * 0.5, step * dt * 0.2])  # Simulated true trajectory\n    estimated_positions.append(fusion_system.get_state()[0:2].copy())\n    measurements.append(true_pos.copy())\n\ntrue_positions = np.array(true_positions)\nestimated_positions = np.array(estimated_positions)\nmeasurements = np.array(measurements)\n\n# Visualize results\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(true_positions[:, 0], true_positions[:, 1], \'g-\', label=\'True Trajectory\', linewidth=2)\nplt.plot(estimated_positions[:, 0], estimated_positions[:, 1], \'b-\', label=\'Estimated Trajectory\', linewidth=2)\nplt.scatter(measurements[:, 0], measurements[:, 1], c=\'r\', s=10, alpha=0.5, label=\'Measurements\')\nplt.title(\'Sensor Fusion: Trajectory Estimation\')\nplt.xlabel(\'X (m)\')\nplt.ylabel(\'Y (m)\')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nposition_error = np.linalg.norm(estimated_positions - true_positions, axis=1)\nplt.plot(position_error, \'r-\', linewidth=2)\nplt.title(\'Position Estimation Error\')\nplt.xlabel(\'Time Step\')\nplt.ylabel(\'Error (m)\')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print final statistics\nfinal_error = np.linalg.norm(estimated_positions[-1] - true_positions[-1])\nprint(f"Final position error: {final_error:.3f} m")\nprint(f"Average position error: {np.mean(position_error):.3f} m")\nprint(f"RMS position error: {np.sqrt(np.mean(position_error**2)):.3f} m")\n'})}),"\n",(0,s.jsx)(e.h2,{id:"exercise-6-complete-environmental-perception-pipeline",children:"Exercise 6: Complete Environmental Perception Pipeline"}),"\n",(0,s.jsx)(e.p,{children:"Let's create a complete environmental perception system that integrates all components:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class CompletePerceptionSystem:\n    def __init__(self):\n        # Initialize all components\n        self.point_cloud_processor = PointCloudPipeline()\n        self.object_recognizer = ObjectRecognitionSystem()\n        self.occupancy_mapper = OccupancyGridMapper(width=20, height=20, resolution=0.1)\n        self.path_planner = PathPlanner(self.occupancy_mapper.occupancy_grid)\n        self.sensor_fusion = SensorFusionSystem()\n\n        # System state\n        self.robot_pose = np.array([0, 0, 0])  # x, y, theta\n        self.detected_objects = []\n        self.map_updated = False\n\n    def process_point_cloud(self, points, colors=None):\n        \"\"\"Process point cloud data\"\"\"\n        self.point_cloud_processor.points = points\n        self.point_cloud_processor.colors = colors\n\n        # Apply processing pipeline\n        self.point_cloud_processor.downsample(voxel_size=0.05)\n        self.point_cloud_processor.remove_outliers()\n\n        # Segment objects\n        objects = self.point_cloud_processor.segment_objects(eps=0.1, min_samples=30)\n        self.detected_objects = objects\n\n        return objects\n\n    def update_map_with_scan(self, robot_pose, ranges, angles):\n        \"\"\"Update occupancy grid with laser scan\"\"\"\n        self.occupancy_mapper.update_with_laser_scan(robot_pose, ranges, angles)\n        self.map_updated = True\n\n        # Update path planner with new map\n        self.path_planner = PathPlanner(self.occupancy_mapper.occupancy_grid,\n                                      resolution=self.occupancy_mapper.resolution)\n\n    def plan_path(self, start, goal):\n        \"\"\"Plan path using current map\"\"\"\n        if not self.map_updated:\n            print(\"Warning: Map not updated, using empty map for planning\")\n\n        path = self.path_planner.a_star(start, goal)\n        return path\n\n    def get_environment_summary(self):\n        \"\"\"Get summary of current environment understanding\"\"\"\n        summary = {\n            'robot_pose': self.robot_pose,\n            'num_detected_objects': len(self.detected_objects),\n            'object_types': [],\n            'map_coverage': np.sum(self.occupancy_mapper.occupancy_grid != 0.5) / self.occupancy_mapper.occupancy_grid.size,\n            'free_space_ratio': np.sum(self.occupancy_mapper.occupancy_grid < 0.3) / self.occupancy_mapper.occupancy_grid.size\n        }\n\n        for obj in self.detected_objects:\n            # Estimate object type based on dimensions\n            dims = obj['dimensions']\n            max_dim = np.max(dims)\n            min_dim = np.min(dims)\n\n            if max_dim / min_dim > 5:  # Very elongated\n                obj_type = 'rod'\n            elif 2 < max_dim / min_dim < 5:  # Moderately elongated\n                obj_type = 'cylinder'\n            else:  # More cube-like\n                obj_type = 'box'\n\n            summary['object_types'].append(obj_type)\n\n        return summary\n\n    def visualize_environment(self):\n        \"\"\"Visualize the complete environment understanding\"\"\"\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n        # Plot 1: Occupancy grid\n        im1 = axes[0, 0].imshow(self.occupancy_mapper.occupancy_grid, cmap='gray', origin='lower')\n        axes[0, 0].set_title('Occupancy Grid Map')\n        axes[0, 0].set_xlabel('X (m)')\n        axes[0, 0].set_ylabel('Y (m)')\n        plt.colorbar(im1, ax=axes[0, 0])\n\n        # Plot 2: Detected objects\n        axes[0, 1].imshow(self.occupancy_mapper.occupancy_grid, cmap='gray', origin='lower',\n                         extent=[-self.occupancy_mapper.width/2, self.occupancy_mapper.width/2,\n                                -self.occupancy_mapper.height/2, self.occupancy_mapper.height/2])\n\n        for obj in self.detected_objects:\n            axes[0, 1].scatter(obj['center'][0], obj['center'][1], c='red', s=100, marker='s', label='Objects' if obj==self.detected_objects[0] else \"\")\n            # Draw bounding box\n            bbox_min, bbox_max = obj['bbox_min'], obj['bbox_max']\n            rect = plt.Rectangle((bbox_min[0], bbox_min[1]),\n                               bbox_max[0]-bbox_min[0], bbox_max[1]-bbox_min[1],\n                               linewidth=1, edgecolor='red', facecolor='none')\n            axes[0, 1].add_patch(rect)\n\n        axes[0, 1].set_title('Detected Objects')\n        axes[0, 1].set_xlabel('X (m)')\n        axes[0, 1].set_ylabel('Y (m)')\n        axes[0, 1].legend()\n\n        # Plot 3: Robot trajectory (if available)\n        # For this example, we'll simulate a trajectory\n        t = np.linspace(0, 10, 100)\n        robot_traj_x = 2 * np.sin(0.3 * t)\n        robot_traj_y = 2 * np.cos(0.3 * t)\n        axes[1, 0].plot(robot_traj_x, robot_traj_y, 'b-', linewidth=2, label='Robot Trajectory')\n        axes[1, 0].scatter([robot_traj_x[0]], [robot_traj_y[0]], c='green', s=100, label='Start', zorder=5)\n        axes[1, 0].scatter([robot_traj_x[-1]], [robot_traj_y[-1]], c='red', s=100, label='End', zorder=5)\n        axes[1, 0].set_title('Robot Trajectory')\n        axes[1, 0].set_xlabel('X (m)')\n        axes[1, 0].set_ylabel('Y (m)')\n        axes[1, 0].legend()\n        axes[1, 0].grid(True, alpha=0.3)\n\n        # Plot 4: Environment summary\n        summary = self.get_environment_summary()\n        axes[1, 1].axis('off')  # Turn off axis for text display\n\n        summary_text = f\"\"\"Environment Summary:\n\nRobot Position: ({summary['robot_pose'][0]:.2f}, {summary['robot_pose'][1]:.2f})\nNumber of Objects: {summary['num_detected_objects']}\nObject Types: {', '.join(summary['object_types'][:5])}{'...' if len(summary['object_types']) > 5 else ''}\nMap Coverage: {summary['map_coverage']:.1%}\nFree Space: {summary['free_space_ratio']:.1%}\n\nProcessing Status:\n- Point Cloud Processing: Complete\n- Object Detection: {len(self.detected_objects)} objects\n- Map Update: {'Complete' if self.map_updated else 'Pending'}\n- Path Planning: Ready\"\"\"\n\n        axes[1, 1].text(0.1, 0.5, summary_text, fontsize=12, verticalalignment='center',\n                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n\n        plt.tight_layout()\n        plt.show()\n\n# Demonstrate the complete system\nperception_system = CompletePerceptionSystem()\n\n# Generate sample point cloud data\npoints, colors = perception_system.point_cloud_processor.generate_sample_point_cloud(2000)\n\n# Process the point cloud\nobjects = perception_system.process_point_cloud(points, colors)\nprint(f\"Detected {len(objects)} objects\")\n\n# Simulate laser scan and update map\nrobot_pose = [0, 0, 0]\nranges = [4.5] * 360  # Simulated ranges (mostly free space)\nangles = np.linspace(0, 2*np.pi, 360, endpoint=False)\nperception_system.update_map_with_scan(robot_pose, ranges, angles)\n\n# Plan a path\npath = perception_system.plan_path((0, 0), (5, 5))\nif path:\n    print(f\"Planned path with {len(path)} waypoints\")\n\n# Get environment summary\nsummary = perception_system.get_environment_summary()\nprint(\"\\nEnvironment Summary:\")\nfor key, value in summary.items():\n    print(f\"{key}: {value}\")\n\n# Visualize the complete system\nperception_system.visualize_environment()\n"})}),"\n",(0,s.jsx)(e.h2,{id:"exercise-7-real-time-perception-simulation",children:"Exercise 7: Real-time Perception Simulation"}),"\n",(0,s.jsx)(e.p,{children:"Create a simulation that demonstrates real-time perception processing:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import time\nfrom collections import deque\n\nclass RealTimePerceptionSimulator:\n    def __init__(self):\n        self.perception_system = CompletePerceptionSystem()\n        self.simulation_time = 0\n        self.time_step = 0.1  # 10 Hz\n        self.robot_trajectory = []\n        self.processing_times = deque(maxlen=50)  # Keep last 50 processing times\n\n    def simulate_robot_movement(self):\n        """Simulate robot moving in environment"""\n        # Robot follows a circular path\n        x = 3 * np.cos(self.simulation_time * 0.2)\n        y = 3 * np.sin(self.simulation_time * 0.2)\n        theta = self.simulation_time * 0.2  # Robot orientation\n        return np.array([x, y, theta])\n\n    def simulate_sensor_data(self, robot_pose):\n        """Simulate sensor data for current robot pose"""\n        # Simulate point cloud around robot\n        num_points = 500\n        points = []\n\n        # Add some objects in the environment\n        for _ in range(num_points):\n            # Sensor range: 5m\n            range_val = np.random.uniform(0.5, 5.0)\n            angle = np.random.uniform(0, 2*np.pi)\n\n            # Convert to robot coordinates\n            local_x = range_val * np.cos(angle)\n            local_y = range_val * np.sin(angle)\n\n            # Transform to world coordinates\n            cos_th = np.cos(robot_pose[2])\n            sin_th = np.sin(robot_pose[2])\n\n            world_x = robot_pose[0] + local_x * cos_th - local_y * sin_th\n            world_y = robot_pose[1] + local_x * sin_th + local_y * cos_th\n            world_z = np.random.uniform(-0.5, 0.5)  # Ground level with variation\n\n            points.append([world_x, world_y, world_z])\n\n        return np.array(points)\n\n    def simulate_laser_scan(self, robot_pose):\n        """Simulate laser scan data"""\n        num_beams = 360\n        ranges = []\n        angles = np.linspace(0, 2*np.pi, num_beams, endpoint=False)\n\n        # Simulate scan with some obstacles\n        for angle in angles:\n            world_angle = robot_pose[2] + angle\n            # Simulate obstacles at specific locations\n            obstacle_distances = [\n                np.linalg.norm([robot_pose[0] + 2 - (robot_pose[0] + 5*np.cos(world_angle)),\n                              robot_pose[1] + 1 - (robot_pose[1] + 5*np.sin(world_angle))]),\n                np.linalg.norm([robot_pose[0] - 1 - (robot_pose[0] + 5*np.cos(world_angle)),\n                              robot_pose[1] + 2 - (robot_pose[1] + 5*np.sin(world_angle))])\n            ]\n\n            min_dist = min(obstacle_distances + [5.0])  # Min of obstacles or max range\n            ranges.append(min(min_dist, 5.0))\n\n        return np.array(ranges), angles\n\n    def run_simulation_step(self):\n        """Run one step of the simulation"""\n        start_time = time.time()\n\n        # Update simulation time\n        self.simulation_time += self.time_step\n\n        # Get robot pose\n        robot_pose = self.simulate_robot_movement()\n        self.robot_trajectory.append(robot_pose.copy())\n\n        # Simulate sensor data\n        point_cloud = self.simulate_sensor_data(robot_pose)\n        laser_ranges, laser_angles = self.simulate_laser_scan(robot_pose)\n\n        # Process perception\n        self.perception_system.process_point_cloud(point_cloud)\n        self.perception_system.update_map_with_scan(robot_pose, laser_ranges, laser_angles)\n\n        # Record processing time\n        processing_time = time.time() - start_time\n        self.processing_times.append(processing_time)\n\n        return processing_time\n\n    def run_simulation(self, duration=10.0):\n        """Run the complete simulation"""\n        print("Starting real-time perception simulation...")\n        print(f"Simulation duration: {duration}s at {1/self.time_step}Hz")\n\n        num_steps = int(duration / self.time_step)\n\n        for step in range(num_steps):\n            processing_time = self.run_simulation_step()\n\n            if step % 20 == 0:  # Print status every 20 steps\n                avg_processing_time = np.mean(self.processing_times) if self.processing_times else 0\n                print(f"Step {step}/{num_steps}, "\n                      f"Processing time: {processing_time*1000:.1f}ms, "\n                      f"Average: {avg_processing_time*1000:.1f}ms, "\n                      f"Objects detected: {len(self.perception_system.detected_objects)}")\n\n        print("Simulation completed!")\n\n        # Print final statistics\n        if self.processing_times:\n            avg_time = np.mean(self.processing_times) * 1000  # Convert to ms\n            max_time = np.max(self.processing_times) * 1000\n            min_time = np.min(self.processing_times) * 1000\n\n            print(f"\\nPerformance Statistics:")\n            print(f"Average processing time: {avg_time:.2f} ms")\n            print(f"Min processing time: {min_time:.2f} ms")\n            print(f"Max processing time: {max_time:.2f} ms")\n            print(f"Required frequency: {1/self.time_step:.1f} Hz")\n            print(f"Required processing time: {self.time_step*1000:.1f} ms")\n            print(f"Real-time capable: {\'Yes\' if avg_time < self.time_step*1000 else \'No\'}")\n\n    def visualize_simulation_results(self):\n        """Visualize simulation results"""\n        if not self.robot_trajectory:\n            print("No trajectory data to visualize")\n            return\n\n        traj = np.array(self.robot_trajectory)\n\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n        # Robot trajectory\n        axes[0, 0].plot(traj[:, 0], traj[:, 1], \'b-\', linewidth=2, label=\'Trajectory\')\n        axes[0, 0].scatter(traj[0, 0], traj[0, 1], c=\'green\', s=100, label=\'Start\', zorder=5)\n        axes[0, 0].scatter(traj[-1, 0], traj[-1, 1], c=\'red\', s=100, label=\'End\', zorder=5)\n        axes[0, 0].set_title(\'Robot Trajectory\')\n        axes[0, 0].set_xlabel(\'X (m)\')\n        axes[0, 0].set_ylabel(\'Y (m)\')\n        axes[0, 0].legend()\n        axes[0, 0].grid(True, alpha=0.3)\n\n        # Processing time over time\n        if self.processing_times:\n            times_ms = [t*1000 for t in list(self.processing_times)[-100:]]  # Last 100 samples\n            time_steps = range(len(times_ms))\n            axes[0, 1].plot(time_steps, times_ms, \'r-\', linewidth=1)\n            axes[0, 1].axhline(y=1000/10, color=\'g\', linestyle=\'--\', label=\'Required (10Hz)\')\n            axes[0, 1].set_title(\'Processing Time\')\n            axes[0, 1].set_xlabel(\'Step\')\n            axes[0, 1].set_ylabel(\'Time (ms)\')\n            axes[0, 1].legend()\n            axes[0, 1].grid(True, alpha=0.3)\n\n        # Number of objects detected over time (simulated)\n        num_objects = [len(self.perception_system.detected_objects) for _ in range(len(traj))]\n        axes[1, 0].plot(range(len(num_objects)), num_objects, \'g-\', linewidth=2)\n        axes[1, 0].set_title(\'Objects Detected Over Time\')\n        axes[1, 0].set_xlabel(\'Step\')\n        axes[1, 0].set_ylabel(\'Number of Objects\')\n        axes[1, 0].grid(True, alpha=0.3)\n\n        # Final environment state\n        final_summary = self.perception_system.get_environment_summary()\n        axes[1, 1].axis(\'off\')\n        summary_text = f"""Final Simulation State:\n\nSimulation Time: {self.simulation_time:.1f}s\nTotal Steps: {len(traj)}\nFinal Robot Pose: ({final_summary[\'robot_pose\'][0]:.2f}, {final_summary[\'robot_pose\'][1]:.2f}, {final_summary[\'robot_pose\'][2]:.2f})\nTotal Objects Detected: {final_summary[\'num_detected_objects\']}\nMap Coverage: {final_summary[\'map_coverage\']:.1%}\nFree Space Ratio: {final_summary[\'free_space_ratio\']:.1%}\n\nPerformance:\nAverage Processing Time: {np.mean(self.processing_times)*1000:.2f}ms\nRequired Processing Time: {self.time_step*1000:.2f}ms\nReal-time Capable: {\'Yes\' if np.mean(self.processing_times) < self.time_step else \'No\'}"""\n\n        axes[1, 1].text(0.1, 0.5, summary_text, fontsize=11, verticalalignment=\'center\',\n                        bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.7))\n\n        plt.tight_layout()\n        plt.show()\n\n# Run the real-time simulation\nsimulator = RealTimePerceptionSimulator()\nsimulator.run_simulation(duration=5.0)  # Run for 5 seconds\nsimulator.visualize_simulation_results()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(e.p,{children:"These practical examples demonstrate the implementation of key environmental perception concepts for humanoid robots:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Point Cloud Processing"}),": Techniques for filtering, segmenting, and analyzing 3D point cloud data"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Object Recognition"}),": Methods for identifying and classifying objects in the environment"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Mapping"}),": Creating occupancy grid maps from sensor data"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Path Planning"}),": Algorithms for finding safe and efficient paths through the environment"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sensor Fusion"}),": Combining data from multiple sensors to create coherent state estimates"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Complete Systems"}),": Integration of multiple perception components into functional systems"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Real-time Processing"}),": Considerations for real-time performance and system integration"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"Students should experiment with these examples, modify parameters, and observe the effects on system behavior. Understanding both the theoretical concepts and their practical implementation is crucial for developing effective environmental perception systems for humanoid robots."})]})}function d(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(c,{...n})}):c(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>r});var s=t(6540);const o={},i=s.createContext(o);function a(n){const e=s.useContext(i);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),s.createElement(i.Provider,{value:e},n.children)}}}]);