"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[318],{6397:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>t,metadata:()=>a,toc:()=>c});var o=i(4848),s=i(8453);const t={},r="Environmental Perception and Mapping",a={id:"module-3-perception-sensing/environmental-perception",title:"Environmental Perception and Mapping",description:"Introduction: Understanding the Surroundings",source:"@site/docs/module-3-perception-sensing/environmental-perception.md",sourceDirName:"module-3-perception-sensing",slug:"/module-3-perception-sensing/environmental-perception",permalink:"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/environmental-perception",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3-perception-sensing/environmental-perception.md",tags:[],version:"current",frontMatter:{},sidebar:"textbookSidebar",previous:{title:"Sensor Fusion and State Estimation",permalink:"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/sensor-fusion"},next:{title:"Practical Examples and Exercises: Environmental Perception Implementation",permalink:"/book-of-physical-ai-and-humainoid-robotics-01/docs/module-3-perception-sensing/practical-examples"}},l={},c=[{value:"Introduction: Understanding the Surroundings",id:"introduction-understanding-the-surroundings",level:2},{value:"The Challenge of Environmental Perception",id:"the-challenge-of-environmental-perception",level:3},{value:"3D Scene Understanding",id:"3d-scene-understanding",level:2},{value:"Point Cloud Processing",id:"point-cloud-processing",level:3},{value:"Surface Reconstruction",id:"surface-reconstruction",level:3},{value:"Object Detection and Recognition in 3D",id:"object-detection-and-recognition-in-3d",level:2},{value:"3D Object Detection",id:"3d-object-detection",level:3},{value:"Feature-Based 3D Recognition",id:"feature-based-3d-recognition",level:3},{value:"Simultaneous Localization and Mapping (SLAM)",id:"simultaneous-localization-and-mapping-slam",level:2},{value:"Occupancy Grid Mapping",id:"occupancy-grid-mapping",level:3},{value:"Graph-Based SLAM",id:"graph-based-slam",level:3},{value:"Navigation and Path Planning",id:"navigation-and-path-planning",level:2},{value:"Path Planning in Dynamic Environments",id:"path-planning-in-dynamic-environments",level:3},{value:"Human-Aware Navigation",id:"human-aware-navigation",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"ROS 2 Environmental Perception Node",id:"ros-2-environmental-perception-node",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h1,{id:"environmental-perception-and-mapping",children:"Environmental Perception and Mapping"}),"\n",(0,o.jsx)(e.h2,{id:"introduction-understanding-the-surroundings",children:"Introduction: Understanding the Surroundings"}),"\n",(0,o.jsx)(e.p,{children:"Environmental perception enables humanoid robots to understand and navigate their surroundings, creating spatial representations that support navigation, manipulation, and safe interaction. This section explores the algorithms and techniques that allow robots to perceive and map their environment, detect obstacles, and understand spatial relationships in 3D space."}),"\n",(0,o.jsx)(e.h3,{id:"the-challenge-of-environmental-perception",children:"The Challenge of Environmental Perception"}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robots face unique challenges in environmental perception:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Dynamic Environments"}),": Humans and objects move continuously"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Multi-Modal Integration"}),": Combining vision, LIDAR, and other sensors"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Real-Time Requirements"}),": Processing must occur at video frame rates"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Uncertainty Management"}),": Dealing with sensor noise and occlusions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Scale Variations"}),": From detailed manipulation to large-scale navigation"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"3d-scene-understanding",children:"3D Scene Understanding"}),"\n",(0,o.jsx)(e.h3,{id:"point-cloud-processing",children:"Point Cloud Processing"}),"\n",(0,o.jsx)(e.p,{children:"Point clouds from RGB-D cameras and LIDAR provide rich 3D spatial information:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import numpy as np\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.linear_model import RANSACRegressor\n\nclass PointCloudProcessor:\n    def __init__(self):\n        self.points = None\n        self.colors = None\n        self.normals = None\n\n    def load_point_cloud(self, points, colors=None):\n        \"\"\"Load point cloud data\"\"\"\n        self.points = np.array(points)\n        self.colors = np.array(colors) if colors is not None else None\n\n    def estimate_normals(self, k=10):\n        \"\"\"Estimate surface normals for each point\"\"\"\n        if self.points is None:\n            return None\n\n        normals = []\n        for i, point in enumerate(self.points):\n            # Find k nearest neighbors\n            distances = np.linalg.norm(self.points - point, axis=1)\n            nearest_indices = np.argpartition(distances, k+1)[:k+1]\n            nearest_points = self.points[nearest_indices[1:]]  # Exclude the point itself\n\n            # Calculate covariance matrix\n            cov_matrix = np.cov(nearest_points.T)\n\n            # Get eigenvectors\n            eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n\n            # Normal is the eigenvector corresponding to smallest eigenvalue\n            normal = eigenvectors[:, 0]\n            normals.append(normal)\n\n        self.normals = np.array(normals)\n        return self.normals\n\n    def remove_outliers(self, method='statistical', k=20, std_ratio=2.0):\n        \"\"\"Remove outlier points using statistical or radius-based method\"\"\"\n        if self.points is None:\n            return None\n\n        if method == 'statistical':\n            # Statistical outlier removal\n            distances = []\n            for i, point in enumerate(self.points):\n                neighbor_distances = np.linalg.norm(self.points - point, axis=1)\n                k_nearest = np.partition(neighbor_distances, k+1)[1:k+1]\n                avg_distance = np.mean(k_nearest)\n                distances.append(avg_distance)\n\n            distances = np.array(distances)\n            mean_dist = np.mean(distances)\n            std_dist = np.std(distances)\n\n            # Keep points with distance within std_ratio * std_dev\n            valid_indices = distances < (mean_dist + std_ratio * std_dist)\n\n        elif method == 'radius':\n            # Radius-based outlier removal\n            valid_indices = np.ones(len(self.points), dtype=bool)\n            for i, point in enumerate(self.points):\n                neighbor_count = np.sum(np.linalg.norm(self.points - point, axis=1) < 0.1)  # 10cm radius\n                if neighbor_count < 5:  # Require at least 5 neighbors\n                    valid_indices[i] = False\n\n        # Filter point cloud\n        self.points = self.points[valid_indices]\n        if self.colors is not None:\n            self.colors = self.colors[valid_indices]\n        if self.normals is not None:\n            self.normals = self.normals[valid_indices]\n\n        return self.points, self.colors\n\n    def segment_planes(self, distance_threshold=0.01, min_points=100):\n        \"\"\"Segment planar surfaces using RANSAC\"\"\"\n        if self.points is None:\n            return []\n\n        remaining_points = self.points.copy()\n        planes = []\n\n        while len(remaining_points) > min_points:\n            # Apply RANSAC to find best plane\n            best_model = None\n            best_inliers = []\n            best_score = 0\n\n            for _ in range(100):  # RANSAC iterations\n                # Randomly sample 3 points\n                sample_indices = np.random.choice(len(remaining_points), 3, replace=False)\n                sample_points = remaining_points[sample_indices]\n\n                # Check for degenerate case (collinear points)\n                v1 = sample_points[1] - sample_points[0]\n                v2 = sample_points[2] - sample_points[0]\n                normal = np.cross(v1, v2)\n                if np.linalg.norm(normal) < 1e-6:  # Points are collinear\n                    continue\n\n                # Create plane equation: ax + by + cz + d = 0\n                normal = normal / np.linalg.norm(normal)\n                d = -np.dot(normal, sample_points[0])\n\n                # Find inliers\n                distances = np.abs(np.dot(remaining_points, normal) + d)\n                inliers = remaining_points[distances < distance_threshold]\n\n                if len(inliers) > len(best_inliers):\n                    best_inliers = inliers\n                    best_model = (normal, d)\n\n            if len(best_inliers) >= min_points and best_model is not None:\n                planes.append({\n                    'model': best_model,\n                    'inliers': best_inliers,\n                    'center': np.mean(best_inliers, axis=0),\n                    'normal': best_model[0],\n                    'd': best_model[1]\n                })\n\n                # Remove inliers from remaining points\n                distances = np.abs(np.dot(remaining_points, best_model[0]) + best_model[1])\n                remaining_points = remaining_points[distances >= distance_threshold]\n            else:\n                break  # No more significant planes found\n\n        return planes\n\n    def segment_objects(self, eps=0.05, min_samples=50):\n        \"\"\"Segment objects using DBSCAN clustering\"\"\"\n        if self.points is None:\n            return []\n\n        # Apply DBSCAN clustering\n        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(self.points)\n        labels = clustering.labels_\n\n        objects = []\n        for label in set(labels):\n            if label == -1:  # Noise points\n                continue\n\n            # Get points belonging to this cluster\n            object_points = self.points[labels == label]\n            object_colors = self.colors[labels == label] if self.colors is not None else None\n\n            # Calculate object properties\n            center = np.mean(object_points, axis=0)\n            bbox_min = np.min(object_points, axis=0)\n            bbox_max = np.max(object_points, axis=0)\n            dimensions = bbox_max - bbox_min\n\n            objects.append({\n                'points': object_points,\n                'colors': object_colors,\n                'center': center,\n                'bbox_min': bbox_min,\n                'bbox_max': bbox_max,\n                'dimensions': dimensions,\n                'label': label\n            })\n\n        return objects\n\n    def downsample(self, voxel_size=0.01):\n        \"\"\"Downsample point cloud using voxel grid filter\"\"\"\n        if self.points is None:\n            return None\n\n        # Create voxel grid\n        min_bound = np.min(self.points, axis=0)\n        max_bound = np.max(self.points, axis=0)\n        dims = np.ceil((max_bound - min_bound) / voxel_size).astype(int)\n\n        # Create voxel hash map\n        voxel_map = {}\n        for i, point in enumerate(self.points):\n            voxel_idx = ((point - min_bound) / voxel_size).astype(int)\n            voxel_key = tuple(voxel_idx)\n\n            if voxel_key not in voxel_map:\n                voxel_map[voxel_key] = {'points': [], 'colors': [] if self.colors is not None else None}\n\n            voxel_map[voxel_key]['points'].append(point)\n            if self.colors is not None:\n                voxel_map[voxel_key]['colors'].append(self.colors[i])\n\n        # Take centroid of each voxel\n        downsampled_points = []\n        downsampled_colors = []\n\n        for voxel_data in voxel_map.values():\n            centroid = np.mean(voxel_data['points'], axis=0)\n            downsampled_points.append(centroid)\n\n            if self.colors is not None:\n                color_centroid = np.mean(voxel_data['colors'], axis=0)\n                downsampled_colors.append(color_centroid)\n\n        self.points = np.array(downsampled_points)\n        if self.colors is not None:\n            self.colors = np.array(downsampled_colors)\n\n        return self.points, self.colors\n"})}),"\n",(0,o.jsx)(e.h3,{id:"surface-reconstruction",children:"Surface Reconstruction"}),"\n",(0,o.jsx)(e.p,{children:"Creating continuous surfaces from discrete point cloud data:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'from scipy.spatial import Delaunay\nfrom scipy.interpolate import griddata\n\nclass SurfaceReconstruction:\n    def __init__(self):\n        self.triangles = None\n        self.grid_resolution = 0.01\n\n    def delaunay_triangulation(self, points_2d):\n        """Create Delaunay triangulation from 2D points"""\n        tri = Delaunay(points_2d)\n        return tri.simplices\n\n    def poisson_surface_reconstruction(self, points, normals):\n        """Simplified Poisson surface reconstruction"""\n        # This is a very simplified version\n        # In practice, you\'d use more sophisticated algorithms like marching cubes\n\n        if len(points) < 3:\n            return None\n\n        # Create a signed distance field (simplified approach)\n        # Find bounding box\n        min_point = np.min(points, axis=0)\n        max_point = np.max(points, axis=0)\n\n        # Create a 3D grid\n        grid_size = (int((max_point[0] - min_point[0]) / self.grid_resolution),\n                    int((max_point[1] - min_point[1]) / self.grid_resolution),\n                    int((max_point[2] - min_point[2]) / self.grid_resolution))\n\n        # For each grid point, calculate distance to nearest surface\n        # This is computationally expensive and simplified here\n        pass\n\n    def extract_mesh(self, points, method=\'alpha_shape\'):\n        """Extract mesh from point cloud using various methods"""\n        if method == \'convex_hull\':\n            from scipy.spatial import ConvexHull\n            hull = ConvexHull(points)\n            return hull.simplices, hull.points[hull.vertices]\n\n        elif method == \'alpha_shape\':\n            # Alpha shape is a generalization of convex hull\n            # It can handle concave shapes\n            return self.alpha_shape(points, alpha=0.02)\n\n        elif method == \'ball_pivoting\':\n            # Ball pivoting algorithm (simplified)\n            return self.ball_pivoting(points, radius=0.02)\n\n    def alpha_shape(self, points, alpha):\n        """Compute alpha shape of point cloud"""\n        # This would typically use specialized libraries like CGAL\n        # For this example, we\'ll use a simplified approach\n        # Create Delaunay triangulation\n        tri = Delaunay(points)\n        simplices = tri.simplices\n\n        # Filter simplices based on circumsphere radius\n        filtered_simplices = []\n        for simplex in simplices:\n            tetra = points[simplex]\n            # Calculate circumsphere radius\n            # For a tetrahedron with vertices a, b, c, d\n            # The circumsphere radius calculation is complex\n            # We\'ll use a simplified distance check\n            centroid = np.mean(tetra, axis=0)\n            max_dist = np.max(np.linalg.norm(tetra - centroid, axis=1))\n            if max_dist < alpha:\n                filtered_simplices.append(simplex)\n\n        return np.array(filtered_simplices) if filtered_simplices else None, points\n\n    def ball_pivoting(self, points, radius):\n        """Ball pivoting algorithm (simplified)"""\n        # Ball pivoting is complex to implement correctly\n        # This is a very simplified version\n        # The real algorithm would involve more sophisticated geometry\n        pass\n'})}),"\n",(0,o.jsx)(e.h2,{id:"object-detection-and-recognition-in-3d",children:"Object Detection and Recognition in 3D"}),"\n",(0,o.jsx)(e.h3,{id:"3d-object-detection",children:"3D Object Detection"}),"\n",(0,o.jsx)(e.p,{children:"Detecting objects using 3D information:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class Object3DDetector:\n    def __init__(self):\n        self.voxel_size = 0.05  # 5cm voxels\n        self.min_object_size = 0.1  # 10cm minimum\n        self.max_object_size = 2.0  # 2m maximum\n\n    def detect_objects_voxel_grid(self, points):\n        \"\"\"Detect objects using voxel grid analysis\"\"\"\n        # Create voxel grid\n        min_bound = np.min(points, axis=0)\n        max_bound = np.max(points, axis=0)\n        dims = np.ceil((max_bound - min_bound) / self.voxel_size).astype(int)\n\n        # Create occupancy grid\n        grid_shape = (dims[0], dims[1], dims[2])\n        occupancy_grid = np.zeros(grid_shape, dtype=bool)\n\n        # Fill occupancy grid\n        for point in points:\n            voxel_idx = ((point - min_bound) / self.voxel_size).astype(int)\n            voxel_idx = np.clip(voxel_idx, [0, 0, 0], np.array(grid_shape) - 1)\n            occupancy_grid[tuple(voxel_idx)] = True\n\n        # Find connected components in 3D\n        from scipy.ndimage import label\n        structure = np.ones((3, 3, 3))  # 6-connectivity\n        labeled_grid, num_objects = label(occupancy_grid, structure=structure)\n\n        # Extract object properties\n        objects = []\n        for i in range(1, num_objects + 1):\n            # Get indices of this object\n            object_indices = np.where(labeled_grid == i)\n            object_points = points[\n                (labeled_grid == i)\n            ]\n\n            # Convert voxel indices back to world coordinates\n            world_points = []\n            for idx in zip(*object_indices):\n                world_point = min_bound + np.array(idx) * self.voxel_size\n                world_points.append(world_point)\n\n            world_points = np.array(world_points)\n\n            if len(world_points) > 10:  # Minimum number of points\n                center = np.mean(world_points, axis=0)\n                bbox_min = np.min(world_points, axis=0)\n                bbox_max = np.max(world_points, axis=0)\n                dimensions = bbox_max - bbox_min\n\n                if (self.min_object_size < dimensions[0] < self.max_object_size and\n                    self.min_object_size < dimensions[1] < self.max_object_size and\n                    self.min_object_size < dimensions[2] < self.max_object_size):\n\n                    objects.append({\n                        'points': world_points,\n                        'center': center,\n                        'bbox': (bbox_min, bbox_max),\n                        'dimensions': dimensions,\n                        'num_points': len(world_points)\n                    })\n\n        return objects\n\n    def estimate_object_pose(self, object_points):\n        \"\"\"Estimate object pose (position and orientation)\"\"\"\n        if len(object_points) < 4:\n            return None\n\n        # Principal Component Analysis for orientation\n        centered_points = object_points - np.mean(object_points, axis=0)\n        cov_matrix = np.cov(centered_points.T)\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n\n        # Sort eigenvectors by eigenvalues (descending)\n        sort_idx = np.argsort(eigenvalues)[::-1]\n        eigenvectors = eigenvectors[:, sort_idx]\n\n        # The eigenvectors form the rotation matrix\n        # (They might need to be adjusted for proper orientation)\n        if np.linalg.det(eigenvectors) < 0:\n            eigenvectors[:, -1] = -eigenvectors[:, -1]  # Ensure right-handed coordinate system\n\n        # Return pose: [x, y, z, qw, qx, qy, qz]\n        position = np.mean(object_points, axis=0)\n        rotation_matrix = eigenvectors\n        quaternion = self.rotation_matrix_to_quaternion(rotation_matrix)\n\n        return np.concatenate([position, quaternion])\n\n    def rotation_matrix_to_quaternion(self, R):\n        \"\"\"Convert rotation matrix to quaternion\"\"\"\n        trace = np.trace(R)\n        if trace > 0:\n            s = np.sqrt(trace + 1.0) * 2  # s = 4 * qw\n            qw = 0.25 * s\n            qx = (R[2, 1] - R[1, 2]) / s\n            qy = (R[0, 2] - R[2, 0]) / s\n            qz = (R[1, 0] - R[0, 1]) / s\n        else:\n            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\n                s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\n                qw = (R[2, 1] - R[1, 2]) / s\n                qx = 0.25 * s\n                qy = (R[0, 1] + R[1, 0]) / s\n                qz = (R[0, 2] + R[2, 0]) / s\n            elif R[1, 1] > R[2, 2]:\n                s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\n                qw = (R[0, 2] - R[2, 0]) / s\n                qx = (R[0, 1] + R[1, 0]) / s\n                qy = 0.25 * s\n                qz = (R[1, 2] + R[2, 1]) / s\n            else:\n                s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\n                qw = (R[1, 0] - R[0, 1]) / s\n                qx = (R[0, 2] + R[2, 0]) / s\n                qy = (R[1, 2] + R[2, 1]) / s\n                qz = 0.25 * s\n\n        return np.array([qw, qx, qy, qz])\n\n    def classify_object_shape(self, dimensions):\n        \"\"\"Classify object shape based on dimensions\"\"\"\n        # Sort dimensions\n        sorted_dims = np.sort(dimensions)\n\n        if sorted_dims[2] / sorted_dims[0] > 10:  # Very elongated\n            return 'rod', 0.8\n        elif sorted_dims[2] / sorted_dims[1] < 1.5 and sorted_dims[1] / sorted_dims[0] < 1.5:\n            # All dimensions similar\n            return 'cube', 0.9\n        elif sorted_dims[2] / sorted_dims[1] > 3 and sorted_dims[1] / sorted_dims[0] < 3:\n            # Flat object\n            return 'disc', 0.8\n        else:\n            return 'irregular', 0.6\n\n    def track_objects(self, current_objects, previous_objects, max_distance=0.1):\n        \"\"\"Simple object tracking by nearest neighbor matching\"\"\"\n        if not previous_objects:\n            # Assign new IDs to current objects\n            for i, obj in enumerate(current_objects):\n                obj['id'] = i\n            return current_objects\n\n        tracked_objects = []\n        used_previous = set()\n\n        for curr_obj in current_objects:\n            min_distance = float('inf')\n            best_match = None\n\n            for j, prev_obj in enumerate(previous_objects):\n                if j in used_previous:\n                    continue\n\n                dist = np.linalg.norm(curr_obj['center'] - prev_obj['center'])\n                if dist < min_distance and dist < max_distance:\n                    min_distance = dist\n                    best_match = j\n\n            if best_match is not None:\n                # Update tracked object\n                tracked_obj = current_objects[current_objects.index(curr_obj)]\n                tracked_obj['id'] = previous_objects[best_match]['id']\n                tracked_obj['velocity'] = (tracked_obj['center'] -\n                                         previous_objects[best_match]['center']) / 0.1  # dt = 0.1s\n                used_previous.add(best_match)\n            else:\n                # New object\n                tracked_obj = curr_obj\n                tracked_obj['id'] = len(previous_objects) + len(tracked_objects)\n\n            tracked_objects.append(tracked_obj)\n\n        return tracked_objects\n"})}),"\n",(0,o.jsx)(e.h3,{id:"feature-based-3d-recognition",children:"Feature-Based 3D Recognition"}),"\n",(0,o.jsx)(e.p,{children:"Recognizing objects using geometric features:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class FeatureBased3DRecognizer:\n    def __init__(self):\n        self.reference_features = {}\n        self.feature_extractor = PointCloudProcessor()\n\n    def extract_geometric_features(self, points):\n        """Extract geometric features for object recognition"""\n        features = {}\n\n        # Statistical features\n        centroid = np.mean(points, axis=0)\n        centered_points = points - centroid\n\n        # Covariance-based features\n        cov_matrix = np.cov(centered_points.T)\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n        eigenvalues = np.sort(eigenvalues)[::-1]  # Sort in descending order\n\n        # Shape descriptors\n        features[\'linearity\'] = (eigenvalues[0] - eigenvalues[1]) / eigenvalues[0]\n        features[\'planarity\'] = (eigenvalues[1] - eigenvalues[2]) / eigenvalues[0]\n        features[\'scattering\'] = eigenvalues[2] / eigenvalues[0]\n        features[\'omnivariance\'] = np.cbrt(eigenvalues[0] * eigenvalues[1] * eigenvalues[2])\n        features[\'anisotropy\'] = (eigenvalues[0] - eigenvalues[2]) / eigenvalues[0]\n        features[\'eigenentropy\'] = -np.sum(eigenvalues / np.sum(eigenvalues) *\n                                          np.log(eigenvalues / np.sum(eigenvalues) + 1e-6))\n\n        # Size features\n        bbox_min = np.min(points, axis=0)\n        bbox_max = np.max(points, axis=0)\n        dimensions = bbox_max - bbox_min\n        features[\'volume\'] = np.prod(dimensions)\n        features[\'surface_area\'] = 2 * (dimensions[0]*dimensions[1] +\n                                       dimensions[1]*dimensions[2] +\n                                       dimensions[0]*dimensions[2])\n        features[\'dimensions\'] = dimensions\n\n        # Density features\n        convex_hull_volume = self.estimate_convex_hull_volume(points)\n        features[\'compactness\'] = features[\'volume\'] / (convex_hull_volume + 1e-6)\n\n        return features\n\n    def estimate_convex_hull_volume(self, points):\n        """Estimate volume using convex hull (simplified)"""\n        try:\n            from scipy.spatial import ConvexHull\n            hull = ConvexHull(points)\n            return hull.volume\n        except:\n            # Fallback: use bounding box volume\n            bbox_min = np.min(points, axis=0)\n            bbox_max = np.max(points, axis=0)\n            return np.prod(bbox_max - bbox_min)\n\n    def register_object(self, object_name, point_cloud):\n        """Register a known object with its features"""\n        features = self.extract_geometric_features(point_cloud)\n        self.reference_features[object_name] = features\n\n    def recognize_object(self, point_cloud, threshold=0.3):\n        """Recognize object by comparing features with registered objects"""\n        if not self.reference_features:\n            return None, 0.0\n\n        query_features = self.extract_geometric_features(point_cloud)\n\n        best_match = None\n        best_score = 0.0\n\n        for obj_name, ref_features in self.reference_features.items():\n            score = self.compare_features(query_features, ref_features)\n            if score > best_score:\n                best_score = score\n                best_match = obj_name\n\n        if best_score > threshold:\n            return best_match, best_score\n        else:\n            return None, best_score\n\n    def compare_features(self, features1, features2):\n        """Compare two feature sets"""\n        scores = []\n        weights = {\n            \'linearity\': 0.15,\n            \'planarity\': 0.15,\n            \'scattering\': 0.15,\n            \'volume\': 0.1,\n            \'dimensions\': 0.2,\n            \'compactness\': 0.15,\n            \'anisotropy\': 0.1\n        }\n\n        for feature_name, weight in weights.items():\n            if feature_name in features1 and feature_name in features2:\n                val1 = features1[feature_name]\n                val2 = features2[feature_name]\n\n                if isinstance(val1, (list, np.ndarray)):\n                    # Handle array features like dimensions\n                    diff = np.mean(np.abs(np.array(val1) - np.array(val2)))\n                    max_val = max(np.max(np.abs(val1)), np.max(np.abs(val2)), 1e-6)\n                    similarity = max(0, 1 - diff / max_val)\n                else:\n                    # Handle scalar features\n                    diff = abs(val1 - val2)\n                    max_val = max(abs(val1), abs(val2), 1e-6)\n                    similarity = max(0, 1 - diff / max_val)\n\n                scores.append(similarity * weight)\n\n        return sum(scores) if scores else 0.0\n\n    def extract_signature(self, points):\n        """Extract a signature that\'s invariant to transformations"""\n        # Create a shape signature that\'s invariant to rotation and translation\n        # Using a simplified approach with radial distribution\n\n        centroid = np.mean(points, axis=0)\n        distances = np.linalg.norm(points - centroid, axis=1)\n\n        # Create histogram of distances\n        hist, _ = np.histogram(distances, bins=20, density=True)\n        return hist\n\n    def match_signatures(self, sig1, sig2):\n        """Match two shape signatures"""\n        # Use histogram intersection\n        intersection = np.sum(np.minimum(sig1, sig2))\n        return intersection\n'})}),"\n",(0,o.jsx)(e.h2,{id:"simultaneous-localization-and-mapping-slam",children:"Simultaneous Localization and Mapping (SLAM)"}),"\n",(0,o.jsx)(e.h3,{id:"occupancy-grid-mapping",children:"Occupancy Grid Mapping"}),"\n",(0,o.jsx)(e.p,{children:"Creating 2D maps from sensor data:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class OccupancyGridMapper:\n    def __init__(self, width=20, height=20, resolution=0.1):\n        self.width = width  # meters\n        self.height = height  # meters\n        self.resolution = resolution  # meters per cell\n        self.grid_size = (int(width / resolution), int(height / resolution))\n\n        # Initialize grid with unknown (0.5) occupancy\n        self.occupancy_grid = np.full(self.grid_size, 0.5)\n\n        # Log-odds representation for mathematical convenience\n        self.log_odds = np.zeros(self.grid_size)\n\n        # Robot position in grid coordinates\n        self.robot_x = self.grid_size[0] // 2\n        self.robot_y = self.grid_size[1] // 2\n\n        # Sensor parameters\n        self.max_range = 5.0  # meters\n        self.angle_resolution = 0.01745  # radians (1 degree)\n\n    def world_to_grid(self, x, y):\n        """Convert world coordinates to grid coordinates"""\n        grid_x = int((x + self.width/2) / self.resolution)\n        grid_y = int((y + self.height/2) / self.resolution)\n        return grid_x, grid_y\n\n    def grid_to_world(self, grid_x, grid_y):\n        """Convert grid coordinates to world coordinates"""\n        x = grid_x * self.resolution - self.width/2\n        y = grid_y * self.resolution - self.height/2\n        return x, y\n\n    def ray_trace(self, start_x, start_y, end_x, end_y):\n        """Ray tracing to update free space along a beam"""\n        # Bresenham\'s algorithm for line drawing\n        dx = abs(end_x - start_x)\n        dy = abs(end_y - start_y)\n        x_step = 1 if end_x > start_x else -1\n        y_step = 1 if end_y > start_y else -1\n\n        error = dx - dy\n        x, y = start_x, start_y\n\n        points = []\n        while True:\n            points.append((x, y))\n            if x == end_x and y == end_y:\n                break\n\n            error2 = 2 * error\n            if error2 > -dy:\n                error -= dy\n                x += x_step\n            if error2 < dx:\n                error += dx\n                y += y_step\n\n        return points\n\n    def update_with_laser_scan(self, robot_pose, ranges, angles):\n        """Update occupancy grid with laser scan data"""\n        robot_x, robot_y, robot_theta = robot_pose\n\n        # Convert robot pose to grid coordinates\n        robot_grid_x, robot_grid_y = self.world_to_grid(robot_x, robot_y)\n\n        # Update grid for each laser beam\n        for i, (range_reading, angle) in enumerate(zip(ranges, angles)):\n            if range_reading < 0.1 or range_reading > self.max_range:\n                continue  # Invalid reading\n\n            # Calculate end point of beam in world coordinates\n            beam_angle = robot_theta + angle\n            end_x = robot_x + range_reading * np.cos(beam_angle)\n            end_y = robot_y + range_reading * np.sin(beam_angle)\n\n            # Convert to grid coordinates\n            end_grid_x, end_grid_y = self.world_to_grid(end_x, end_y)\n\n            # Ray trace to update free space\n            free_cells = self.ray_trace(robot_grid_x, robot_grid_y,\n                                      end_grid_x, end_grid_y)\n\n            for x, y in free_cells[:-1]:  # Don\'t update the obstacle cell yet\n                if 0 <= x < self.grid_size[0] and 0 <= y < self.grid_size[1]:\n                    # Update with free space (decrease occupancy)\n                    self.log_odds[x, y] += self.log_probability_update(-0.1)  # Free space\n\n            # Update the endpoint with obstacle information\n            if 0 <= end_grid_x < self.grid_size[0] and 0 <= end_grid_y < self.grid_size[1]:\n                self.log_odds[end_grid_x, end_grid_y] += self.log_probability_update(0.8)  # Obstacle\n\n        # Convert log odds back to probability\n        self.occupancy_grid = self.log_odds_to_probability(self.log_odds)\n\n    def log_probability_update(self, prob):\n        """Convert probability to log odds for updating"""\n        prob = max(0.01, min(0.99, prob))  # Clamp to avoid log(0)\n        return np.log(prob / (1 - prob))\n\n    def log_odds_to_probability(self, log_odds):\n        """Convert log odds back to probability"""\n        odds = np.exp(log_odds)\n        return odds / (1 + odds)\n\n    def get_occupancy_probability(self, x, y):\n        """Get occupancy probability at world coordinates"""\n        grid_x, grid_y = self.world_to_grid(x, y)\n        if 0 <= grid_x < self.grid_size[0] and 0 <= grid_y < self.grid_size[1]:\n            return self.occupancy_grid[grid_x, grid_y]\n        return 0.5  # Unknown\n\n    def find_free_space(self, robot_x, robot_y, radius=1.0):\n        """Find nearby free space"""\n        grid_x, grid_y = self.world_to_grid(robot_x, robot_y)\n        radius_cells = int(radius / self.resolution)\n\n        # Search in a square around the robot\n        for dx in range(-radius_cells, radius_cells + 1):\n            for dy in range(-radius_cells, radius_cells + 1):\n                x = grid_x + dx\n                y = grid_y + dy\n\n                if (0 <= x < self.grid_size[0] and\n                    0 <= y < self.grid_size[1] and\n                    self.occupancy_grid[x, y] < 0.3):  # Free space threshold\n                    return self.grid_to_world(x, y)\n\n        return robot_x, robot_y  # Return current position if no free space found\n\n    def get_pathable_area(self, threshold=0.7):\n        """Get coordinates of potentially pathable areas"""\n        free_cells = np.where(self.occupancy_grid < threshold)\n        free_coords = []\n\n        for x, y in zip(free_cells[0], free_cells[1]):\n            world_x, world_y = self.grid_to_world(x, y)\n            free_coords.append((world_x, world_y))\n\n        return free_coords\n\n    def get_map_as_image(self):\n        """Return occupancy grid as an image array"""\n        # Normalize to 0-255 for image representation\n        img = (self.occupancy_grid * 255).astype(np.uint8)\n        return img\n'})}),"\n",(0,o.jsx)(e.h3,{id:"graph-based-slam",children:"Graph-Based SLAM"}),"\n",(0,o.jsx)(e.p,{children:"Advanced SLAM using pose graphs:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom scipy.spatial.transform import Rotation as R\nfrom scipy.optimize import minimize\n\nclass PoseGraphSLAM:\n    def __init__(self):\n        self.poses = []  # List of [x, y, theta] poses\n        self.constraints = []  # List of relative pose constraints\n        self.optimization_result = None\n\n    def add_pose(self, pose, is_fixed=False):\n        """Add a new pose to the graph"""\n        pose_id = len(self.poses)\n        self.poses.append({\n            \'id\': pose_id,\n            \'pose\': np.array(pose),  # [x, y, theta]\n            \'is_fixed\': is_fixed,\n            \'constraints\': []\n        })\n        return pose_id\n\n    def add_constraint(self, from_id, to_id, relative_pose, information_matrix=None):\n        """Add a constraint between two poses"""\n        if information_matrix is None:\n            # Default information matrix (inverse covariance)\n            information_matrix = np.eye(3) * 10  # High confidence\n\n        constraint = {\n            \'from\': from_id,\n            \'to\': to_id,\n            \'relative_pose\': np.array(relative_pose),  # [dx, dy, dtheta]\n            \'information\': information_matrix\n        }\n\n        self.constraints.append(constraint)\n        self.poses[from_id][\'constraints\'].append(len(self.constraints) - 1)\n\n    def error_function(self, x):\n        """Error function for pose graph optimization"""\n        # Reshape x back to poses\n        num_poses = len(self.poses)\n        reshaped_poses = x.reshape(num_poses, 3)\n\n        total_error = 0\n\n        for constraint in self.constraints:\n            from_id = constraint[\'from\']\n            to_id = constraint[\'to\']\n\n            pose_from = reshaped_poses[from_id]\n            pose_to = reshaped_poses[to_id]\n            relative_target = constraint[\'relative_pose\']\n\n            # Calculate relative pose from current estimates\n            # Transform from pose_from to pose_to\n            dx = pose_to[0] - pose_from[0]\n            dy = pose_to[1] - pose_from[1]\n            dtheta = pose_to[2] - pose_from[2]\n\n            # Convert to the frame of pose_from\n            dx_local = dx * np.cos(-pose_from[2]) - dy * np.sin(-pose_from[2])\n            dy_local = dx * np.sin(-pose_from[2]) + dy * np.cos(-pose_from[2])\n            dtheta_local = dtheta\n\n            relative_current = np.array([dx_local, dy_local, dtheta_local])\n\n            # Calculate error\n            error = relative_current - relative_target\n\n            # Apply information matrix (covariance weighting)\n            weighted_error = error.T @ constraint[\'information\'] @ error\n            total_error += weighted_error\n\n        return total_error\n\n    def optimize_poses(self):\n        """Optimize the pose graph"""\n        if len(self.poses) < 2:\n            return self.poses\n\n        # Initialize optimization vector\n        initial_poses = []\n        free_pose_indices = []\n\n        for i, pose_info in enumerate(self.poses):\n            initial_poses.extend(pose_info[\'pose\'])\n            if not pose_info[\'is_fixed\']:\n                free_pose_indices.extend([i*3, i*3+1, i*3+2])\n\n        if not free_pose_indices:\n            return self.poses  # No free poses to optimize\n\n        # Separate into fixed and variable poses\n        all_indices = set(range(len(initial_poses)))\n        fixed_indices = list(all_indices - set(free_pose_indices))\n\n        def reduced_error_function(free_variables):\n            """Error function with fixed variables substituted"""\n            full_vector = np.array(initial_poses)\n            full_vector[free_pose_indices] = free_variables\n\n            return self.error_function(full_vector)\n\n        # Optimize only the free variables\n        result = minimize(\n            reduced_error_function,\n            x0=np.array(initial_poses)[free_pose_indices],\n            method=\'BFGS\'\n        )\n\n        # Update poses with optimization result\n        optimized_vector = np.array(initial_poses)\n        optimized_vector[free_pose_indices] = result.x\n\n        for i in range(len(self.poses)):\n            self.poses[i][\'pose\'] = optimized_vector[i*3:(i+1)*3]\n\n        self.optimization_result = result\n        return self.poses\n\n    def get_trajectory(self):\n        """Get the optimized trajectory"""\n        trajectory = []\n        for pose_info in self.poses:\n            trajectory.append(pose_info[\'pose\'])\n        return np.array(trajectory)\n\n    def add_odometry_constraint(self, from_id, to_id, odometry_measurement, covariance_scale=1.0):\n        """Add an odometry-based constraint"""\n        # Odometry typically has higher uncertainty\n        information_matrix = np.diag([1.0, 1.0, 0.1]) / covariance_scale\n        self.add_constraint(from_id, to_id, odometry_measurement, information_matrix)\n\n    def add_loop_closure_constraint(self, from_id, to_id, measurement, covariance_scale=0.1):\n        """Add a loop closure constraint (typically more accurate)"""\n        # Loop closures are typically more accurate than odometry\n        information_matrix = np.diag([10.0, 10.0, 1.0]) / covariance_scale\n        self.add_constraint(from_id, to_id, measurement, information_matrix)\n'})}),"\n",(0,o.jsx)(e.h2,{id:"navigation-and-path-planning",children:"Navigation and Path Planning"}),"\n",(0,o.jsx)(e.h3,{id:"path-planning-in-dynamic-environments",children:"Path Planning in Dynamic Environments"}),"\n",(0,o.jsx)(e.p,{children:"Planning paths that account for dynamic obstacles:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import heapq\n\nclass DynamicPathPlanner:\n    def __init__(self, occupancy_grid, resolution=0.1):\n        self.occupancy_grid = occupancy_grid\n        self.resolution = resolution\n        self.grid_height, self.grid_width = occupancy_grid.shape\n        self.object_predictor = ObjectMotionPredictor()\n\n    def a_star(self, start, goal, dynamic_obstacles=None):\n        """A* path planning with consideration for dynamic obstacles"""\n        start_x, start_y = start\n        goal_x, goal_y = goal\n\n        # Convert to grid coordinates\n        start_grid = (int(start_x / self.resolution), int(start_y / self.resolution))\n        goal_grid = (int(goal_x / self.resolution), int(goal_y / self.resolution))\n\n        # Check if start and goal are valid\n        if not self.is_valid_cell(start_grid[0], start_grid[1]) or \\\n           not self.is_valid_cell(goal_grid[0], goal_grid[1]):\n            return None\n\n        # A* algorithm\n        open_set = [(0, start_grid)]\n        came_from = {}\n        g_score = {start_grid: 0}\n        f_score = {start_grid: self.heuristic(start_grid, goal_grid)}\n\n        while open_set:\n            current = heapq.heappop(open_set)[1]\n\n            if current == goal_grid:\n                # Reconstruct path\n                path = []\n                while current in came_from:\n                    path.append(current)\n                    current = came_from[current]\n                path.append(start_grid)\n                path.reverse()\n\n                # Convert back to world coordinates\n                world_path = [(x * self.resolution, y * self.resolution) for x, y in path]\n                return world_path\n\n            for neighbor in self.get_neighbors(current):\n                tentative_g_score = g_score[current] + self.distance(current, neighbor)\n\n                # Check dynamic obstacle prediction\n                if dynamic_obstacles:\n                    neighbor_world = (neighbor[0] * self.resolution, neighbor[1] * self.resolution)\n                    time_to_reach = tentative_g_score * 0.1  # Assume 10 m/s max speed\n                    collision_risk = self.check_dynamic_collision(neighbor_world, time_to_reach, dynamic_obstacles)\n                    if collision_risk > 0.7:  # High collision risk\n                        continue\n\n                if neighbor not in g_score or tentative_g_score < g_score[neighbor]:\n                    came_from[neighbor] = current\n                    g_score[neighbor] = tentative_g_score\n                    f_score[neighbor] = tentative_g_score + self.heuristic(neighbor, goal_grid)\n                    heapq.heappush(open_set, (f_score[neighbor], neighbor))\n\n        return None  # No path found\n\n    def is_valid_cell(self, x, y):\n        """Check if a grid cell is valid (not occupied)"""\n        if 0 <= x < self.grid_width and 0 <= y < self.grid_height:\n            return self.occupancy_grid[y, x] < 0.7  # Occupancy threshold\n        return False\n\n    def get_neighbors(self, cell):\n        """Get valid neighboring cells"""\n        x, y = cell\n        neighbors = []\n        for dx, dy in [(-1,-1), (-1,0), (-1,1), (0,-1), (0,1), (1,-1), (1,0), (1,1)]:\n            nx, ny = x + dx, y + dy\n            if self.is_valid_cell(nx, ny):\n                neighbors.append((nx, ny))\n        return neighbors\n\n    def heuristic(self, a, b):\n        """Heuristic function (Euclidean distance)"""\n        return np.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)\n\n    def distance(self, a, b):\n        """Distance between adjacent cells"""\n        return np.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)\n\n    def check_dynamic_collision(self, position, time, dynamic_obstacles):\n        """Check collision risk with predicted obstacle positions"""\n        risk = 0.0\n        for obstacle in dynamic_obstacles:\n            predicted_pos = self.object_predictor.predict_position(obstacle, time)\n            dist = np.linalg.norm(np.array(position) - np.array(predicted_pos))\n            if dist < 0.5:  # 50cm safety margin\n                # Higher risk as distance decreases\n                risk = max(risk, 1.0 - dist/0.5)\n        return risk\n\nclass ObjectMotionPredictor:\n    def __init__(self):\n        self.object_trajectories = {}  # Store past positions for prediction\n\n    def update_object_position(self, object_id, position, timestamp):\n        """Update object position and store for prediction"""\n        if object_id not in self.object_trajectories:\n            self.object_trajectories[object_id] = []\n\n        self.object_trajectories[object_id].append((position, timestamp))\n\n        # Keep only recent positions (last 5 seconds)\n        current_time = timestamp\n        self.object_trajectories[object_id] = [\n            (pos, time) for pos, time in self.object_trajectories[object_id]\n            if current_time - time < 5.0\n        ]\n\n    def predict_position(self, object_data, future_time):\n        """Predict object position at future time"""\n        object_id = object_data[\'id\']\n\n        if object_id not in self.object_trajectories or len(self.object_trajectories[object_id]) < 2:\n            # No prediction possible, return current position\n            return object_data[\'position\']\n\n        # Use the last two positions to estimate velocity\n        positions = self.object_trajectories[object_id]\n        pos1, time1 = positions[-2]\n        pos2, time2 = positions[-1]\n\n        if time2 == time1:\n            return pos2\n\n        velocity = (np.array(pos2) - np.array(pos1)) / (time2 - time1)\n        predicted_position = np.array(pos2) + velocity * future_time\n\n        return predicted_position.tolist()\n\n    def predict_trajectory(self, object_id, time_horizon=2.0, dt=0.1):\n        """Predict trajectory over time horizon"""\n        trajectory = []\n        for t in np.arange(0, time_horizon, dt):\n            pos = self.predict_position({\'id\': object_id, \'position\': [0, 0]}, t)\n            trajectory.append((t, pos))\n        return trajectory\n'})}),"\n",(0,o.jsx)(e.h3,{id:"human-aware-navigation",children:"Human-Aware Navigation"}),"\n",(0,o.jsx)(e.p,{children:"Navigation that considers human presence and behavior:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class HumanAwareNavigator:\n    def __init__(self):\n        self.social_force_model = SocialForceModel()\n        self.anticipation_horizon = 3.0  # seconds to look ahead\n        self.personal_space_radius = 0.8  # meters\n        self.comfort_zone_radius = 1.2   # meters\n\n    def plan_path_with_human_awareness(self, start, goal, humans, robot_speed=1.0):\n        \"\"\"Plan path considering human presence and expected movements\"\"\"\n        # Use social force model to predict human movements\n        predicted_human_trajectories = []\n        for human in humans:\n            trajectory = self.social_force_model.predict_trajectory(\n                human['position'], human['velocity'], self.anticipation_horizon\n            )\n            predicted_human_trajectories.append({\n                'id': human['id'],\n                'trajectory': trajectory,\n                'radius': self.personal_space_radius\n            })\n\n        # Plan path using a method that considers dynamic obstacles\n        path_planner = DynamicPathPlanner(None)  # Would use actual occupancy grid\n        path = path_planner.a_star(start, goal, predicted_human_trajectories)\n\n        return path\n\n    def calculate_social_comfort(self, robot_position, humans):\n        \"\"\"Calculate how comfortable the robot's position is for humans\"\"\"\n        comfort_score = 1.0  # 1.0 is maximum comfort\n\n        for human in humans:\n            dist_to_human = np.linalg.norm(\n                np.array(robot_position) - np.array(human['position'])\n            )\n\n            if dist_to_human < self.comfort_zone_radius:\n                # Reduce comfort as we get closer to comfort zone\n                penalty = (self.comfort_zone_radius - dist_to_human) / self.comfort_zone_radius\n                comfort_score -= penalty * 0.5  # Weight the penalty\n\n        return max(0, comfort_score)\n\n    def adapt_behavior_for_humans(self, robot_state, humans):\n        \"\"\"Adapt robot behavior based on human presence\"\"\"\n        closest_human_dist = float('inf')\n        closest_human = None\n\n        for human in humans:\n            dist = np.linalg.norm(\n                np.array(robot_state['position']) - np.array(human['position'])\n            )\n            if dist < closest_human_dist:\n                closest_human_dist = dist\n                closest_human = human\n\n        if closest_human_dist < 2.0:  # Within 2m\n            # Adjust behavior based on distance\n            if closest_human_dist < 1.0:\n                # Very close - slow down and give space\n                speed_factor = 0.3\n                prefer_side = self.get_preferred_passing_side(closest_human)\n            elif closest_human_dist < 1.5:\n                # Close - moderate caution\n                speed_factor = 0.7\n                prefer_side = self.get_preferred_passing_side(closest_human)\n            else:\n                # Far enough - normal speed\n                speed_factor = 1.0\n                prefer_side = None\n\n            return {\n                'speed_factor': speed_factor,\n                'prefer_side': prefer_side,\n                'is_alert': True\n            }\n\n        return {\n            'speed_factor': 1.0,\n            'prefer_side': None,\n            'is_alert': False\n        }\n\n    def get_preferred_passing_side(self, human):\n        \"\"\"Determine preferred side to pass human\"\"\"\n        # Simple heuristic: pass on the side where there's more free space\n        # In practice, this would consider hallway geometry, walking direction, etc.\n        return 'right'  # Convention in many countries\n\nclass SocialForceModel:\n    def __init__(self):\n        self.pedestrian_attraction = 1.0  # Tendency to follow others\n        self.wall_repulsion = 5.0         # Repulsion from walls\n        self.personal_space = 0.8         # Desired personal space (m)\n\n    def predict_trajectory(self, position, velocity, time_horizon, dt=0.1):\n        \"\"\"Predict human trajectory using social forces\"\"\"\n        trajectory = []\n        current_pos = np.array(position)\n        current_vel = np.array(velocity)\n\n        for t in np.arange(0, time_horizon, dt):\n            # Calculate social forces (simplified)\n            acceleration = self.calculate_social_forces(current_pos, current_vel)\n\n            # Integrate velocity and position\n            current_vel += acceleration * dt\n            current_pos += current_vel * dt\n\n            trajectory.append(current_pos.copy())\n\n        return trajectory\n\n    def calculate_social_forces(self, position, velocity):\n        \"\"\"Calculate net force on pedestrian\"\"\"\n        # This would include:\n        # - Desired direction force\n        # - Repulsive forces from other pedestrians and obstacles\n        # - Attractive forces (e.g., following behavior)\n        # For this example, return a simple force toward a destination\n\n        # Example: move toward (5, 5) with some noise\n        desired_pos = np.array([5.0, 5.0])\n        desired_vel = (desired_pos - position) * 0.5  # 0.5 m/s desired speed\n\n        # Calculate force to reach desired velocity\n        max_speed = 1.3  # Max human walking speed\n        current_speed = np.linalg.norm(velocity)\n        if current_speed > max_speed:\n            velocity = velocity * max_speed / current_speed\n\n        force = (desired_vel - velocity) * 2.0  # Acceleration toward desired velocity\n\n        return force\n"})}),"\n",(0,o.jsx)(e.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,o.jsx)(e.h3,{id:"ros-2-environmental-perception-node",children:"ROS 2 Environmental Perception Node"}),"\n",(0,o.jsx)(e.p,{children:"Integrating environmental perception with ROS 2:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import PointCloud2, LaserScan\nfrom nav_msgs.msg import OccupancyGrid, Path\nfrom geometry_msgs.msg import PoseStamped, Point\nfrom visualization_msgs.msg import MarkerArray, Marker\nfrom std_msgs.msg import Header\nimport sensor_msgs.point_cloud2 as pc2\n\nclass EnvironmentalPerceptionNode(Node):\n    def __init__(self):\n        super().__init__('environmental_perception_node')\n\n        # Publishers\n        self.map_pub = self.create_publisher(OccupancyGrid, '/map', 10)\n        self.path_pub = self.create_publisher(Path, '/planned_path', 10)\n        self.marker_pub = self.create_publisher(MarkerArray, '/perception_markers', 10)\n\n        # Subscribers\n        self.pointcloud_sub = self.create_subscription(\n            PointCloud2, '/points', self.pointcloud_callback, 10\n        )\n        self.scan_sub = self.create_subscription(\n            LaserScan, '/scan', self.scan_callback, 10\n        )\n\n        # Initialize perception components\n        self.point_processor = PointCloudProcessor()\n        self.object_detector = Object3DDetector()\n        self.mapper = OccupancyGridMapper()\n        self.path_planner = DynamicPathPlanner(self.mapper.occupancy_grid)\n        self.human_navigator = HumanAwareNavigator()\n\n        # Previous detections for tracking\n        self.previous_objects = []\n\n        # Timer for periodic processing\n        self.process_timer = self.create_timer(0.1, self.process_perception)\n\n        self.get_logger().info('Environmental perception node initialized')\n\n    def pointcloud_callback(self, msg):\n        \"\"\"Process incoming point cloud data\"\"\"\n        try:\n            # Convert ROS PointCloud2 to numpy array\n            points = []\n            for point in pc2.read_points(msg, field_names=(\"x\", \"y\", \"z\"), skip_nans=True):\n                points.append([point[0], point[1], point[2]])\n\n            if points:\n                self.point_processor.load_point_cloud(points)\n                self.point_processor.remove_outliers()\n\n                # Process point cloud for objects\n                objects = self.object_detector.detect_objects_voxel_grid(self.point_processor.points)\n\n                # Update object tracking\n                self.previous_objects = self.object_detector.track_objects(\n                    objects, self.previous_objects\n                )\n\n                self.get_logger().info(f'Detected {len(objects)} objects')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing point cloud: {e}')\n\n    def scan_callback(self, msg):\n        \"\"\"Process laser scan data for mapping\"\"\"\n        try:\n            # Convert laser scan to ranges and angles\n            ranges = np.array(msg.ranges)\n            angles = np.array([msg.angle_min + i * msg.angle_increment\n                              for i in range(len(ranges))])\n\n            # Filter invalid ranges\n            valid_indices = (ranges > msg.range_min) & (ranges < msg.range_max)\n            valid_ranges = ranges[valid_indices]\n            valid_angles = angles[valid_indices]\n\n            # Update occupancy grid (assuming robot is at origin for this example)\n            robot_pose = [0.0, 0.0, 0.0]  # [x, y, theta]\n            self.mapper.update_with_laser_scan(robot_pose, valid_ranges, valid_angles)\n\n            self.get_logger().info(f'Processed {len(valid_ranges)} laser points')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing laser scan: {e}')\n\n    def process_perception(self):\n        \"\"\"Periodic processing of perception data\"\"\"\n        try:\n            # Publish current map\n            self.publish_map()\n\n            # Publish detected objects as markers\n            self.publish_object_markers()\n\n            # Example: plan a path (in a real system, goal would come from navigation)\n            if len(self.previous_objects) > 0:\n                start = [0.0, 0.0]\n                goal = [2.0, 2.0]  # Example goal\n\n                path = self.path_planner.a_star(start, goal)\n                if path:\n                    self.publish_path(path)\n\n        except Exception as e:\n            self.get_logger().error(f'Error in perception processing: {e}')\n\n    def publish_map(self):\n        \"\"\"Publish occupancy grid map\"\"\"\n        msg = OccupancyGrid()\n        msg.header.stamp = self.get_clock().now().to_msg()\n        msg.header.frame_id = 'map'\n\n        msg.info.resolution = self.mapper.resolution\n        msg.info.width = self.mapper.grid_size[0]\n        msg.info.height = self.mapper.grid_size[1]\n        msg.info.origin.position.x = -self.mapper.width / 2\n        msg.info.origin.position.y = -self.mapper.height / 2\n\n        # Convert occupancy grid to the format needed by OccupancyGrid\n        flat_grid = (self.mapper.occupancy_grid * 100).astype(np.int8).flatten()\n        # Convert probabilities to ROS format (-1: unknown, 0: free, 100: occupied)\n        flat_grid = np.where(flat_grid > 50, 100, np.where(flat_grid < 30, 0, -1))\n        msg.data = flat_grid.tolist()\n\n        self.map_pub.publish(msg)\n\n    def publish_object_markers(self):\n        \"\"\"Publish detected objects as visualization markers\"\"\"\n        marker_array = MarkerArray()\n\n        for i, obj in enumerate(self.previous_objects):\n            # Create marker for object position\n            marker = Marker()\n            marker.header.frame_id = 'map'\n            marker.header.stamp = self.get_clock().now().to_msg()\n            marker.ns = 'objects'\n            marker.id = i\n            marker.type = Marker.SPHERE\n            marker.action = Marker.ADD\n\n            marker.pose.position.x = float(obj['center'][0])\n            marker.pose.position.y = float(obj['center'][1])\n            marker.pose.position.z = float(obj['center'][2])\n            marker.pose.orientation.w = 1.0\n\n            # Size based on object dimensions\n            max_dim = max(obj['dimensions'])\n            marker.scale.x = max_dim\n            marker.scale.y = max_dim\n            marker.scale.z = max_dim\n\n            marker.color.r = 1.0\n            marker.color.g = 0.0\n            marker.color.b = 0.0\n            marker.color.a = 0.5  # Semi-transparent\n\n            marker_array.markers.append(marker)\n\n        self.marker_pub.publish(marker_array)\n\n    def publish_path(self, path):\n        \"\"\"Publish planned path\"\"\"\n        path_msg = Path()\n        path_msg.header.stamp = self.get_clock().now().to_msg()\n        path_msg.header.frame_id = 'map'\n\n        for point in path:\n            pose = PoseStamped()\n            pose.header.stamp = path_msg.header.stamp\n            pose.header.frame_id = path_msg.header.frame_id\n            pose.pose.position.x = point[0]\n            pose.pose.position.y = point[1]\n            pose.pose.position.z = 0.0\n            pose.pose.orientation.w = 1.0\n\n            path_msg.poses.append(pose)\n\n        self.path_pub.publish(path_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_node = EnvironmentalPerceptionNode()\n\n    try:\n        rclpy.spin(perception_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        perception_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,o.jsx)(e.p,{children:"Environmental perception systems provide humanoid robots with the ability to understand and navigate their surroundings. From 3D point cloud processing to SLAM and path planning, these systems enable robots to operate safely and effectively in complex, dynamic environments. The integration of multiple sensor modalities through sensor fusion allows for robust and reliable environmental understanding, while real-time processing capabilities ensure that perception can keep pace with the demands of dynamic humanoid behavior."}),"\n",(0,o.jsx)(e.p,{children:"The next section will provide practical examples and exercises that demonstrate the implementation of environmental perception techniques, allowing students to gain hands-on experience with these important concepts."})]})}function p(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>r,x:()=>a});var o=i(6540);const s={},t=o.createContext(s);function r(n){const e=o.useContext(t);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:r(n.components),o.createElement(t.Provider,{value:e},n.children)}}}]);